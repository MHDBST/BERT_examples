{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joint_ MaskLM_document_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/Joint__MaskLM_document_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "# import python modules defined by BERT\n",
        "from bert import modeling\n",
        "# import optimization\n",
        "# import run_classifier\n",
        "from bert import run_classifier_with_tfhub\n",
        "# import tokenization\n",
        "\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "6a20669d-f483-495b-cd1c-9b448f522f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/aug19_models/joint/multitask/last3layers'.format(BUCKET)\n",
        "# V2_augment/doc_level/v2/smallBERT-docLevel-seq512/experiment1'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "# !gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/aug19_models/joint/multitask/last3layers *****\n",
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import modeling\n",
        "# import run_classifier\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  pre_tvars = tf.trainable_variables()\n",
        "#   print('all trainable variables: >>',pre_tvars)\n",
        "  tvars = pre_tvars\n",
        "  tvars = [item for item in pre_tvars if not '/layer_0/'  in item.name and not '/layer_1/'  in item.name and not '/layer_2/'  in item.name\n",
        "          and not '/layer_3/'  in item.name and not '/layer_4/'  in item.name and not '/layer_5/'  in item.name and not '/layer_6/'  in item.name \n",
        "          and not '/layer_7/'  in item.name and not '/layer_8/'  in item.name ]\n",
        "           #and not '/layer_9/' in item.name]\n",
        "#   and not '/layer_10/'  in item.name \n",
        "#           and not '/layer_11/'  in item.name ]\n",
        "  print('excluded trainable variables: >>',tvars)\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_TRAIN_EPOCHS = 10.0  ## Activate if ** is Not ACTIVATED\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 50\n",
        "SAVE_SUMMARY_STEPS = 20\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fix_doc_tgt(doc,index,df):\n",
        "  new_doc = doc.replace('.[TGT]',' .\\n[TGT] ')\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "\n",
        "  new_doc = new_doc.replace('?[TGT]',' ?\\n[TGT] ')\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "  new_doc = new_doc.replace( '[TGT][TGT]', '[TGT] \\n [TGT]' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "\n",
        "  new_doc = new_doc.replace( '. USA TODAY', '. USA TODAY Sports \\n' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "  \n",
        "  new_doc = new_doc.replace('![TGT]','!\\n[TGT]' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  return new_doc\n",
        "\n",
        "data_pref = 'gs://bert_example/data_aug19/masked_lm/mask_lm_combined_shuffled_3Dec_7Dec_aug19_reindex_%s.csv'\n",
        "\n",
        "data_train = pd.read_csv(tf.gfile.GFile(data_pref % str('train')), encoding='latin-1')\n",
        "data_dev   = pd.read_csv(tf.gfile.GFile(data_pref % 'dev'), encoding='latin-1')\n",
        "data_test  = pd.read_csv(tf.gfile.GFile(data_pref % 'random_test'), encoding='latin-1')\n",
        "data_test_fixed= pd.read_csv(tf.gfile.GFile(data_pref % 'fixed_test'), encoding='latin-1')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(df):\n",
        "#   print('df length>>',len(df['DOCUMENT']))\n",
        "  data = {}\n",
        "  df['DOCUMENT'] = df['DOCUMENT'].str.replace('\\[TGT\\]','tgt')\n",
        "  data[\"sentence\"] = df['DOCUMENT']\n",
        "  data[\"label\"] =df[\"LABEL\"]\n",
        "  data['doc_id'] = df[\"DOCUMENT_INDEX\"]\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None):\n",
        "  df_new = load_directory_data(df[:index])\n",
        "#   print(df_new)\n",
        "  true_df = df_new[df_new['label'] == True]\n",
        "  false_df = df_new[df_new['label'] == False]\n",
        "#   print('true_df>>>',len(true_df))\n",
        "#   true_df[\"polarity\"] = 1\n",
        "#   false_df[\"polarity\"] = 0\n",
        "  return pd.concat([true_df, false_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train = load_dataset(data_train)\n",
        "# train_augment = load_dataset(data_augment)\n",
        "# train = pd.concat([train, train_augment]).sample(frac=1).reset_index(drop=True)\n",
        "dev = load_dataset(data_dev)\n",
        "test = load_dataset(data_test)\n",
        "test_fixed = load_dataset(data_test_fixed)\n",
        "\n",
        "# print('train set length: %d,dev set length: %d, test set lenght: %d,  fixed test: %d'%\n",
        "#       (len(train),len(dev),len(test),len(test_fixed)))\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ybZ7j8NUWKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "2f4f843a-91be-4743-83d1-245957c33454"
      },
      "source": [
        "\n",
        "\n",
        "## if pl is true, append  paragraph leve data and label to the output\n",
        "## if dl is true, append document leve data and label to the output\n",
        "#### Both pl and dl can not be false\n",
        "def load_paragraphs_documents(df,pl=True,dl=True,ent=False,column= 'MASKED_DOCUMENT'):\n",
        "    if not dl and not pl:\n",
        "      print('both document level label and paragaph level label is false, choose one of them a True')\n",
        "      return \n",
        "    labels = []\n",
        "    texts = []\n",
        "    doc_ids = []\n",
        "    uniq_ents = []\n",
        "    num_doc = 0\n",
        "    index = -1\n",
        "    for doc in df[column]:\n",
        "      index += 1\n",
        "      docs = doc.split('\\n')\n",
        "      doc_length = len(docs)\n",
        "\n",
        "      if pd.isnull(df['Paragraph0'].iloc[index]):\n",
        "      # add documents with no paragraph labels as one document and its label to the input data dataframe\n",
        "        if dl:\n",
        "          labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "          texts.append(doc)\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "        num_doc +=1\n",
        "        \n",
        "        continue\n",
        "      try:\n",
        "        if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "         \n",
        "          doc = fix_doc_tgt(doc,index,df)\n",
        "          docs = doc.split('\\n')\n",
        "          \n",
        "          doc_length = len(docs)\n",
        "          if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "            # print(doc)\n",
        "            if column == 'summary':\n",
        "              pass\n",
        "            else:\n",
        "              print('error on document %d'% df['DOCUMENT_INDEX'].iloc[index])\n",
        "              print('document length is %s'%str(doc_length))\n",
        "              continue\n",
        "\n",
        "      except Exception as e:\n",
        "        print('err is %s'%str(e))\n",
        "        print('this document has %d paragraphs %d' %(doc_length,df['DOCUMENT_INDEX'].iloc[index]))\n",
        "\n",
        "      if pl:\n",
        "        for i in range(doc_length):\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "          \n",
        "          texts.append(docs[i])\n",
        "          label_i = df['Paragraph%d'%i].iloc[index]\n",
        "          labels.append(label_i)\n",
        "        ### increase the effect of documents by adding the whole document per each paragraph :D\n",
        "          if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "      else:\n",
        "         if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              \n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "         \n",
        "    print('number of one-paragraph docs: %d'%num_doc)\n",
        "    return(texts,labels,doc_ids,uniq_ents)\n",
        " \n",
        "\n",
        "data_pref = 'gs://bert_example/data_aug19/all_data_combined_shuffled_3Dec_7Dec_aug19_reindex_%s.csv'\n",
        "train_df = pd.read_csv(tf.gfile.GFile(data_pref % str('train')), encoding='latin-1')\n",
        "dev_df = pd.read_csv(tf.gfile.GFile(data_pref % str('dev')), encoding='latin-1')\n",
        "dev_ent_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data_aug19/dev_entities_v5.csv'), encoding='latin-1')\n",
        "\n",
        "# dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data_aug19/abstractive_pgn_summary_dev_comma.csv'), encoding='latin-1')\n",
        "test_random_df= pd.read_csv(tf.gfile.GFile(data_pref % str('random_test')), encoding='latin-1')\n",
        "test_fixed_df= pd.read_csv(tf.gfile.GFile(data_pref % str('fixed_test')), encoding='latin-1')\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_file(df,dl=True,pl=True,ent=False,column= 'MASKED_DOCUMENT'):\n",
        "  data = {}\n",
        "  (texts,labels,doc_ids,uniq_ent ) = load_paragraphs_documents(df,dl=dl,pl=pl,column= column,ent=ent)\n",
        "  data[\"sentence\"] = texts\n",
        "  data[\"sentiment\"] =labels\n",
        "  data[\"doc_id\"] = doc_ids\n",
        "  if ent:\n",
        "    data[\"uniq_ent\"] = uniq_ent\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None,dl=True,pl=True,column= 'MASKED_DOCUMENT',ent=False):\n",
        "  df_new = load_file(df,dl,pl,column= column,ent=ent)\n",
        "  pos_df = df_new[df_new['sentiment'] == 'Positive']\n",
        "  neg_df = df_new[df_new['sentiment'] == 'Negative']\n",
        "  neu_df = df_new[df_new['sentiment'] == 'Neutral']\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = -1\n",
        "  neu_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df,neu_df]).sample(frac=1).reset_index(drop=True)\n",
        "# train_all,dev_par,dev_doc,test_par,test_doc,test_fixed_par,test_fixed_doc= [],[],[],[],[],[],[]\n",
        "### train should consist both paragraph level and document level labels\n",
        "print('processing train set')\n",
        "\n",
        "train_doc = load_dataset(train_df,dl=True,pl=False)\n",
        "### two dev set, to test the model with paragraph level dev and document level dev\n",
        "print('processing dev set')\n",
        "# dev_doc = load_dataset(dev_df,pl=pl,dl=dl)\n",
        "dev_ent = load_dataset(dev_ent_df,pl=False,dl=True,ent=True)\n",
        "\n",
        "print('processing random test set')\n",
        "test_doc = load_dataset(test_random_df,pl=False,dl=True)\n",
        "# ### two fixed tests like above\n",
        "print('processing fixed test set')\n",
        "test_fixed_doc = load_dataset(test_fixed_df,pl=False,dl=True)\n",
        "\n",
        "print('Number of train inputs: %d\\n \\\\\n",
        "        Number of document level dev inputs: %d\\n  \\\\\n",
        "        Number of document level fixed test inputs: %d \\n \\\\\n",
        "       Number of document level random test inputs: %d'\n",
        "      %(len(train_doc),len(dev_ent),len(test_fixed_doc),len(test_doc)))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing train set\n",
            "number of one-paragraph docs: 252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "processing dev set\n",
            "number of one-paragraph docs: 45\n",
            "processing random test set\n",
            "number of one-paragraph docs: 52\n",
            "processing fixed test set\n",
            "number of one-paragraph docs: 114\n",
            "Number of train inputs: 3355\n",
            " \\        Number of document level dev inputs: 578\n",
            "  \\        Number of document level fixed test inputs: 827 \n",
            " \\       Number of document level random test inputs: 579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqkRCGPetz38",
        "colab_type": "code",
        "outputId": "8e24e454-c757-4d02-f5b6-005e6636c94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(train))\n",
        "print(len(train_doc))\n",
        "# print(set(train['doc_id']))\n",
        "# print(set(train['doc_id']) == set(train_doc['doc_id']))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45944\n",
            "3355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXxXpuxZ03Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f1fd382-3ffb-4ce5-911c-ce8514802377"
      },
      "source": [
        "print(len(dev_ent))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCjV87FWKCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "29e73ddd-e669-4b1e-8680-0fb0acfbad59"
      },
      "source": [
        "train_joint = pd.merge(left=train,right=train_doc, left_on='doc_id', right_on='doc_id',suffixes=('','_y'))[['doc_id','label','sentence','sentiment', 'polarity']]\n",
        "dev_joint = pd.merge(left=dev,right=dev_ent, left_on='doc_id', right_on='doc_id',suffixes=('','_y'))[['doc_id','label','sentence','sentiment', 'polarity']]\n",
        "random_test_joint = pd.merge(left=test,right=test_doc, left_on='doc_id', right_on='doc_id',suffixes=('','_y'))[['doc_id','label','sentence','sentiment', 'polarity']]\n",
        "fixed_test_joint = pd.merge(left=test_fixed,right=test_fixed_doc, left_on='doc_id', right_on='doc_id',suffixes=('','_y'))[['doc_id','label','sentence','sentiment', 'polarity']]\n",
        "print(list(train_joint))\n",
        "print(len(train_joint),len(dev_joint),len(random_test_joint),len(fixed_test_joint))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['doc_id', 'label', 'sentence', 'sentiment', 'polarity']\n",
            "45944 7187 8130 12604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIqrDeUldrKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f753d1d5-c8af-4390-8e5f-efeec58eaff1"
      },
      "source": [
        "train_joint['joint_label'] = train_joint['label'].astype(str)+'_'+train_joint['polarity'].astype(str)\n",
        "\n",
        "dev_joint['joint_label'] = dev_joint['label'].astype(str)+'_'+dev_joint['polarity'].astype(str)\n",
        "\n",
        "random_test_joint['joint_label'] = random_test_joint['label'].astype(str)+'_'+random_test_joint['polarity'].astype(str)\n",
        "\n",
        "fixed_test_joint['joint_label'] = fixed_test_joint['label'].astype(str)+'_'+fixed_test_joint['polarity'].astype(str)\n",
        "\n",
        "print(list(train_joint))\n",
        "print(list(set(train_joint['joint_label'])))\n",
        "\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'joint_label'\n",
        "label_list = list(set(train_joint['joint_label']))\n",
        "use_tpu = True\n",
        "len(set(train_joint[LABEL_COLUMN]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['doc_id', 'label', 'sentence', 'sentiment', 'polarity', 'joint_label']\n",
            "['False_1', 'False_0', 'False_-1', 'True_-1', 'True_1', 'True_0']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLEWLUG0Fmi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import tokenization\n",
        "from bert import run_classifier\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/uncased_L-12_H-768_A-12', 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_joint_InputExamples = train_joint.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_joint_InputExamples = dev_joint.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# dev_features = run_classifier.convert_examples_to_features(dev_joint_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "test_joint_InputExamples = random_test_joint.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# test_joint_features = run_classifier.convert_examples_to_features(test_joint_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_joint_InputExamples_fixed = fixed_test_joint.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "# test_joint_features_fixed = run_classifier.convert_examples_to_features(test_joint_InputExamples_fixed, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# ## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_joint_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_joint_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOesPBlKTv94",
        "colab_type": "code",
        "outputId": "950d9a3f-c70d-4685-cbfe-f45816f8504d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pickle\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "try:\n",
        "  train_joint_features = pickle.load(open('train_joint_features_large','rb'))\n",
        "except Exception as e: \n",
        "    print('can not load train features, creating train features: %s'%str(e))\n",
        "#     train_features = pickle.load(tf.gfile.GFile('gs://bert_example/mask_lm/models/V2_augment/doc_leve/last_2/smallBERT-docLevel-seq512/data/train_features_large.dms', \"rb\"))\n",
        "\n",
        "\n",
        "  \n",
        "    train_joint_features = run_classifier.convert_examples_to_features(train_joint_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    with open('train_joint_features_large','wb') as f:\n",
        "      pickle.dump(train_joint_features,f)\n",
        "try:\n",
        "\n",
        "  dev_joint_features = pickle.load(open('dev_joint_features_large','rb'))\n",
        "except Exception as e:\n",
        "\n",
        "   \n",
        "    print('can not load from GC, creating dev features')\n",
        "    dev_joint_features = run_classifier.convert_examples_to_features(dev_joint_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    with open('dev_joint_features_large','wb') as f:\n",
        "      pickle.dump(dev_joint_features,f)\n",
        "print(len(train_joint_features))\n",
        "print(len(dev_joint_features))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1124 21:34:08.753768 140629443692416 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I1124 21:34:08.755482 140629443692416 run_classifier.py:774] Writing example 0 of 45944\n",
            "I1124 21:34:08.781603 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:34:08.783005 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:34:08.785175 140629443692416 run_classifier.py:464] tokens: [CLS] catalan regional police officers walk through a street covered with ballots for the banned independence referendum thrown by people outside people ' s party ( pp ) regional headquarters during a protest in barcelona spain october 3 2017 . reuters / jon na ##z ##ca madrid ( reuters ) - leaders of spain a s industrial ##ised northeastern region of catalonia said the regional population had voted for independence in a ballot on sunday that the central government said was illegal and non - representative . with 95 percent of the vote counted authorities said the a yes a vote stood at 90 . 1 percent albeit with a turnout of just a little over 40 percent - 2 . 26 million out of 5 . 34 million registered voters . on tuesday barcelona metro stations were closed picket ##s blocked main roads and civil servants walked out in response to a strike called by pro - independence groups . the stand - off could now take three different paths . tensions es ##cala ##te tgt ##con ##tin ##ues with tgt pledge to declare un ##ila ##tera ##l independence likely prompting the central government to trigger article 155 of the spanish constitution which would strip the region of [ mask ] autonomous status and put tgt under direct madrid rule . if article 155 is triggered the next step would be snap elections in catalonia . pro - independence parties only hold the majority in the regional parliament with the help of the far - left group cup and polls before sunday a s ballot suggest these groups could lose their hold on power if a new election was called . however after events on sunday when police swung tr ##un ##cheon ##s and fired rubber bullets in the direction of mostly peaceful demonstrators to shut down the referendum pro - independence parties may again win the election . such a result would raise doubts surrounding prime minister mariano raj ##oy a s ability to govern . students wrapped with este ##lad ##as ( catalan sep ##arat ##ist flags ) walk through a street as they arrive to attend a protest two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / jon na ##z ##ca raj ##oy a s immediate opposition in the national parliament lacks the numbers to force him out through a no - confidence vote but without cross - party support on catalonia he may be forced to call a snap national election himself . raj ##oy was voted in as a minority leader twice in last year a s two elections and a fra ##ct ##ious parliament suggests a snap election may prompt a similar result . talks students hold catalan sep ##arat ##ist flags during a demonstration two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / enrique cal ##vo if the catalan parliament gives up the push for independence raj ##oy may sit down with pu ##ig ##de ##mont to hammer out [SEP]\n",
            "I1124 21:34:08.786683 140629443692416 run_classifier.py:465] input_ids: 101 13973 3164 2610 3738 3328 2083 1037 2395 3139 2007 17069 2005 1996 7917 4336 9782 6908 2011 2111 2648 2111 1005 1055 2283 1006 4903 1007 3164 4075 2076 1037 6186 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 6921 1006 26665 1007 1011 4177 1997 3577 1037 1055 3919 5084 8763 2555 1997 16711 2056 1996 3164 2313 2018 5444 2005 4336 1999 1037 10428 2006 4465 2008 1996 2430 2231 2056 2001 6206 1998 2512 1011 4387 1012 2007 5345 3867 1997 1996 3789 8897 4614 2056 1996 1037 2748 1037 3789 2768 2012 3938 1012 1015 3867 12167 2007 1037 15512 1997 2074 1037 2210 2058 2871 3867 1011 1016 1012 2656 2454 2041 1997 1019 1012 4090 2454 5068 7206 1012 2006 9857 7623 6005 3703 2020 2701 28972 2015 8534 2364 4925 1998 2942 8858 2939 2041 1999 3433 2000 1037 4894 2170 2011 4013 1011 4336 2967 1012 1996 3233 1011 2125 2071 2085 2202 2093 2367 10425 1012 13136 9686 25015 2618 1 8663 7629 15808 2007 1 16393 2000 13520 4895 11733 14621 2140 4336 3497 15870 1996 2430 2231 2000 9495 3720 14168 1997 1996 3009 4552 2029 2052 6167 1996 2555 1997 1031 7308 1033 8392 3570 1998 2404 1 2104 3622 6921 3627 1012 2065 3720 14168 2003 13330 1996 2279 3357 2052 2022 10245 3864 1999 16711 1012 4013 1011 4336 4243 2069 2907 1996 3484 1999 1996 3164 3323 2007 1996 2393 1997 1996 2521 1011 2187 2177 2452 1998 14592 2077 4465 1037 1055 10428 6592 2122 2967 2071 4558 2037 2907 2006 2373 2065 1037 2047 2602 2001 2170 1012 2174 2044 2824 2006 4465 2043 2610 7671 19817 4609 28099 2015 1998 5045 8903 10432 1999 1996 3257 1997 3262 9379 28337 2000 3844 2091 1996 9782 4013 1011 4336 4243 2089 2153 2663 1996 2602 1012 2107 1037 2765 2052 5333 13579 4193 3539 2704 22695 11948 6977 1037 1055 3754 2000 21208 1012 2493 5058 2007 28517 27266 3022 1006 13973 19802 25879 2923 9245 1007 3328 2083 1037 2395 2004 2027 7180 2000 5463 1037 6186 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 11948 6977 1037 1055 6234 4559 1999 1996 2120 3323 14087 1996 3616 2000 2486 2032 2041 2083 1037 2053 1011 7023 3789 2021 2302 2892 1011 2283 2490 2006 16711 2002 2089 2022 3140 2000 2655 1037 10245 2120 2602 2370 1012 11948 6977 2001 5444 1999 2004 1037 7162 3003 3807 1999 2197 2095 1037 1055 2048 3864 1998 1037 25312 6593 6313 3323 6083 1037 10245 2602 2089 25732 1037 2714 2765 1012 7566 2493 2907 13973 19802 25879 2923 9245 2076 1037 10467 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 15769 10250 6767 2065 1996 13973 3323 3957 2039 1996 5245 2005 4336 11948 6977 2089 4133 2091 2007 16405 8004 3207 9629 2000 8691 2041 102\n",
            "I1124 21:34:08.788599 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:34:08.789773 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:34:08.790796 140629443692416 run_classifier.py:468] label: True_0 (id = 5)\n",
            "I1124 21:34:08.817665 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:34:08.818862 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:34:08.820930 140629443692416 run_classifier.py:464] tokens: [CLS] catalan regional police officers walk through a street covered with ballots for the banned independence referendum thrown by people outside people ' s party ( pp ) regional headquarters during a protest in barcelona spain october 3 2017 . reuters / jon na ##z ##ca madrid ( reuters ) - leaders of spain a s industrial ##ised northeastern region of catalonia said the regional population had voted for independence in a ballot on sunday that the central government said was illegal and non - representative . with 95 percent of the vote counted authorities said the a yes a vote stood at 90 . 1 percent albeit with a turnout of just a little over 40 percent - 2 . 26 million out of 5 . 34 million registered voters . on tuesday barcelona metro stations were closed picket ##s blocked main roads and civil servants walked out in response to a strike called by pro - independence groups . the stand - off could now take three different paths . tensions es ##cala ##te tgt ##con ##tin ##ues with tgt pledge to declare un ##ila ##tera ##l independence likely prompting the central government to trigger article 155 of the spanish constitution which would strip the region of tgt autonomous status and put tgt under direct madrid rule . if article 155 is triggered the next step would be snap elections in catalonia . pro - independence parties only hold the majority in the regional parliament with the help of the far - left group cup and polls before sunday a s ballot suggest these groups could lose their hold on power if a new election was called . however after events on sunday when police swung tr ##un ##cheon ##s and fired rubber bullets in the direction of mostly peaceful demonstrators to shut down the referendum pro - independence parties may again win the election . such a result would raise doubts surrounding prime minister mariano raj ##oy a s ability to govern . students wrapped with este ##lad ##as ( catalan sep ##arat ##ist flags ) walk through a street as they arrive to attend a protest two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / jon na ##z ##ca raj ##oy a s immediate opposition in the national parliament lacks the numbers to force him out through a no - confidence vote but without cross - party support on catalonia he may be forced to call a snap national election himself . raj ##oy was voted in as a minority leader twice in last year a s two elections and a fra ##ct ##ious parliament suggests a snap election may prompt a similar result . talks students hold catalan sep ##arat ##ist flags during a demonstration two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / enrique cal ##vo if the catalan parliament gives up the push for independence raj ##oy may sit down with pu ##ig ##de ##mont to hammer out a new [SEP]\n",
            "I1124 21:34:08.822293 140629443692416 run_classifier.py:465] input_ids: 101 13973 3164 2610 3738 3328 2083 1037 2395 3139 2007 17069 2005 1996 7917 4336 9782 6908 2011 2111 2648 2111 1005 1055 2283 1006 4903 1007 3164 4075 2076 1037 6186 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 6921 1006 26665 1007 1011 4177 1997 3577 1037 1055 3919 5084 8763 2555 1997 16711 2056 1996 3164 2313 2018 5444 2005 4336 1999 1037 10428 2006 4465 2008 1996 2430 2231 2056 2001 6206 1998 2512 1011 4387 1012 2007 5345 3867 1997 1996 3789 8897 4614 2056 1996 1037 2748 1037 3789 2768 2012 3938 1012 1015 3867 12167 2007 1037 15512 1997 2074 1037 2210 2058 2871 3867 1011 1016 1012 2656 2454 2041 1997 1019 1012 4090 2454 5068 7206 1012 2006 9857 7623 6005 3703 2020 2701 28972 2015 8534 2364 4925 1998 2942 8858 2939 2041 1999 3433 2000 1037 4894 2170 2011 4013 1011 4336 2967 1012 1996 3233 1011 2125 2071 2085 2202 2093 2367 10425 1012 13136 9686 25015 2618 1 8663 7629 15808 2007 1 16393 2000 13520 4895 11733 14621 2140 4336 3497 15870 1996 2430 2231 2000 9495 3720 14168 1997 1996 3009 4552 2029 2052 6167 1996 2555 1997 1 8392 3570 1998 2404 1 2104 3622 6921 3627 1012 2065 3720 14168 2003 13330 1996 2279 3357 2052 2022 10245 3864 1999 16711 1012 4013 1011 4336 4243 2069 2907 1996 3484 1999 1996 3164 3323 2007 1996 2393 1997 1996 2521 1011 2187 2177 2452 1998 14592 2077 4465 1037 1055 10428 6592 2122 2967 2071 4558 2037 2907 2006 2373 2065 1037 2047 2602 2001 2170 1012 2174 2044 2824 2006 4465 2043 2610 7671 19817 4609 28099 2015 1998 5045 8903 10432 1999 1996 3257 1997 3262 9379 28337 2000 3844 2091 1996 9782 4013 1011 4336 4243 2089 2153 2663 1996 2602 1012 2107 1037 2765 2052 5333 13579 4193 3539 2704 22695 11948 6977 1037 1055 3754 2000 21208 1012 2493 5058 2007 28517 27266 3022 1006 13973 19802 25879 2923 9245 1007 3328 2083 1037 2395 2004 2027 7180 2000 5463 1037 6186 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 11948 6977 1037 1055 6234 4559 1999 1996 2120 3323 14087 1996 3616 2000 2486 2032 2041 2083 1037 2053 1011 7023 3789 2021 2302 2892 1011 2283 2490 2006 16711 2002 2089 2022 3140 2000 2655 1037 10245 2120 2602 2370 1012 11948 6977 2001 5444 1999 2004 1037 7162 3003 3807 1999 2197 2095 1037 1055 2048 3864 1998 1037 25312 6593 6313 3323 6083 1037 10245 2602 2089 25732 1037 2714 2765 1012 7566 2493 2907 13973 19802 25879 2923 9245 2076 1037 10467 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 15769 10250 6767 2065 1996 13973 3323 3957 2039 1996 5245 2005 4336 11948 6977 2089 4133 2091 2007 16405 8004 3207 9629 2000 8691 2041 1037 2047 102\n",
            "I1124 21:34:08.824193 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:34:08.825321 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:34:08.826663 140629443692416 run_classifier.py:468] label: False_0 (id = 1)\n",
            "I1124 21:34:08.858455 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:34:08.859610 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:34:08.861648 140629443692416 run_classifier.py:464] tokens: [CLS] catalan regional police officers walk through a street covered with ballots for the banned independence referendum thrown by people outside people ' s party ( pp ) regional headquarters during a protest in barcelona spain october 3 2017 . reuters / [ mask ] madrid ( reuters ) - leaders of spain a s industrial ##ised northeastern region of catalonia said the regional population had voted for independence in a ballot on sunday that the central government said was illegal and non - representative . with 95 percent of the vote counted authorities said the a yes a vote stood at 90 . 1 percent albeit with a turnout of just a little over 40 percent - 2 . 26 million out of 5 . 34 million registered voters . on tuesday barcelona metro stations were closed picket ##s blocked main roads and civil servants walked out in response to a strike called by pro - independence groups . the stand - off could now take three different paths . tensions es ##cala ##te tgt ##con ##tin ##ues with tgt pledge to declare un ##ila ##tera ##l independence likely prompting the central government to trigger article 155 of the spanish constitution which would strip the region of tgt autonomous status and put tgt under direct madrid rule . if article 155 is triggered the next step would be snap elections in catalonia . pro - independence parties only hold the majority in the regional parliament with the help of the far - left group cup and polls before sunday a s ballot suggest these groups could lose their hold on power if a new election was called . however after events on sunday when police swung tr ##un ##cheon ##s and fired rubber bullets in the direction of mostly peaceful demonstrators to shut down the referendum pro - independence parties may again win the election . such a result would raise doubts surrounding prime minister mariano raj ##oy a s ability to govern . students wrapped with este ##lad ##as ( catalan sep ##arat ##ist flags ) walk through a street as they arrive to attend a protest two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / [ mask ] raj ##oy a s immediate opposition in the national parliament lacks the numbers to force him out through a no - confidence vote but without cross - party support on catalonia he may be forced to call a snap national election himself . raj ##oy was voted in as a minority leader twice in last year a s two elections and a fra ##ct ##ious parliament suggests a snap election may prompt a similar result . talks students hold catalan sep ##arat ##ist flags during a demonstration two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / enrique cal ##vo if the catalan parliament gives up the push for independence raj ##oy may sit down with pu ##ig ##de ##mont to hammer out a new deal for [SEP]\n",
            "I1124 21:34:08.863167 140629443692416 run_classifier.py:465] input_ids: 101 13973 3164 2610 3738 3328 2083 1037 2395 3139 2007 17069 2005 1996 7917 4336 9782 6908 2011 2111 2648 2111 1005 1055 2283 1006 4903 1007 3164 4075 2076 1037 6186 1999 7623 3577 2255 1017 2418 1012 26665 1013 1031 7308 1033 6921 1006 26665 1007 1011 4177 1997 3577 1037 1055 3919 5084 8763 2555 1997 16711 2056 1996 3164 2313 2018 5444 2005 4336 1999 1037 10428 2006 4465 2008 1996 2430 2231 2056 2001 6206 1998 2512 1011 4387 1012 2007 5345 3867 1997 1996 3789 8897 4614 2056 1996 1037 2748 1037 3789 2768 2012 3938 1012 1015 3867 12167 2007 1037 15512 1997 2074 1037 2210 2058 2871 3867 1011 1016 1012 2656 2454 2041 1997 1019 1012 4090 2454 5068 7206 1012 2006 9857 7623 6005 3703 2020 2701 28972 2015 8534 2364 4925 1998 2942 8858 2939 2041 1999 3433 2000 1037 4894 2170 2011 4013 1011 4336 2967 1012 1996 3233 1011 2125 2071 2085 2202 2093 2367 10425 1012 13136 9686 25015 2618 1 8663 7629 15808 2007 1 16393 2000 13520 4895 11733 14621 2140 4336 3497 15870 1996 2430 2231 2000 9495 3720 14168 1997 1996 3009 4552 2029 2052 6167 1996 2555 1997 1 8392 3570 1998 2404 1 2104 3622 6921 3627 1012 2065 3720 14168 2003 13330 1996 2279 3357 2052 2022 10245 3864 1999 16711 1012 4013 1011 4336 4243 2069 2907 1996 3484 1999 1996 3164 3323 2007 1996 2393 1997 1996 2521 1011 2187 2177 2452 1998 14592 2077 4465 1037 1055 10428 6592 2122 2967 2071 4558 2037 2907 2006 2373 2065 1037 2047 2602 2001 2170 1012 2174 2044 2824 2006 4465 2043 2610 7671 19817 4609 28099 2015 1998 5045 8903 10432 1999 1996 3257 1997 3262 9379 28337 2000 3844 2091 1996 9782 4013 1011 4336 4243 2089 2153 2663 1996 2602 1012 2107 1037 2765 2052 5333 13579 4193 3539 2704 22695 11948 6977 1037 1055 3754 2000 21208 1012 2493 5058 2007 28517 27266 3022 1006 13973 19802 25879 2923 9245 1007 3328 2083 1037 2395 2004 2027 7180 2000 5463 1037 6186 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 1031 7308 1033 11948 6977 1037 1055 6234 4559 1999 1996 2120 3323 14087 1996 3616 2000 2486 2032 2041 2083 1037 2053 1011 7023 3789 2021 2302 2892 1011 2283 2490 2006 16711 2002 2089 2022 3140 2000 2655 1037 10245 2120 2602 2370 1012 11948 6977 2001 5444 1999 2004 1037 7162 3003 3807 1999 2197 2095 1037 1055 2048 3864 1998 1037 25312 6593 6313 3323 6083 1037 10245 2602 2089 25732 1037 2714 2765 1012 7566 2493 2907 13973 19802 25879 2923 9245 2076 1037 10467 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 15769 10250 6767 2065 1996 13973 3323 3957 2039 1996 5245 2005 4336 11948 6977 2089 4133 2091 2007 16405 8004 3207 9629 2000 8691 2041 1037 2047 3066 2005 102\n",
            "I1124 21:34:08.865036 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:34:08.866297 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:34:08.867383 140629443692416 run_classifier.py:468] label: False_0 (id = 1)\n",
            "I1124 21:34:08.892292 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:34:08.893523 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:34:08.895509 140629443692416 run_classifier.py:464] tokens: [CLS] catalan regional police officers walk through a street covered with ballots for the banned independence referendum thrown by people outside people ' s party ( pp ) regional headquarters during a protest in barcelona spain october 3 2017 . reuters / jon na ##z ##ca madrid ( reuters ) - leaders of spain a s industrial ##ised northeastern region of catalonia said the regional population had voted for independence in a ballot on sunday that the central government said was illegal and non - representative . with 95 percent of the vote counted authorities said the a yes a vote stood at 90 . 1 percent albeit with a turnout of just a little over 40 percent - 2 . 26 million out of 5 . 34 million registered voters . on tuesday barcelona metro stations were closed picket ##s blocked main roads and civil servants walked out in response to a strike called by pro - independence groups . the stand - off could now take three different paths . tensions es ##cala ##te tgt ##con ##tin ##ues with tgt pledge to declare un ##ila ##tera ##l independence likely prompting the central government to trigger article 155 of the spanish constitution which would strip the region of tgt autonomous status and put tgt under direct madrid rule . if article 155 is triggered the next step would be snap elections in catalonia . pro - independence parties only hold the majority in the regional parliament with the help of the far - left group cup and polls before sunday a s ballot suggest these groups could lose their hold on power if a new election was called . however after events on sunday when police swung tr ##un ##cheon ##s and fired rubber bullets in the direction of mostly peaceful demonstrators to shut down the referendum pro - independence parties may again win the election . such a result would raise doubts surrounding prime minister mariano raj ##oy a s ability to govern . students wrapped with este ##lad ##as ( catalan sep ##arat ##ist flags ) walk through a street as they arrive to attend a protest two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / jon na ##z ##ca raj ##oy a s immediate opposition in the national parliament lacks the numbers to force him out through a no - confidence vote but without cross - party support on catalonia he may be forced to call a snap national election himself . raj ##oy was voted in as a minority leader twice in last year a s two elections and a fra ##ct ##ious parliament suggests a snap election may prompt a similar result . talks students hold catalan sep ##arat ##ist flags during a demonstration two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / [ mask ] if the catalan parliament gives up the push for independence raj ##oy may sit down with pu ##ig ##de ##mont to hammer out a new [SEP]\n",
            "I1124 21:34:08.896991 140629443692416 run_classifier.py:465] input_ids: 101 13973 3164 2610 3738 3328 2083 1037 2395 3139 2007 17069 2005 1996 7917 4336 9782 6908 2011 2111 2648 2111 1005 1055 2283 1006 4903 1007 3164 4075 2076 1037 6186 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 6921 1006 26665 1007 1011 4177 1997 3577 1037 1055 3919 5084 8763 2555 1997 16711 2056 1996 3164 2313 2018 5444 2005 4336 1999 1037 10428 2006 4465 2008 1996 2430 2231 2056 2001 6206 1998 2512 1011 4387 1012 2007 5345 3867 1997 1996 3789 8897 4614 2056 1996 1037 2748 1037 3789 2768 2012 3938 1012 1015 3867 12167 2007 1037 15512 1997 2074 1037 2210 2058 2871 3867 1011 1016 1012 2656 2454 2041 1997 1019 1012 4090 2454 5068 7206 1012 2006 9857 7623 6005 3703 2020 2701 28972 2015 8534 2364 4925 1998 2942 8858 2939 2041 1999 3433 2000 1037 4894 2170 2011 4013 1011 4336 2967 1012 1996 3233 1011 2125 2071 2085 2202 2093 2367 10425 1012 13136 9686 25015 2618 1 8663 7629 15808 2007 1 16393 2000 13520 4895 11733 14621 2140 4336 3497 15870 1996 2430 2231 2000 9495 3720 14168 1997 1996 3009 4552 2029 2052 6167 1996 2555 1997 1 8392 3570 1998 2404 1 2104 3622 6921 3627 1012 2065 3720 14168 2003 13330 1996 2279 3357 2052 2022 10245 3864 1999 16711 1012 4013 1011 4336 4243 2069 2907 1996 3484 1999 1996 3164 3323 2007 1996 2393 1997 1996 2521 1011 2187 2177 2452 1998 14592 2077 4465 1037 1055 10428 6592 2122 2967 2071 4558 2037 2907 2006 2373 2065 1037 2047 2602 2001 2170 1012 2174 2044 2824 2006 4465 2043 2610 7671 19817 4609 28099 2015 1998 5045 8903 10432 1999 1996 3257 1997 3262 9379 28337 2000 3844 2091 1996 9782 4013 1011 4336 4243 2089 2153 2663 1996 2602 1012 2107 1037 2765 2052 5333 13579 4193 3539 2704 22695 11948 6977 1037 1055 3754 2000 21208 1012 2493 5058 2007 28517 27266 3022 1006 13973 19802 25879 2923 9245 1007 3328 2083 1037 2395 2004 2027 7180 2000 5463 1037 6186 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 11948 6977 1037 1055 6234 4559 1999 1996 2120 3323 14087 1996 3616 2000 2486 2032 2041 2083 1037 2053 1011 7023 3789 2021 2302 2892 1011 2283 2490 2006 16711 2002 2089 2022 3140 2000 2655 1037 10245 2120 2602 2370 1012 11948 6977 2001 5444 1999 2004 1037 7162 3003 3807 1999 2197 2095 1037 1055 2048 3864 1998 1037 25312 6593 6313 3323 6083 1037 10245 2602 2089 25732 1037 2714 2765 1012 7566 2493 2907 13973 19802 25879 2923 9245 2076 1037 10467 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 1031 7308 1033 2065 1996 13973 3323 3957 2039 1996 5245 2005 4336 11948 6977 2089 4133 2091 2007 16405 8004 3207 9629 2000 8691 2041 1037 2047 102\n",
            "I1124 21:34:08.898299 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:34:08.899837 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:34:08.900984 140629443692416 run_classifier.py:468] label: False_0 (id = 1)\n",
            "I1124 21:34:08.924191 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:34:08.925421 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:34:08.928107 140629443692416 run_classifier.py:464] tokens: [CLS] catalan regional police officers walk through a street covered with ballots for the banned independence referendum thrown by people outside people ' s party ( pp ) regional headquarters during a protest in barcelona spain october 3 2017 . reuters / jon na ##z ##ca madrid ( reuters ) - leaders of spain a s industrial ##ised northeastern region of catalonia said the regional population had voted for independence in a ballot on sunday that the central government said was illegal and non - representative . with 95 percent of the vote counted authorities said the a yes a vote stood at 90 . 1 percent albeit with a turnout of just a little over 40 percent - 2 . 26 million out of 5 . 34 million registered voters . on tuesday barcelona metro stations were closed picket ##s blocked main roads and civil servants walked out in response to a strike called by pro - independence groups . the stand - off could now take three different paths . tensions es ##cala ##te tgt ##con ##tin ##ues with tgt pledge to declare un ##ila ##tera ##l independence likely prompting the central government to trigger article 155 of the spanish constitution which would strip the region of tgt autonomous status and put tgt under direct madrid rule . if article 155 is triggered the next step would be snap elections in catalonia . pro - independence parties only hold the majority in the regional parliament with the help of the far - left group cup and polls before sunday a s ballot suggest these groups could lose their hold on power if a new election was called . however after events on sunday when police swung tr ##un ##cheon ##s and fired rubber bullets in the direction of mostly peaceful demonstrators to shut down the referendum pro - independence parties may again win the election . such a result would raise doubts surrounding prime minister mariano raj ##oy a s ability to govern . students wrapped with este ##lad ##as ( catalan sep ##arat ##ist flags ) walk through a street as they arrive to attend a protest two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / jon na ##z ##ca raj ##oy a s immediate opposition in the national parliament lacks the numbers to force him out through a no - confidence vote but without cross - party support on catalonia he may be forced to call a snap national election himself . raj ##oy was voted in as a minority leader twice in last year a s two elections and a fra ##ct ##ious parliament suggests a snap election may prompt a similar result . talks students hold catalan sep ##arat ##ist flags during a demonstration two days after the banned independence referendum in barcelona spain october 3 2017 . reuters / enrique cal ##vo if the catalan parliament gives up the push for independence raj ##oy may sit down with [ mask ] to hammer out a new deal [SEP]\n",
            "I1124 21:34:08.929646 140629443692416 run_classifier.py:465] input_ids: 101 13973 3164 2610 3738 3328 2083 1037 2395 3139 2007 17069 2005 1996 7917 4336 9782 6908 2011 2111 2648 2111 1005 1055 2283 1006 4903 1007 3164 4075 2076 1037 6186 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 6921 1006 26665 1007 1011 4177 1997 3577 1037 1055 3919 5084 8763 2555 1997 16711 2056 1996 3164 2313 2018 5444 2005 4336 1999 1037 10428 2006 4465 2008 1996 2430 2231 2056 2001 6206 1998 2512 1011 4387 1012 2007 5345 3867 1997 1996 3789 8897 4614 2056 1996 1037 2748 1037 3789 2768 2012 3938 1012 1015 3867 12167 2007 1037 15512 1997 2074 1037 2210 2058 2871 3867 1011 1016 1012 2656 2454 2041 1997 1019 1012 4090 2454 5068 7206 1012 2006 9857 7623 6005 3703 2020 2701 28972 2015 8534 2364 4925 1998 2942 8858 2939 2041 1999 3433 2000 1037 4894 2170 2011 4013 1011 4336 2967 1012 1996 3233 1011 2125 2071 2085 2202 2093 2367 10425 1012 13136 9686 25015 2618 1 8663 7629 15808 2007 1 16393 2000 13520 4895 11733 14621 2140 4336 3497 15870 1996 2430 2231 2000 9495 3720 14168 1997 1996 3009 4552 2029 2052 6167 1996 2555 1997 1 8392 3570 1998 2404 1 2104 3622 6921 3627 1012 2065 3720 14168 2003 13330 1996 2279 3357 2052 2022 10245 3864 1999 16711 1012 4013 1011 4336 4243 2069 2907 1996 3484 1999 1996 3164 3323 2007 1996 2393 1997 1996 2521 1011 2187 2177 2452 1998 14592 2077 4465 1037 1055 10428 6592 2122 2967 2071 4558 2037 2907 2006 2373 2065 1037 2047 2602 2001 2170 1012 2174 2044 2824 2006 4465 2043 2610 7671 19817 4609 28099 2015 1998 5045 8903 10432 1999 1996 3257 1997 3262 9379 28337 2000 3844 2091 1996 9782 4013 1011 4336 4243 2089 2153 2663 1996 2602 1012 2107 1037 2765 2052 5333 13579 4193 3539 2704 22695 11948 6977 1037 1055 3754 2000 21208 1012 2493 5058 2007 28517 27266 3022 1006 13973 19802 25879 2923 9245 1007 3328 2083 1037 2395 2004 2027 7180 2000 5463 1037 6186 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 6285 6583 2480 3540 11948 6977 1037 1055 6234 4559 1999 1996 2120 3323 14087 1996 3616 2000 2486 2032 2041 2083 1037 2053 1011 7023 3789 2021 2302 2892 1011 2283 2490 2006 16711 2002 2089 2022 3140 2000 2655 1037 10245 2120 2602 2370 1012 11948 6977 2001 5444 1999 2004 1037 7162 3003 3807 1999 2197 2095 1037 1055 2048 3864 1998 1037 25312 6593 6313 3323 6083 1037 10245 2602 2089 25732 1037 2714 2765 1012 7566 2493 2907 13973 19802 25879 2923 9245 2076 1037 10467 2048 2420 2044 1996 7917 4336 9782 1999 7623 3577 2255 1017 2418 1012 26665 1013 15769 10250 6767 2065 1996 13973 3323 3957 2039 1996 5245 2005 4336 11948 6977 2089 4133 2091 2007 1031 7308 1033 2000 8691 2041 1037 2047 3066 102\n",
            "I1124 21:34:08.931037 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:34:08.932492 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:34:08.933598 140629443692416 run_classifier.py:468] label: False_0 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can not load train features, creating train features: [Errno 2] No such file or directory: 'train_joint_features_large'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1124 21:39:24.036262 140629443692416 run_classifier.py:774] Writing example 10000 of 45944\n",
            "I1124 21:41:38.132019 140629443692416 run_classifier.py:774] Writing example 20000 of 45944\n",
            "I1124 21:43:22.792792 140629443692416 run_classifier.py:774] Writing example 30000 of 45944\n",
            "I1124 21:44:38.843560 140629443692416 run_classifier.py:774] Writing example 40000 of 45944\n",
            "I1124 21:46:46.213128 140629443692416 run_classifier.py:774] Writing example 0 of 7187\n",
            "I1124 21:46:46.232873 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:46:46.234083 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:46:46.235904 140629443692416 run_classifier.py:464] tokens: [CLS] ' i came i saw i self ##ied ' : how ins ##tagram transformed the way we experience art en ##lar ##ge this image to ##ggle capt ##ion brendan sm ##ial ##owski / af ##p / get ##ty images brendan sm ##ial ##owski / af ##p / get ##ty images you are suspended in an endless dark chamber as thousands of red green yellow and blue lights flicker across the air like tiny diamonds in the sky . or at least that ' s how it appears in the self ##ie you just posted on ins ##tagram . ya ##yo ##i ku ##sam ##a ' s \" infinity mirrors \" a mirror - lined rooms that seem to go on forever a is part of the latest art cr ##az ##e to take over social media . im ##mers ##ive exhibits are driving people to museums in search of the perfect snaps ##hot . ariel ##le par ##des senior associate editor of technology and culture at wired tells here & now ' s jeremy ho ##bson that the explosion of \" made - for - ins ##tagram \" art exhibits is creating a market for art that is intended to be rein ##ter ##pre ##ted by people online . \" you can sort of skip that whole first part where you have an artistic process and an artistic critique of something and just go straight to the self ##ie by building a space that exists primarily for replication online \" she says . in a ted talk last year on \" art in the age of ins ##tagram \" jia ##jia [ mask ] director of digital at the jewish museum of new york highlights the as ##cend ##ance of \" ins ##tagram ##mable \" exhibitions since the social media platform launched in 2010 . \" you think of ya ##yo ##i ku ##sam ##a and her infinity mirrored room . . . and then artists like james tu ##rrell or the rain room at mom ##a \" she said in the ted talk . \" these are artists who really have very critical bodies of work but [ created installations ] that have taken on new meaning because of social media . \" many of the master ##mind ##s behind these exhibits admit that creating social - media - worthy art was a key goal . in san francisco the 12 000 - square - foot color factory features a pit filled with 207 000 plastic yellow balls a giant lit ##e - brit ##e game and a life - size coloring book . \" our goal was that every exhibit be three things \" founder jordan fern ##ey told the san francisco chronicle . \" it would be conceptual ; very photo ##genic and do well on social media ; and be an experience that you couldn ' t get anywhere else . \" across the country in washington d . c . the interactive \" sensory art experiences \" at arte ##cho ##use seem tailor - made [SEP]\n",
            "I1124 21:46:46.237144 140629443692416 run_classifier.py:465] input_ids: 101 1005 1045 2234 1045 2387 1045 2969 6340 1005 1024 2129 16021 23091 8590 1996 2126 2057 3325 2396 4372 8017 3351 2023 3746 2000 24679 14408 3258 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 2017 2024 6731 1999 2019 10866 2601 4574 2004 5190 1997 2417 2665 3756 1998 2630 4597 17909 2408 1996 2250 2066 4714 11719 1999 1996 3712 1012 2030 2012 2560 2008 1005 1055 2129 2009 3544 1999 1996 2969 2666 2017 2074 6866 2006 16021 23091 1012 8038 7677 2072 13970 21559 2050 1005 1055 1000 15579 13536 1000 1037 5259 1011 7732 4734 2008 4025 2000 2175 2006 5091 1037 2003 2112 1997 1996 6745 2396 13675 10936 2063 2000 2202 2058 2591 2865 1012 10047 16862 3512 10637 2024 4439 2111 2000 9941 1999 3945 1997 1996 3819 20057 12326 1012 16126 2571 11968 6155 3026 5482 3559 1997 2974 1998 3226 2012 17502 4136 2182 1004 2085 1005 1055 7441 7570 27355 2008 1996 7738 1997 1000 2081 1011 2005 1011 16021 23091 1000 2396 10637 2003 4526 1037 3006 2005 2396 2008 2003 3832 2000 2022 27788 3334 28139 3064 2011 2111 3784 1012 1000 2017 2064 4066 1997 13558 2008 2878 2034 2112 2073 2017 2031 2019 6018 2832 1998 2019 6018 16218 1997 2242 1998 2074 2175 3442 2000 1996 2969 2666 2011 2311 1037 2686 2008 6526 3952 2005 21647 3784 1000 2016 2758 1012 1999 1037 6945 2831 2197 2095 2006 1000 2396 1999 1996 2287 1997 16021 23091 1000 25871 26541 1031 7308 1033 2472 1997 3617 2012 1996 3644 2688 1997 2047 2259 11637 1996 2004 23865 6651 1997 1000 16021 23091 24088 1000 8596 2144 1996 2591 2865 4132 3390 1999 2230 1012 1000 2017 2228 1997 8038 7677 2072 13970 21559 2050 1998 2014 15579 22243 2282 1012 1012 1012 1998 2059 3324 2066 2508 10722 14069 2030 1996 4542 2282 2012 3566 2050 1000 2016 2056 1999 1996 6945 2831 1012 1000 2122 2024 3324 2040 2428 2031 2200 4187 4230 1997 2147 2021 1031 2580 14111 1033 2008 2031 2579 2006 2047 3574 2138 1997 2591 2865 1012 1000 2116 1997 1996 3040 23356 2015 2369 2122 10637 6449 2008 4526 2591 1011 2865 1011 11007 2396 2001 1037 3145 3125 1012 1999 2624 3799 1996 2260 2199 1011 2675 1011 3329 3609 4713 2838 1037 6770 3561 2007 19843 2199 6081 3756 7395 1037 5016 5507 2063 1011 28101 2063 2208 1998 1037 2166 1011 2946 22276 2338 1012 1000 2256 3125 2001 2008 2296 8327 2022 2093 2477 1000 3910 5207 20863 3240 2409 1996 2624 3799 9519 1012 1000 2009 2052 2022 17158 1025 2200 6302 16505 1998 2079 2092 2006 2591 2865 1025 1998 2022 2019 3325 2008 2017 2481 1005 1056 2131 5973 2842 1012 1000 2408 1996 2406 1999 2899 1040 1012 1039 1012 1996 9123 1000 16792 2396 6322 1000 2012 16185 9905 8557 4025 22701 1011 2081 102\n",
            "I1124 21:46:46.238259 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:46:46.239336 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:46:46.240267 140629443692416 run_classifier.py:468] label: False_1 (id = 0)\n",
            "I1124 21:46:46.264961 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:46:46.265978 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:46:46.267679 140629443692416 run_classifier.py:464] tokens: [CLS] ' i came i saw i self ##ied ' : how ins ##tagram transformed the way we experience art en ##lar ##ge this image to ##ggle capt ##ion brendan sm ##ial ##owski / af ##p / get ##ty images brendan sm ##ial ##owski / af ##p / get ##ty images you are suspended in an endless dark chamber as thousands of red green yellow and blue lights flicker across the air like tiny diamonds in the sky . or at least that ' s how it appears in the self ##ie you just posted on ins ##tagram . ya ##yo ##i ku ##sam ##a ' s \" infinity mirrors \" a mirror - lined rooms that seem to go on forever a is part of the latest art cr ##az ##e to take over social media . im ##mers ##ive exhibits are driving people to museums in search of the perfect snaps ##hot . ariel ##le par ##des senior associate editor of technology and culture at wired tells here & now ' s jeremy ho ##bson that the explosion of \" made - for - ins ##tagram \" art exhibits is creating a market for art that is intended to be rein ##ter ##pre ##ted by people online . \" you can sort of skip that whole first part where you have an artistic process and an artistic critique of something and just go straight to the self ##ie by building a space that exists primarily for replication online \" she says . in a ted talk last year on \" art in the age of ins ##tagram \" jia ##jia fei director of digital at the jewish museum of new york highlights the as ##cend ##ance of \" ins ##tagram ##mable \" exhibitions since the social media platform launched in 2010 . \" you think of ya ##yo ##i ku ##sam ##a and her infinity mirrored room . . . and then artists like james tu ##rrell or the rain room at mom ##a \" she said in the ted talk . \" these are artists who really have very critical bodies of work but [ created installations ] that have taken on new meaning because of social media . \" many of the master ##mind ##s behind these exhibits admit that creating social - media - worthy art was a key goal . in san francisco the 12 000 - square - foot color factory features a pit filled with 207 000 plastic yellow balls a giant lit ##e - brit ##e game and a life - size coloring book . \" our goal was that every exhibit be three things \" founder jordan fern ##ey told the san francisco chronicle . \" it would be conceptual ; very photo ##genic and do well on social media ; and be an experience that you couldn ' t get anywhere else . \" across the country in washington d . c . the interactive \" sensory art experiences \" at arte ##cho ##use seem tailor - made for ins [SEP]\n",
            "I1124 21:46:46.268836 140629443692416 run_classifier.py:465] input_ids: 101 1005 1045 2234 1045 2387 1045 2969 6340 1005 1024 2129 16021 23091 8590 1996 2126 2057 3325 2396 4372 8017 3351 2023 3746 2000 24679 14408 3258 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 2017 2024 6731 1999 2019 10866 2601 4574 2004 5190 1997 2417 2665 3756 1998 2630 4597 17909 2408 1996 2250 2066 4714 11719 1999 1996 3712 1012 2030 2012 2560 2008 1005 1055 2129 2009 3544 1999 1996 2969 2666 2017 2074 6866 2006 16021 23091 1012 8038 7677 2072 13970 21559 2050 1005 1055 1000 15579 13536 1000 1037 5259 1011 7732 4734 2008 4025 2000 2175 2006 5091 1037 2003 2112 1997 1996 6745 2396 13675 10936 2063 2000 2202 2058 2591 2865 1012 10047 16862 3512 10637 2024 4439 2111 2000 9941 1999 3945 1997 1996 3819 20057 12326 1012 16126 2571 11968 6155 3026 5482 3559 1997 2974 1998 3226 2012 17502 4136 2182 1004 2085 1005 1055 7441 7570 27355 2008 1996 7738 1997 1000 2081 1011 2005 1011 16021 23091 1000 2396 10637 2003 4526 1037 3006 2005 2396 2008 2003 3832 2000 2022 27788 3334 28139 3064 2011 2111 3784 1012 1000 2017 2064 4066 1997 13558 2008 2878 2034 2112 2073 2017 2031 2019 6018 2832 1998 2019 6018 16218 1997 2242 1998 2074 2175 3442 2000 1996 2969 2666 2011 2311 1037 2686 2008 6526 3952 2005 21647 3784 1000 2016 2758 1012 1999 1037 6945 2831 2197 2095 2006 1000 2396 1999 1996 2287 1997 16021 23091 1000 25871 26541 24664 2472 1997 3617 2012 1996 3644 2688 1997 2047 2259 11637 1996 2004 23865 6651 1997 1000 16021 23091 24088 1000 8596 2144 1996 2591 2865 4132 3390 1999 2230 1012 1000 2017 2228 1997 8038 7677 2072 13970 21559 2050 1998 2014 15579 22243 2282 1012 1012 1012 1998 2059 3324 2066 2508 10722 14069 2030 1996 4542 2282 2012 3566 2050 1000 2016 2056 1999 1996 6945 2831 1012 1000 2122 2024 3324 2040 2428 2031 2200 4187 4230 1997 2147 2021 1031 2580 14111 1033 2008 2031 2579 2006 2047 3574 2138 1997 2591 2865 1012 1000 2116 1997 1996 3040 23356 2015 2369 2122 10637 6449 2008 4526 2591 1011 2865 1011 11007 2396 2001 1037 3145 3125 1012 1999 2624 3799 1996 2260 2199 1011 2675 1011 3329 3609 4713 2838 1037 6770 3561 2007 19843 2199 6081 3756 7395 1037 5016 5507 2063 1011 28101 2063 2208 1998 1037 2166 1011 2946 22276 2338 1012 1000 2256 3125 2001 2008 2296 8327 2022 2093 2477 1000 3910 5207 20863 3240 2409 1996 2624 3799 9519 1012 1000 2009 2052 2022 17158 1025 2200 6302 16505 1998 2079 2092 2006 2591 2865 1025 1998 2022 2019 3325 2008 2017 2481 1005 1056 2131 5973 2842 1012 1000 2408 1996 2406 1999 2899 1040 1012 1039 1012 1996 9123 1000 16792 2396 6322 1000 2012 16185 9905 8557 4025 22701 1011 2081 2005 16021 102\n",
            "I1124 21:46:46.269933 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:46:46.270982 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:46:46.271869 140629443692416 run_classifier.py:468] label: True_1 (id = 4)\n",
            "I1124 21:46:46.296775 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:46:46.297755 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:46:46.299559 140629443692416 run_classifier.py:464] tokens: [CLS] ' i came i saw i self ##ied ' : how ins ##tagram transformed the way we experience art en ##lar ##ge this image to ##ggle capt ##ion brendan sm ##ial ##owski / af ##p / get ##ty images brendan sm ##ial ##owski / af ##p / get ##ty images you are suspended in an endless dark chamber as thousands of red green yellow and blue lights flicker across the air like tiny diamonds in the sky . or at least that ' s how it appears in the self ##ie you just posted on ins ##tagram . ya ##yo ##i ku ##sam ##a ' s \" infinity mirrors \" a mirror - lined rooms that seem to go on forever a is part of the latest art cr ##az ##e to take over social media . im ##mers ##ive exhibits are driving people to museums in search of the perfect snaps ##hot . ariel ##le par ##des senior associate editor of technology and culture at wired tells here & now ' s [ mask ] that the explosion of \" made - for - ins ##tagram \" art exhibits is creating a market for art that is intended to be rein ##ter ##pre ##ted by people online . \" you can sort of skip that whole first part where you have an artistic process and an artistic critique of something and just go straight to the self ##ie by building a space that exists primarily for replication online \" she says . in a ted talk last year on \" art in the age of ins ##tagram \" jia ##jia fei director of digital at the jewish museum of new york highlights the as ##cend ##ance of \" ins ##tagram ##mable \" exhibitions since the social media platform launched in 2010 . \" you think of ya ##yo ##i ku ##sam ##a and her infinity mirrored room . . . and then artists like james tu ##rrell or the rain room at mom ##a \" she said in the ted talk . \" these are artists who really have very critical bodies of work but [ created installations ] that have taken on new meaning because of social media . \" many of the master ##mind ##s behind these exhibits admit that creating social - media - worthy art was a key goal . in san francisco the 12 000 - square - foot color factory features a pit filled with 207 000 plastic yellow balls a giant lit ##e - brit ##e game and a life - size coloring book . \" our goal was that every exhibit be three things \" founder jordan fern ##ey told the san francisco chronicle . \" it would be conceptual ; very photo ##genic and do well on social media ; and be an experience that you couldn ' t get anywhere else . \" across the country in washington d . c . the interactive \" sensory art experiences \" at arte ##cho ##use seem tailor - made for ins [SEP]\n",
            "I1124 21:46:46.300801 140629443692416 run_classifier.py:465] input_ids: 101 1005 1045 2234 1045 2387 1045 2969 6340 1005 1024 2129 16021 23091 8590 1996 2126 2057 3325 2396 4372 8017 3351 2023 3746 2000 24679 14408 3258 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 2017 2024 6731 1999 2019 10866 2601 4574 2004 5190 1997 2417 2665 3756 1998 2630 4597 17909 2408 1996 2250 2066 4714 11719 1999 1996 3712 1012 2030 2012 2560 2008 1005 1055 2129 2009 3544 1999 1996 2969 2666 2017 2074 6866 2006 16021 23091 1012 8038 7677 2072 13970 21559 2050 1005 1055 1000 15579 13536 1000 1037 5259 1011 7732 4734 2008 4025 2000 2175 2006 5091 1037 2003 2112 1997 1996 6745 2396 13675 10936 2063 2000 2202 2058 2591 2865 1012 10047 16862 3512 10637 2024 4439 2111 2000 9941 1999 3945 1997 1996 3819 20057 12326 1012 16126 2571 11968 6155 3026 5482 3559 1997 2974 1998 3226 2012 17502 4136 2182 1004 2085 1005 1055 1031 7308 1033 2008 1996 7738 1997 1000 2081 1011 2005 1011 16021 23091 1000 2396 10637 2003 4526 1037 3006 2005 2396 2008 2003 3832 2000 2022 27788 3334 28139 3064 2011 2111 3784 1012 1000 2017 2064 4066 1997 13558 2008 2878 2034 2112 2073 2017 2031 2019 6018 2832 1998 2019 6018 16218 1997 2242 1998 2074 2175 3442 2000 1996 2969 2666 2011 2311 1037 2686 2008 6526 3952 2005 21647 3784 1000 2016 2758 1012 1999 1037 6945 2831 2197 2095 2006 1000 2396 1999 1996 2287 1997 16021 23091 1000 25871 26541 24664 2472 1997 3617 2012 1996 3644 2688 1997 2047 2259 11637 1996 2004 23865 6651 1997 1000 16021 23091 24088 1000 8596 2144 1996 2591 2865 4132 3390 1999 2230 1012 1000 2017 2228 1997 8038 7677 2072 13970 21559 2050 1998 2014 15579 22243 2282 1012 1012 1012 1998 2059 3324 2066 2508 10722 14069 2030 1996 4542 2282 2012 3566 2050 1000 2016 2056 1999 1996 6945 2831 1012 1000 2122 2024 3324 2040 2428 2031 2200 4187 4230 1997 2147 2021 1031 2580 14111 1033 2008 2031 2579 2006 2047 3574 2138 1997 2591 2865 1012 1000 2116 1997 1996 3040 23356 2015 2369 2122 10637 6449 2008 4526 2591 1011 2865 1011 11007 2396 2001 1037 3145 3125 1012 1999 2624 3799 1996 2260 2199 1011 2675 1011 3329 3609 4713 2838 1037 6770 3561 2007 19843 2199 6081 3756 7395 1037 5016 5507 2063 1011 28101 2063 2208 1998 1037 2166 1011 2946 22276 2338 1012 1000 2256 3125 2001 2008 2296 8327 2022 2093 2477 1000 3910 5207 20863 3240 2409 1996 2624 3799 9519 1012 1000 2009 2052 2022 17158 1025 2200 6302 16505 1998 2079 2092 2006 2591 2865 1025 1998 2022 2019 3325 2008 2017 2481 1005 1056 2131 5973 2842 1012 1000 2408 1996 2406 1999 2899 1040 1012 1039 1012 1996 9123 1000 16792 2396 6322 1000 2012 16185 9905 8557 4025 22701 1011 2081 2005 16021 102\n",
            "I1124 21:46:46.301876 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:46:46.302922 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:46:46.303862 140629443692416 run_classifier.py:468] label: False_1 (id = 0)\n",
            "I1124 21:46:46.328193 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:46:46.329184 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:46:46.330941 140629443692416 run_classifier.py:464] tokens: [CLS] ' i came i saw i self ##ied ' : how ins ##tagram transformed the way we experience art en ##lar ##ge this image to ##ggle capt ##ion brendan sm ##ial ##owski / af ##p / get ##ty images brendan sm ##ial ##owski / af ##p / get ##ty images you are suspended in an endless dark chamber as thousands of red green yellow and blue lights flicker across the air like tiny diamonds in the sky . or at least that ' s how it appears in the self ##ie you just posted on ins ##tagram . [ mask ] ' s \" infinity mirrors \" a mirror - lined rooms that seem to go on forever a is part of the latest art cr ##az ##e to take over social media . im ##mers ##ive exhibits are driving people to museums in search of the perfect snaps ##hot . ariel ##le par ##des senior associate editor of technology and culture at wired tells here & now ' s jeremy ho ##bson that the explosion of \" made - for - ins ##tagram \" art exhibits is creating a market for art that is intended to be rein ##ter ##pre ##ted by people online . \" you can sort of skip that whole first part where you have an artistic process and an artistic critique of something and just go straight to the self ##ie by building a space that exists primarily for replication online \" she says . in a ted talk last year on \" art in the age of ins ##tagram \" jia ##jia fei director of digital at the jewish museum of new york highlights the as ##cend ##ance of \" ins ##tagram ##mable \" exhibitions since the social media platform launched in 2010 . \" you think of [ mask ] and her infinity mirrored room . . . and then artists like james tu ##rrell or the rain room at mom ##a \" she said in the ted talk . \" these are artists who really have very critical bodies of work but [ created installations ] that have taken on new meaning because of social media . \" many of the master ##mind ##s behind these exhibits admit that creating social - media - worthy art was a key goal . in san francisco the 12 000 - square - foot color factory features a pit filled with 207 000 plastic yellow balls a giant lit ##e - brit ##e game and a life - size coloring book . \" our goal was that every exhibit be three things \" founder jordan fern ##ey told the san francisco chronicle . \" it would be conceptual ; very photo ##genic and do well on social media ; and be an experience that you couldn ' t get anywhere else . \" across the country in washington d . c . the interactive \" sensory art experiences \" at arte ##cho ##use seem tailor - made for ins ##tagram . but the art and [SEP]\n",
            "I1124 21:46:46.332175 140629443692416 run_classifier.py:465] input_ids: 101 1005 1045 2234 1045 2387 1045 2969 6340 1005 1024 2129 16021 23091 8590 1996 2126 2057 3325 2396 4372 8017 3351 2023 3746 2000 24679 14408 3258 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 15039 15488 4818 15249 1013 21358 2361 1013 2131 3723 4871 2017 2024 6731 1999 2019 10866 2601 4574 2004 5190 1997 2417 2665 3756 1998 2630 4597 17909 2408 1996 2250 2066 4714 11719 1999 1996 3712 1012 2030 2012 2560 2008 1005 1055 2129 2009 3544 1999 1996 2969 2666 2017 2074 6866 2006 16021 23091 1012 1031 7308 1033 1005 1055 1000 15579 13536 1000 1037 5259 1011 7732 4734 2008 4025 2000 2175 2006 5091 1037 2003 2112 1997 1996 6745 2396 13675 10936 2063 2000 2202 2058 2591 2865 1012 10047 16862 3512 10637 2024 4439 2111 2000 9941 1999 3945 1997 1996 3819 20057 12326 1012 16126 2571 11968 6155 3026 5482 3559 1997 2974 1998 3226 2012 17502 4136 2182 1004 2085 1005 1055 7441 7570 27355 2008 1996 7738 1997 1000 2081 1011 2005 1011 16021 23091 1000 2396 10637 2003 4526 1037 3006 2005 2396 2008 2003 3832 2000 2022 27788 3334 28139 3064 2011 2111 3784 1012 1000 2017 2064 4066 1997 13558 2008 2878 2034 2112 2073 2017 2031 2019 6018 2832 1998 2019 6018 16218 1997 2242 1998 2074 2175 3442 2000 1996 2969 2666 2011 2311 1037 2686 2008 6526 3952 2005 21647 3784 1000 2016 2758 1012 1999 1037 6945 2831 2197 2095 2006 1000 2396 1999 1996 2287 1997 16021 23091 1000 25871 26541 24664 2472 1997 3617 2012 1996 3644 2688 1997 2047 2259 11637 1996 2004 23865 6651 1997 1000 16021 23091 24088 1000 8596 2144 1996 2591 2865 4132 3390 1999 2230 1012 1000 2017 2228 1997 1031 7308 1033 1998 2014 15579 22243 2282 1012 1012 1012 1998 2059 3324 2066 2508 10722 14069 2030 1996 4542 2282 2012 3566 2050 1000 2016 2056 1999 1996 6945 2831 1012 1000 2122 2024 3324 2040 2428 2031 2200 4187 4230 1997 2147 2021 1031 2580 14111 1033 2008 2031 2579 2006 2047 3574 2138 1997 2591 2865 1012 1000 2116 1997 1996 3040 23356 2015 2369 2122 10637 6449 2008 4526 2591 1011 2865 1011 11007 2396 2001 1037 3145 3125 1012 1999 2624 3799 1996 2260 2199 1011 2675 1011 3329 3609 4713 2838 1037 6770 3561 2007 19843 2199 6081 3756 7395 1037 5016 5507 2063 1011 28101 2063 2208 1998 1037 2166 1011 2946 22276 2338 1012 1000 2256 3125 2001 2008 2296 8327 2022 2093 2477 1000 3910 5207 20863 3240 2409 1996 2624 3799 9519 1012 1000 2009 2052 2022 17158 1025 2200 6302 16505 1998 2079 2092 2006 2591 2865 1025 1998 2022 2019 3325 2008 2017 2481 1005 1056 2131 5973 2842 1012 1000 2408 1996 2406 1999 2899 1040 1012 1039 1012 1996 9123 1000 16792 2396 6322 1000 2012 16185 9905 8557 4025 22701 1011 2081 2005 16021 23091 1012 2021 1996 2396 1998 102\n",
            "I1124 21:46:46.333236 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:46:46.334283 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:46:46.335203 140629443692416 run_classifier.py:468] label: False_1 (id = 0)\n",
            "I1124 21:46:46.359550 140629443692416 run_classifier.py:461] *** Example ***\n",
            "I1124 21:46:46.360569 140629443692416 run_classifier.py:462] guid: None\n",
            "I1124 21:46:46.362354 140629443692416 run_classifier.py:464] tokens: [CLS] ' i came i saw i self ##ied ' : how ins ##tagram transformed the way we experience art en ##lar ##ge this image to ##ggle capt ##ion [ mask ] / af ##p / get ##ty images [ mask ] / af ##p / get ##ty images you are suspended in an endless dark chamber as thousands of red green yellow and blue lights flicker across the air like tiny diamonds in the sky . or at least that ' s how it appears in the self ##ie you just posted on ins ##tagram . ya ##yo ##i ku ##sam ##a ' s \" infinity mirrors \" a mirror - lined rooms that seem to go on forever a is part of the latest art cr ##az ##e to take over social media . im ##mers ##ive exhibits are driving people to museums in search of the perfect snaps ##hot . ariel ##le par ##des senior associate editor of technology and culture at wired tells here & now ' s jeremy ho ##bson that the explosion of \" made - for - ins ##tagram \" art exhibits is creating a market for art that is intended to be rein ##ter ##pre ##ted by people online . \" you can sort of skip that whole first part where you have an artistic process and an artistic critique of something and just go straight to the self ##ie by building a space that exists primarily for replication online \" she says . in a ted talk last year on \" art in the age of ins ##tagram \" jia ##jia fei director of digital at the jewish museum of new york highlights the as ##cend ##ance of \" ins ##tagram ##mable \" exhibitions since the social media platform launched in 2010 . \" you think of ya ##yo ##i ku ##sam ##a and her infinity mirrored room . . . and then artists like james tu ##rrell or the rain room at mom ##a \" she said in the ted talk . \" these are artists who really have very critical bodies of work but [ created installations ] that have taken on new meaning because of social media . \" many of the master ##mind ##s behind these exhibits admit that creating social - media - worthy art was a key goal . in san francisco the 12 000 - square - foot color factory features a pit filled with 207 000 plastic yellow balls a giant lit ##e - brit ##e game and a life - size coloring book . \" our goal was that every exhibit be three things \" founder jordan fern ##ey told the san francisco chronicle . \" it would be conceptual ; very photo ##genic and do well on social media ; and be an experience that you couldn ' t get anywhere else . \" across the country in washington d . c . the interactive \" sensory art experiences \" at arte ##cho ##use seem tailor - made for ins ##tagram . [SEP]\n",
            "I1124 21:46:46.363622 140629443692416 run_classifier.py:465] input_ids: 101 1005 1045 2234 1045 2387 1045 2969 6340 1005 1024 2129 16021 23091 8590 1996 2126 2057 3325 2396 4372 8017 3351 2023 3746 2000 24679 14408 3258 1031 7308 1033 1013 21358 2361 1013 2131 3723 4871 1031 7308 1033 1013 21358 2361 1013 2131 3723 4871 2017 2024 6731 1999 2019 10866 2601 4574 2004 5190 1997 2417 2665 3756 1998 2630 4597 17909 2408 1996 2250 2066 4714 11719 1999 1996 3712 1012 2030 2012 2560 2008 1005 1055 2129 2009 3544 1999 1996 2969 2666 2017 2074 6866 2006 16021 23091 1012 8038 7677 2072 13970 21559 2050 1005 1055 1000 15579 13536 1000 1037 5259 1011 7732 4734 2008 4025 2000 2175 2006 5091 1037 2003 2112 1997 1996 6745 2396 13675 10936 2063 2000 2202 2058 2591 2865 1012 10047 16862 3512 10637 2024 4439 2111 2000 9941 1999 3945 1997 1996 3819 20057 12326 1012 16126 2571 11968 6155 3026 5482 3559 1997 2974 1998 3226 2012 17502 4136 2182 1004 2085 1005 1055 7441 7570 27355 2008 1996 7738 1997 1000 2081 1011 2005 1011 16021 23091 1000 2396 10637 2003 4526 1037 3006 2005 2396 2008 2003 3832 2000 2022 27788 3334 28139 3064 2011 2111 3784 1012 1000 2017 2064 4066 1997 13558 2008 2878 2034 2112 2073 2017 2031 2019 6018 2832 1998 2019 6018 16218 1997 2242 1998 2074 2175 3442 2000 1996 2969 2666 2011 2311 1037 2686 2008 6526 3952 2005 21647 3784 1000 2016 2758 1012 1999 1037 6945 2831 2197 2095 2006 1000 2396 1999 1996 2287 1997 16021 23091 1000 25871 26541 24664 2472 1997 3617 2012 1996 3644 2688 1997 2047 2259 11637 1996 2004 23865 6651 1997 1000 16021 23091 24088 1000 8596 2144 1996 2591 2865 4132 3390 1999 2230 1012 1000 2017 2228 1997 8038 7677 2072 13970 21559 2050 1998 2014 15579 22243 2282 1012 1012 1012 1998 2059 3324 2066 2508 10722 14069 2030 1996 4542 2282 2012 3566 2050 1000 2016 2056 1999 1996 6945 2831 1012 1000 2122 2024 3324 2040 2428 2031 2200 4187 4230 1997 2147 2021 1031 2580 14111 1033 2008 2031 2579 2006 2047 3574 2138 1997 2591 2865 1012 1000 2116 1997 1996 3040 23356 2015 2369 2122 10637 6449 2008 4526 2591 1011 2865 1011 11007 2396 2001 1037 3145 3125 1012 1999 2624 3799 1996 2260 2199 1011 2675 1011 3329 3609 4713 2838 1037 6770 3561 2007 19843 2199 6081 3756 7395 1037 5016 5507 2063 1011 28101 2063 2208 1998 1037 2166 1011 2946 22276 2338 1012 1000 2256 3125 2001 2008 2296 8327 2022 2093 2477 1000 3910 5207 20863 3240 2409 1996 2624 3799 9519 1012 1000 2009 2052 2022 17158 1025 2200 6302 16505 1998 2079 2092 2006 2591 2865 1025 1998 2022 2019 3325 2008 2017 2481 1005 1056 2131 5973 2842 1012 1000 2408 1996 2406 1999 2899 1040 1012 1039 1012 1996 9123 1000 16792 2396 6322 1000 2012 16185 9905 8557 4025 22701 1011 2081 2005 16021 23091 1012 102\n",
            "I1124 21:46:46.364732 140629443692416 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1124 21:46:46.365766 140629443692416 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1124 21:46:46.366731 140629443692416 run_classifier.py:468] label: False_1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can not load from GC, creating dev features\n",
            "45944\n",
            "7187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnRhr2sAzrYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = train_joint['joint_label']\n",
        "test_label = random_test_joint['joint_label']\n",
        "test_fixed_label = fixed_test_joint['joint_label']\n",
        "dev_label = dev_joint['joint_label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FUAfD2tNnYx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "629492f1-40e4-45f2-f244-45a63e3b174a"
      },
      "source": [
        "len(set(test_fixed_label))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6TrW6rjlZZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.head(500).to_csv(open('train_500.csv','w'),encoding='latin-1')\n",
        "data_train = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2x0zox7vWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_selective -f '\\brandom_test_joint\\b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz3_JA2X6eAc",
        "colab_type": "code",
        "outputId": "96aaaea2-b8d0-409d-978b-40f305d6b7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# reset_selective -f data_train\n",
        "# who_ls\n",
        "import sys\n",
        "ipython_vars =[]# ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
        "\n",
        "\n",
        "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('data_test', 117534962),\n",
              " ('test', 117469922),\n",
              " ('dev_joint', 71121300),\n",
              " ('data_dev', 70042071),\n",
              " ('dev', 69984575),\n",
              " ('train_df', 60479996),\n",
              " ('train_doc', 28013976),\n",
              " ('test_fixed_df', 16047828),\n",
              " ('test_random_df', 10887664),\n",
              " ('dev_ent_df', 9664744),\n",
              " ('dev_df', 9655496),\n",
              " ('test_fixed_doc', 7478148),\n",
              " ('test_doc', 5052140),\n",
              " ('dev_ent', 4447104),\n",
              " ('train_label', 2732054),\n",
              " ('train_joint_InputExamples', 2205344),\n",
              " ('test_joint_InputExamples_fixed', 605024),\n",
              " ('test_label', 483662),\n",
              " ('train_joint_features', 406504),\n",
              " ('test_joint_InputExamples', 390272),\n",
              " ('dev_joint_InputExamples', 345008),\n",
              " ('lines', 253640),\n",
              " ('test_fixed_label', 113468),\n",
              " ('dev_label', 64715),\n",
              " ('dev_joint_features', 61440),\n",
              " ('auth_info', 1048),\n",
              " ('AdamWeightDecayOptimizer', 904),\n",
              " ('Out', 280),\n",
              " ('In', 272),\n",
              " ('f', 144),\n",
              " ('label_list', 144),\n",
              " ('data_pref', 123),\n",
              " ('create_optimizer', 120),\n",
              " ('fix_doc_tgt', 120),\n",
              " ('load_dataset', 120),\n",
              " ('load_directory_data', 120),\n",
              " ('load_file', 120),\n",
              " ('load_paragraphs_documents', 120),\n",
              " ('CONFIG_FILE', 109),\n",
              " ('INIT_CHECKPOINT', 108),\n",
              " ('VOCAB_FILE', 97),\n",
              " ('path', 97),\n",
              " ('e', 96),\n",
              " ('OUTPUT_DIR', 95),\n",
              " ('BERT_MODEL_HUB', 92),\n",
              " ('BERT_PRETRAINED_DIR', 92),\n",
              " ('get_ipython', 80),\n",
              " ('absolute_import', 72),\n",
              " ('division', 72),\n",
              " ('ipython_vars', 72),\n",
              " ('print_function', 72),\n",
              " ('exit', 64),\n",
              " ('f_in', 64),\n",
              " ('f_out', 64),\n",
              " ('quit', 64),\n",
              " ('session', 64),\n",
              " ('tf', 64),\n",
              " ('tokenizer', 64),\n",
              " ('tpu_cluster_resolver', 64),\n",
              " ('TPU_ADDRESS', 63),\n",
              " ('BERT_MODEL', 60),\n",
              " ('auth', 56),\n",
              " ('hub', 56),\n",
              " ('modeling', 56),\n",
              " ('pd', 56),\n",
              " ('run_classifier', 56),\n",
              " ('run_classifier_with_tfhub', 56),\n",
              " ('tokenization', 56),\n",
              " ('BUCKET', 49),\n",
              " ('LABEL_COLUMN', 48),\n",
              " ('DATA_COLUMN', 45),\n",
              " ('line', 43),\n",
              " ('x', 38),\n",
              " ('DO_LOWER_CASE', 24),\n",
              " ('EVAL_BATCH_SIZE', 24),\n",
              " ('ITERATIONS_PER_LOOP', 24),\n",
              " ('LEARNING_RATE', 24),\n",
              " ('MAX_SEQ_LENGTH', 24),\n",
              " ('NUM_TPU_CORES', 24),\n",
              " ('NUM_TRAIN_EPOCHS', 24),\n",
              " ('PREDICT_BATCH_SIZE', 24),\n",
              " ('SAVE_CHECKPOINTS_STEPS', 24),\n",
              " ('SAVE_SUMMARY_STEPS', 24),\n",
              " ('TRAIN_BATCH_SIZE', 24),\n",
              " ('WARMUP_PROPORTION', 24),\n",
              " ('num_train_steps', 24),\n",
              " ('num_warmup_steps', 24),\n",
              " ('use_tpu', 24)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Sj7di-oaUB",
        "colab_type": "code",
        "outputId": "4db46fdb-5d14-4132-9333-5a15c34e7679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxSg4FBwkmyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c2501a2-6f5d-4885-80bb-eef17cca8bbe"
      },
      "source": [
        "True_ng = (len(dev_joint[dev_joint['joint_label']=='True_-1']))\n",
        "True_nu = (len(dev_joint[dev_joint['joint_label']=='True_0']))\n",
        "True_po = (len(dev_joint[dev_joint['joint_label']=='True_1']))\n",
        "False_ng = (len(dev_joint[dev_joint['joint_label']=='False_-1']))\n",
        "False_nu = (len(dev_joint[dev_joint['joint_label']=='False_0']))\n",
        "False_po = (len(dev_joint[dev_joint['joint_label']=='False_1']))\n",
        "print(label_list)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['False_1', 'False_0', 'False_-1', 'True_-1', 'True_1', 'True_0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_3OgSqGUPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# class_weights_arr = [neg_w/(neg_w+pos_w),pos_w/(neg_w+pos_w)]\n",
        "# class_weights_arr = [0.6,0.4]\n",
        "\n",
        "# sent_proj = [[0,0,1],[0,1,0],[1,0,0],[1,0,0],[0,0,1],[0,1,0]]\n",
        "# mask_proj = [[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]]\n",
        "trainable = True\n",
        "# print(class_weights_arr)\n",
        "# class_weights_arr = [0.5,0.5]\n",
        "def create_model(bert_config,is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels_mask,num_labels_sent,\n",
        "                 use_one_hot_embeddings):#, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "    \n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB, tags=tags, trainable=trainable)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "\n",
        "  #### mask lm task\n",
        "  # print('number of labels mask: ' , num_labels_mask)\n",
        "\n",
        "  # output_weights_mask = tf.get_variable(\n",
        "  #     \"output_weights\", [num_labels_mask, hidden_size],\n",
        "  #     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  # output_bias_mask = tf.get_variable(\n",
        "  #     \"output_bias\", [num_labels_mask], initializer=tf.zeros_initializer())\n",
        "\n",
        "  output_weights_mask = tf.get_variable(\n",
        "      \"output_weights\", [num_labels_mask, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias_mask = tf.get_variable(\n",
        "      \"output_bias\", [num_labels_mask], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits_mask = tf.matmul(output_layer, output_weights_mask, transpose_b=True)\n",
        "    logits_mask = tf.nn.bias_add(logits_mask, output_bias_mask)\n",
        "    probabilities_mask = tf.nn.softmax(logits_mask, axis=-1)\n",
        "    log_probs_mask = tf.nn.log_softmax(logits_mask, axis=-1)\n",
        "    print(labels.get_shape())\n",
        "    # print(labels_mask.get_shape())\n",
        "    # mask_proj_arr = tf.constant(mask_proj)\n",
        "    mask_proj_arr = tf.constant([1,0,1,0,1,0,0,1,0,1,0,1], shape=[num_labels_mask*num_labels_sent, num_labels_mask], dtype=tf.float32)\n",
        "\n",
        "    print(mask_proj_arr.get_shape())\n",
        "    one_hot_labels_mask = tf.one_hot(labels, depth=num_labels_mask*num_labels_sent, dtype=tf.float32)\n",
        "    one_hot_labels_mask = tf.matmul(one_hot_labels_mask,mask_proj_arr)\n",
        "    print(one_hot_labels_mask.get_shape())\n",
        "\n",
        "    per_example_loss_mask = -tf.reduce_sum(one_hot_labels_mask * log_probs_mask, axis=-1)\n",
        "    loss_mask = tf.reduce_mean(per_example_loss_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #### sentiment task\n",
        "    # print('number of labels sent: ' , num_labels_sent)\n",
        "\n",
        "    # output_weights_sent = tf.get_variable(\n",
        "    #     \"output_weights\", [num_labels_sent, hidden_size],\n",
        "    #     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "    # output_bias_sent = tf.get_variable(\n",
        "    #     \"output_bias\", [num_labels_sent], initializer=tf.zeros_initializer())\n",
        "\n",
        "    output_weights_sent = tf.get_variable(\n",
        "        \"output_weights\", [num_labels_sent, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "    output_bias_sent = tf.get_variable(\n",
        "        \"output_bias\", [num_labels_sent], initializer=tf.zeros_initializer())\n",
        "\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "      if is_training:\n",
        "        # I.e., 0.1 dropout\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits_sent = tf.matmul(output_layer, output_weights_sent, transpose_b=True)\n",
        "    logits_sent = tf.nn.bias_add(logits_sent, output_bias_sent)\n",
        "    probabilities_sent = tf.nn.softmax(logits_sent, axis=-1)\n",
        "    log_probs_sent = tf.nn.log_softmax(logits_sent, axis=-1)\n",
        "\n",
        "    # one_hot_labels_sent = tf.one_hot(labels_sent, depth=num_labels_sent, dtype=tf.float32)\n",
        "    sent_proj_arr = tf.constant([0,0,1,0,1,0,1,0,0,1,0,0,0,0,1,0,1,0], shape=[num_labels_mask*num_labels_sent, num_labels_sent], dtype=tf.float32)\n",
        "    one_hot_labels_sent = tf.one_hot(labels, depth=num_labels_mask*num_labels_sent, dtype=tf.float32)\n",
        "    one_hot_labels_sent = tf.matmul(one_hot_labels_sent,sent_proj_arr)\n",
        "    print(one_hot_labels_sent.get_shape())\n",
        "\n",
        "    per_example_loss_sent = -tf.reduce_sum(one_hot_labels_sent * log_probs_sent, axis=-1)\n",
        "    loss_sent = tf.reduce_mean(per_example_loss_sent)\n",
        "\n",
        "# ValueError: Shape must be rank 2 but is rank 1 for 'loss/MatMul_1' (op: 'MatMul') with input shapes: [2], [6,2].\n",
        "    total_loss = loss_sent + loss_mask\n",
        "    total_per_example_loss = per_example_loss_sent + per_example_loss_mask\n",
        "\n",
        "       \n",
        "    return (total_loss,  per_example_loss_mask,per_example_loss_sent, logits_sent,logits_mask, probabilities_sent,probabilities_mask)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config,\n",
        "                     num_labels_mask,num_labels_sent,\n",
        "                     init_checkpoint, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu,use_one_hot_embeddings):# bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    print('label_ids ar: ', label_ids)\n",
        "    print('label ids shape: , ' ,label_ids.get_shape())\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss_mask,per_example_loss_sent, logits_sent,logits_mask, probabilities_sent,probabilities_mask) =\\\n",
        "    create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\\\n",
        "        num_labels_mask,num_labels_sent,\n",
        "        use_one_hot_embeddings)\n",
        "\n",
        "    \n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "     \n",
        "      train_op = create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss_sent, label_ids_sent, logits_sent, is_real_example):\n",
        "        predictions_sent = tf.argmax(logits_sent, axis=-1, output_type=tf.int32)\n",
        "        accuracy_sent = tf.metrics.accuracy(label_ids_sent, predictions_sent,weights=is_real_example)\n",
        "        loss_sent = tf.metrics.mean(per_example_loss_sent,weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy_sent,\n",
        "            \"eval_loss\": loss_sent,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss_sent, label_ids_sent, logits_sent])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics,scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities_sent},scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_joint_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_joint_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "### Activate it if ** part is not activated \n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels_mask=2,\n",
        "    num_labels_sent=3,\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True\n",
        "#   bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=get_run_config(OUTPUT_DIR),\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE, \n",
        "# )\n",
        "# #####################################################################\n",
        "## No Error\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    keep_checkpoint_max=15,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "# model_fn = run_classifier.model_fn_builder(\n",
        "#     bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#     num_labels=len(label_list),\n",
        "#     init_checkpoint=INIT_CHECKPOINT,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     num_train_steps=num_train_steps,\n",
        "#     num_warmup_steps=num_warmup_steps,\n",
        "#     use_tpu=use_tpu,\n",
        "#     use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "\n",
        "# estimator_from_tfhub._export_to_tpu = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "def model_train(estimator,train_features=train_joint_features):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_joint_features)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_joint_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  print('start running estimator')\n",
        "#   estimator._export_to_tpu = False\n",
        "  md = estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "  return md\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator,eval_examples=None,eval_features=dev_joint_features):\n",
        "  # Eval the model.\n",
        "#   eval_examples = dev_InputExamples#processor.get_dev_examples(TASK_DATA_DIR)\n",
        "#   eval_features = run_classifier.convert_examples_to_features(\n",
        "#       eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "#   with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "  print(\"***** Eval results *****\")\n",
        "  for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "#       writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "      \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# labels = [1,0]\n",
        "# def model_predict(estimator,prediction_examples):\n",
        "#   # Make predictions on a subset of eval examples\n",
        "# #   prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "#   input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#   predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "#   predictions = estimator.predict(predict_input_fn)\n",
        "#   return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n",
        "def model_predict(estimator,input_features,input_examples,checkpoint_path=None):\n",
        "  # Make predictions on a subset of eval examples\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmU7E5v1-k0",
        "colab_type": "code",
        "outputId": "778d7434-0b1d-42fc-df96-a4246ead1b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "model_train(estimator,train_features=train_joint_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started training at 2019-11-25 00:13:19.236149 *****\n",
            "  Num examples = 45944\n",
            "  Batch size = 16\n",
            "start running estimator\n",
            "label_ids ar:  Tensor(\"InfeedQueue/dequeue:2\", shape=(2,), dtype=int32, device=/device:TPU_REPLICATED_CORE:0)\n",
            "label ids shape: ,  (2,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1125 00:14:29.805067 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.806912 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.813230 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.815362 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.826102 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.836774 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.847382 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.854237 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.857425 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.881263 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.885097 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.892699 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.896068 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.905778 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.909356 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.928527 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.932607 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.943021 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.946204 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.957191 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.961400 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.975831 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.979353 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.990113 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:29.993815 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.006228 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.009453 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.018066 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.022555 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.029078 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.031971 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.055413 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.059506 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.068470 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.071521 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.080909 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.086550 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.097187 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.100728 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.111578 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.115350 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.129926 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.134994 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.145123 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.149725 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.158688 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.162391 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.182806 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.186167 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.198496 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.202784 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.213196 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.218633 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.229609 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.233324 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.244179 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.247735 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.261895 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.265625 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.276134 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.283421 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.291493 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.294769 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.314800 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.319067 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.329752 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.333472 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.346421 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.351532 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.361067 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.365001 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.373945 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.377259 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.392010 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.395761 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.404012 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.407295 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.414167 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.417155 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.436676 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.442516 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.452172 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.455538 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.466644 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.470901 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.480483 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.484463 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.494421 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.500708 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.514487 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.520632 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.528889 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.535749 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.542865 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.546037 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.566387 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.571721 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.581059 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.586930 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.597776 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.602827 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.613053 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.619489 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.630117 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.633161 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.645946 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.651329 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.658387 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.661787 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.669097 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.676619 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.697839 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.704106 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.713187 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.719099 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.728871 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.733422 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.743710 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.749480 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.757922 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.761054 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.773677 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.778285 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.785562 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.792138 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.799038 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.803260 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.821703 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.825257 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.836405 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.839665 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.850162 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.854346 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.864497 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.870213 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.879630 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.884884 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.896739 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.902395 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.908737 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.912220 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.919688 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.922849 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.941585 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.945681 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.956373 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.959887 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.970396 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.975023 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.984678 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:30.988240 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.000240 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.004538 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.016506 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.020718 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.028511 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.032437 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.038794 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.044115 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.062324 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.067617 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.077316 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.083368 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.094477 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.099785 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.111207 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.117317 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.128442 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.132870 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.144814 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.150515 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.158845 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.164494 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.171294 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.175781 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.198369 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.202321 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.214283 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.218355 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.229266 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.233222 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.243316 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.247581 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.257618 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.261013 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.275016 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.280060 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.286808 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.290303 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.296627 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.303275 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.322120 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.328107 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.337992 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.341324 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.353420 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.358280 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.368393 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.372359 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.383969 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.387885 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.422858 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.427571 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.441873 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.445652 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.456461 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.459734 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1125 00:14:31.469582 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "(6, 2)\n",
            "(2, 2)\n",
            "(2, 3)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>, <tf.Variable 'loss/output_weights:0' shape=(3, 768) dtype=float32>, <tf.Variable 'loss/output_bias:0' shape=(3,) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4SrW-bL1Lel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "af28d664-43b3-4d06-f026-54d90f058848"
      },
      "source": [
        "# dev_ent['joint_label'] = dev_ent['sentiment']\n",
        "dev_ent['joint_label'] = 'False_'+dev_ent['polarity'].astype(str)\n",
        "\n",
        "dev_ent_InputExamples = dev_ent.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_ent_features = run_classifier.convert_examples_to_features(dev_ent_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "pd = model_predict(estimator,dev_ent_features,dev_ent_InputExamples)\n",
        "true_label = list(dev_ent['polarity']+1)\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "\n",
        "print(labels_val)\n",
        "print(true_label)\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2]\n",
            "[2, 1, 2, 2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 2, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 0, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 0, 1, 1, 0, 2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 1, 2, 1, 0, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 0, 0, 1, 1, 2, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 0, 1, 0, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 0, 1, 2, 1, 1, 0, 1, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1]\n",
            "[[  0  28  30]\n",
            " [  0 100 116]\n",
            " [  0 102 202]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        58\n",
            "           1       0.43      0.46      0.45       216\n",
            "           2       0.58      0.66      0.62       304\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       578\n",
            "   macro avg       0.34      0.38      0.36       578\n",
            "weighted avg       0.47      0.52      0.49       578\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVKQ3p0i-Tdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['joint_label'] = 'False_'+train['polarity'].astype(str)\n",
        "\n",
        "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "pd = model_predict(estimator,train_features,train_InputExamples)\n",
        "true_label = list(train['polarity']+1)\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "\n",
        "print(labels_val)\n",
        "print(true_label)\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYlSED2fBRC",
        "colab_type": "code",
        "outputId": "55def4f4-08b6-4fef-d0b9-73d22a52f8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(2871,5743,2871):\n",
        "  print('evaluating epoch: %d'%(i/2871))\n",
        "  pd = model_predict(estimator,dev_joint_features,dev_joint_InputExamples,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%(i))\n",
        "  true_label = list(dev_joint['label'])\n",
        "  labels_val = []\n",
        "  for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating epoch: 1\n",
            "label_ids ar:  Tensor(\"InfeedQueue/dequeue:2\", shape=(1,), dtype=int32, device=/device:TPU_REPLICATED_CORE:0)\n",
            "label ids shape: ,  (1,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1124 23:33:54.967506 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:54.969516 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:54.971601 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:54.974431 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:54.986263 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:54.997546 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.006858 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.014538 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.018177 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.037236 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.040277 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.048363 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.052165 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.060085 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.063862 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.077500 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.080753 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.087897 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.092035 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.102539 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.107511 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.120332 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.123989 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.131474 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.134752 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.146254 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.149544 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.157749 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.161391 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.169433 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.173178 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.189831 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.193629 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.201730 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.204616 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.217277 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.223047 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.233042 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.236584 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.244730 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.248378 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.261567 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.265639 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.272536 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.275979 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.282888 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.286278 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.300460 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.305250 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.311594 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.314759 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.325191 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.329222 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.339158 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.342258 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.347990 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.351099 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.362534 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.365407 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.372082 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.376871 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.384543 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.389733 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.406364 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.409780 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.416867 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.422142 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.436151 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.442240 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.452936 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.457304 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.463634 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.466943 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.478991 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.482470 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.490139 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.493824 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.501310 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.504681 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.520217 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.524034 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.530936 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.534727 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.548870 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.553744 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.564234 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.567840 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.575253 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.579125 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.591722 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.595366 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.602655 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.606420 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.613769 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.616967 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.632714 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.636497 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.644289 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.651443 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.662894 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.669855 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.680293 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.684107 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.691515 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.694999 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.706983 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.710341 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.717586 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.721281 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.729850 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.733541 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.748281 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.751720 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.758886 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.762913 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.773407 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.778479 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.789336 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.793586 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.800555 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.805080 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.818578 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.823045 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.830334 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.834012 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.841056 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.843802 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.861862 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.865906 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.871939 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.874773 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.887430 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.890928 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.902312 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.905474 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.911819 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.914838 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.933141 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.938672 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.948462 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.954283 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.964409 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.970896 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.985984 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.992382 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:55.998667 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.001621 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.011879 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.016278 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.026643 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.029731 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.036664 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.041049 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.056766 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.061521 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.068274 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.073656 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.081000 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.083967 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.100529 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.106697 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.113250 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.119061 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.128128 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.132862 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.142501 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.148209 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.156344 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.161945 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.173316 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.178853 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.184822 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.190810 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.197434 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.204253 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.219600 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.223768 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.232101 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.238050 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.247268 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.251667 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.261461 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.265685 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.271457 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.275942 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.287746 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.291759 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.298842 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.302467 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.310834 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.319489 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.335223 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.339550 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.345607 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.350509 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.360367 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.366753 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.375307 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.381027 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.387320 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.390275 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.429128 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.435992 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.448996 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.454504 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.463095 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.466490 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:33:56.478013 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1,)\n",
            "(6, 2)\n",
            "(1, 2)\n",
            "(1, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7fe668dd11e0> ignored\n",
            "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[   0 1020 1541]\n",
            " [   0 1750 2876]\n",
            " [   0    0    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2561\n",
            "           1       0.63      0.38      0.47      4626\n",
            "           2       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.24      0.24      0.24      7187\n",
            "   macro avg       0.21      0.13      0.16      7187\n",
            "weighted avg       0.41      0.24      0.30      7187\n",
            "\n",
            "evaluating epoch: 2\n",
            "label_ids ar:  Tensor(\"InfeedQueue/dequeue:2\", shape=(1,), dtype=int32, device=/device:TPU_REPLICATED_CORE:0)\n",
            "label ids shape: ,  (1,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1124 23:34:52.494195 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.496404 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.499064 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.502912 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.511504 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.528852 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.539406 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.547722 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.550857 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.571115 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.574984 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.583489 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.587979 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.594805 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.598644 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.615406 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.620613 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.628745 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.633132 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.643656 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.648504 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.657736 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.661042 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.668559 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.671838 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.684159 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.687684 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.696763 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.700239 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.710325 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.715640 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.732841 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.736150 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.744153 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.749306 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.759788 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.764054 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.777642 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.783181 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.790728 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.796776 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.808708 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.812030 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.820398 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.823911 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.832223 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.836127 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.854723 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.861052 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.867492 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.872989 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.882929 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.887780 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.897674 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.904743 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.911461 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.914932 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.928172 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.931205 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.939734 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.942873 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.952526 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.956316 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.973866 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.976943 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.985352 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.988531 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:52.999166 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.005706 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.016170 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.019665 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.029810 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.035109 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.050601 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.057239 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.068905 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.073546 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.082369 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.086556 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.102828 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.107515 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.115628 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.122102 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.134243 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.139264 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.150968 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.155282 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.161226 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.164908 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.181874 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.185848 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.193350 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.196341 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.203607 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.208444 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.224698 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.228403 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.234469 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.242306 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.252157 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.256880 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.270524 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.274291 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.281162 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.286087 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.300201 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.303894 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.312568 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.316921 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.324824 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.330580 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.348506 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.353252 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.360043 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.365973 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.376060 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.380549 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.391733 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.398653 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.406208 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.409811 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.423391 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.430047 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.438239 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.443753 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.451740 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.456777 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.471080 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.475662 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.481966 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.485421 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.496730 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.501914 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.511609 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.515435 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.520899 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.528850 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.543391 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.546812 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.556988 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.560645 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.569300 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.573076 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.591763 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.595350 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.601824 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.605221 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.620124 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.625035 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.635324 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.638974 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.645359 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.648601 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.661020 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.663930 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.671096 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.674508 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.682444 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.686381 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.706438 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.709631 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.719069 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.723727 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.735284 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.739059 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.751560 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.755633 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.763123 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.767179 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.780472 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.784002 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.794181 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.799201 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.808523 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.812026 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.830275 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.836210 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.842531 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.846800 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.858014 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.862703 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.873917 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.879122 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.892760 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.899869 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.913741 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.917867 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.926604 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.930641 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.940243 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.944514 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.963063 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.967080 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.973642 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.978471 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.990612 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:53.995625 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.009685 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.015115 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.021761 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.025619 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.058301 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.062297 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.077127 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.080996 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.090060 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.093170 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1124 23:34:54.107708 140629443692416 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1,)\n",
            "(6, 2)\n",
            "(1, 2)\n",
            "(1, 3)\n",
            "[[   0 1097 1464]\n",
            " [   0 1832 2794]\n",
            " [   0    0    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2561\n",
            "           1       0.63      0.40      0.48      4626\n",
            "           2       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.25      0.25      0.25      7187\n",
            "   macro avg       0.21      0.13      0.16      7187\n",
            "weighted avg       0.40      0.25      0.31      7187\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7fe65f269550> ignored\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUkPORA68JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd = model_predict(estimator,test_features_fixed,test_InputExamples_fixed)\n",
        "true_label = list(test_fixed['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQReX-Hf0WiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(1,13):\n",
        "  \n",
        "#   print('processing epoch: %d'%i)\n",
        "#   pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/saved_models/model.ckpt-%d'%(i*3000))\n",
        "#   true_label = list(dev['label'])\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#       labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpz2FhG3_sS",
        "colab_type": "code",
        "outputId": "0a2b0d58-4c7e-4236-8af2-7ab20d131a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas\n",
        "pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%(1392*3))\n",
        "true_label = list(dev['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "      labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "print(labels_val)\n",
        "print(dev)\n",
        "dev['predicted'] = pandas.Series(labels_val)\n",
        "dev.to_csv('/content/dev_predicted.csv', encoding='utf8')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[ 969  259]\n",
            " [ 194 2058]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.83      0.79      0.81      1228\n",
            "        True       0.89      0.91      0.90      2252\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      3480\n",
            "   macro avg       0.86      0.85      0.86      3480\n",
            "weighted avg       0.87      0.87      0.87      3480\n",
            "\n",
            "[1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1]\n",
            "      doc_id  label                                           sentence\n",
            "0       1862   True  -LRB- tgt has called on the international comm...\n",
            "1       1906   True  But as the Finals have progressed  and Bryant'...\n",
            "2       1842  False  tgthas drawn national attention -- and calls f...\n",
            "3       1867   True  -LRB- tgt said Mass in the chapel of the Casa ...\n",
            "4       1920   True  tgt's grass-court winning streak ended at 14 w...\n",
            "5       1852  False  Disgruntled wide receiver Martavis Bryant foll...\n",
            "6       2085  False  tgta Hall of Fame broadcaster known as much fo...\n",
            "7       2074   True  After tgtreiterated  tgt  vow on Monday to ben...\n",
            "8       2004  False  WEST PALM BEACH  Fla. (Reuters) -  U.S. Presid...\n",
            "9       1978  False  President Donald Trump said  he  was surprised...\n",
            "10      2057  False  New Poll: In Swing Districts  Voters Strongly ...\n",
            "11      2096   True  Forty years ago this month   tgtbecame the fir...\n",
            "12      2035   True  CLOSE  tgtArrived in Myanmar Time\\nEthnic Kach...\n",
            "13      2083   True  Mother Jones scribe tgthas a different approac...\n",
            "14      1875  False  If you bought a Bitcoin in early 2017  when on...\n",
            "15      1875  False  If you bought a Bitcoin in early 2017  when on...\n",
            "16      2011  False  The idea that the Knicks would somehow turn  t...\n",
            "17      1845  False  NAIROBI   Kenya ( AP ) â Kenya ' s attorney ...\n",
            "18      1914   True  An economic downturn has a way of bringing out...\n",
            "19      2007   True  [MASK]are winning the internet over with  tgt ...\n",
            "20      1934  False  A Chicago police officer who faces potential f...\n",
            "21      1896  False  When President [MASK] goes to Guadalajara  Mex...\n",
            "22      1972  False  In a tweet on Monday   tgt  claimed  he  had g...\n",
            "23      2018  False  Lawyers for tgtshown last month  acknowleged T...\n",
            "24      1968  False  Reports surfaced last week that  tgttried to p...\n",
            "25      1853   True  ' I Came   I Saw   I Selfied ': How Instagram ...\n",
            "26      1924   True  Having just wrapped up a visit to China on the...\n",
            "27      1892  False  -- Many of my stadium visits have dovetailed w...\n",
            "28      2103   True  FILE - In this Dec. 17   2017  file photo  tgt...\n",
            "29      1852  False  Disgruntled wide receiver Martavis Bryant foll...\n",
            "...      ...    ...                                                ...\n",
            "3450    1946   True  But [MASK] were so startling and so disturbing...\n",
            "3451    1965   True  tgtwas expecting a different sort of reception...\n",
            "3452    2069  False  Virginiaâs DeâAndre  Hunter  did his part ...\n",
            "3453    1952   True  The  tgtstory continues to reverberate  as new...\n",
            "3454    2082  False  More than decade ago  Shah and a friend  tgt  ...\n",
            "3455    1969  False  Washington (CNN) White House press secretary S...\n",
            "3456    1859   True  tgtgot the surprise of a lifetime when  tgt  s...\n",
            "3457    2083  False  Mother Jones scribe tgthas a different approac...\n",
            "3458    2049   True  UPDATE: tgttells Rolling Stone that an erroneo...\n",
            "3459    1890   True  Italian former leftist guerrilla tgt  leaves t...\n",
            "3460    2082   True  More than decade ago  Shah and a friend  tgt  ...\n",
            "3461    1854   True  The Harvey Weinsteins of the world aren ' t al...\n",
            "3462    1998   True  tgt arrives to celebrate Mass at the Las Palma...\n",
            "3463    1952   True  The  tgtstory continues to reverberate  as new...\n",
            "3464    1951  False  Who  Pat Shurmur? No. No way Kyle learned that...\n",
            "3465    2072   True  City ethics officials  are looking into whethe...\n",
            "3466    2033   True  tgthasn't been on the road much in 2017  but t...\n",
            "3467    2035  False  CLOSE  tgtArrived in Myanmar Time\\nEthnic Kach...\n",
            "3468    1998   True  tgt arrives to celebrate Mass at the Las Palma...\n",
            "3469    2060  False  In the wake of the Harvey Weinstein revelation...\n",
            "3470    2051   True  Seven women Democratic Senators turned on  tgt...\n",
            "3471    2048  False  Meet tgt .  tgt 're all turning 36  and so is ...\n",
            "3472    2070   True  tgt Tarly   told TV Guide that characters are ...\n",
            "3473    1892  False  -- Many of my stadium visits have dovetailed w...\n",
            "3474    1850  False  What Went Wrong In Charlottesville ? Almost Ev...\n",
            "3475    1955   True  tgtsaid in a series of tweets on Sunday that  ...\n",
            "3476    2079   True  tgtsaid the Corker plan would basically be imp...\n",
            "3477    2103   True  FILE - In this Dec. 17   2017  file photo  tgt...\n",
            "3478    1865   True  God is the patient and merciful artisan of our...\n",
            "3479    1889   True  Paul Manafort  Rick Gates Back In Court For Ba...\n",
            "\n",
            "[3480 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GMYpmM-Q_B",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascfbXXbzEpw",
        "colab_type": "code",
        "outputId": "3b443645-20f0-4a63-9812-e3219a444502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# ## Run it if you want to train for a range of epochs and see the validation error and save the prediction on than\n",
        "# ## ** Part Name **\n",
        "# # mds = []\n",
        "# # evs = []\n",
        "# pds_dev = []\n",
        "# pds_tr = []\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) \n",
        "\n",
        "# for i in range(1,11):\n",
        "#   print('----------------------- Starting Epoch %d-----------------------'%i)\n",
        "\n",
        "#   NUM_TRAIN_EPOCHS = i\n",
        "  \n",
        "#   num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "#   num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# #   model_fn = model_fn_builder(\n",
        "# #   num_labels=len(label_list),\n",
        "# #   learning_rate=LEARNING_RATE,\n",
        "# #   num_train_steps=num_train_steps,\n",
        "# #   num_warmup_steps=num_warmup_steps,\n",
        "# #   use_tpu=True,\n",
        "# #   bert_hub_module_handle=BERT_MODEL_HUB)\n",
        "\n",
        "#   model_fn = model_fn_builder(\n",
        "#   bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#   num_labels=len(label_list),\n",
        "#   init_checkpoint=INIT_CHECKPOINT,\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps,\n",
        "#   use_tpu=True,\n",
        "#   use_one_hot_embeddings=True)\n",
        "  \n",
        "  \n",
        "#   estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=run_config,\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "  \n",
        "#   model_train(estimator_from_tfhub)\n",
        "# #   ev = model_eval(estimator_from_tfhub)\n",
        "\n",
        "# #   print(' -------------------- Train Prediction --------------------')\n",
        "# #   pd = model_predict(estimator_from_tfhub,train_features,train_InputExamples)\n",
        "# #   true_label = list(train['label'])\n",
        "# #   pds_tr.append(pd)\n",
        "# #   labels_val = []\n",
        "# #   for item in pd:\n",
        "# #     labels_val.append(np.argmax(item[1]))\n",
        "# #   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# #   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "#   print(' -------------------- Dev Prediction --------------------')\n",
        "#   pd = model_predict(estimator_from_tfhub,dev_features[:3100],dev_InputExamples[:3100])\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   pds_dev.append(pd)\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for tr,dv in zip(pds_tr,pds_dev):\n",
        "#   labels_val = []\n",
        "#   true_label = list(train['label'])\n",
        "#   for item in tr:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "#   labels_val = []\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   for item in dv:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------- Starting Epoch 1-----------------------\n",
            "***** Started training at 2019-05-15 17:57:14.080193 *****\n",
            "  Num examples = 122150\n",
            "  Batch size = 8\n",
            "start running estimator\n",
            "(1, 2)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_dvH1MmpAr3",
        "colab_type": "code",
        "outputId": "ae794af2-b7a8-4188-b4ef-8dc97e42c452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2520
        }
      },
      "source": [
        "# prd = model_predict(estimator,train_features,train_InputExamples)\n",
        "# prd = [item for item in predictions]\n",
        "len(prd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9efd420be1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_InputExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# prd = [item for item in predictions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-3ccd6d9b41c7>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(estimator, input_features, input_examples)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mpredict_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2498\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.pyc\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2492\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m           yield_single_examples=yield_single_examples):\n\u001b[0m\u001b[1;32m   2495\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mscaffold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 config=self._session_config),\n\u001b[0;32m--> 627\u001b[0;31m             hooks=all_hooks) as mon_sess:\n\u001b[0m\u001b[1;32m    628\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpreds_evaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    932\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    933\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \"\"\"\n\u001b[1;32m   1121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1312\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n\nCaused by op u'save_2/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-9efd420be1df>\", line 1, in <module>\n    prd = model_predict(estimator,train_features,train_InputExamples)\n  File \"<ipython-input-45-3ccd6d9b41c7>\", line 18, in model_predict\n    return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2494, in predict\n    yield_single_examples=yield_single_examples):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 627, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 934, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1122, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1127, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 805, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 562, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 3033, in _finalize\n    wrapped_finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 217, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 604, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 507, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 385, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khuNEnMw2wiv",
        "colab_type": "code",
        "outputId": "83f8b377-31f6-4976-8cf4-d26796869d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "len(a)\n",
        "# import numpy as np\n",
        "# from sklearn import metrics\n",
        "\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(dev['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(np.argmax(item[1]))\n",
        "# true_label = list(train['label'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  48]\n",
            " [  0 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        48\n",
            "        True       0.76      1.00      0.86       152\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       200\n",
            "   macro avg       0.38      0.50      0.43       200\n",
            "weighted avg       0.58      0.76      0.66       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "outputId": "f4559586-e7a1-4674-c430-a3b426c101f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed['sentiment'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-985837170ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_from_tfhub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_InputExamples_fixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabels_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fixed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_InputExamples_fixed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzzg7MRSk5E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5SpMUg7b9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.logging.set_verbosity(tf.logging.DEBUG) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test['sentiment'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKRWIAgn_YAE",
        "colab_type": "code",
        "outputId": "a5acf1b3-8717-4321-fd7c-b4804432240e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# len(labels_val)\n",
        "# len(test_InputExamples)\n",
        "# len(predictions)\n",
        "model_eval(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started evaluation at 2019-05-09 17:10:37.771525 *****\n",
            "  Num examples = 3479\n",
            "  Batch size = 8\n",
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-904242ebce51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_from_tfhub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-2dd67f62c231>\u001b[0m in \u001b[0;36mmodel_eval\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       drop_remainder=True)\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'***** Finished evaluation at {} *****'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0moutput_eval_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval_results.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m   def predict(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.pyc\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2474\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m   def _actual_eval(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 493\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    494\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    495\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1424\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1460\u001b[0;31m         features, labels, model_fn_lib.ModeKeys.EVAL, config)\n\u001b[0m\u001b[1;32m   1461\u001b[0m     eval_metric_ops = _verify_and_create_loss_metric(\n\u001b[1;32m   1462\u001b[0m         estimator_spec.eval_metric_ops, estimator_spec.loss)\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   2249\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m       return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n\u001b[0;32m-> 2251\u001b[0;31m                                                       config)\n\u001b[0m\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn_for_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config, params)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           compile_op, total_loss, host_calls, scaffold, eval_hooks = (\n\u001b[0;32m-> 2651\u001b[0;31m               _eval_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n\u001b[0m\u001b[1;32m   2652\u001b[0m           \u001b[0miterations_per_loop_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_or_get_iterations_per_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m           mean_loss = math_ops.div(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_eval_on_tpu_system\u001b[0;34m(ctx, model_fn_wrapper, dequeue_fn)\u001b[0m\n\u001b[1;32m   2867\u001b[0m       \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2868\u001b[0m       \u001b[0moutputs_from_all_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2869\u001b[0;31m       device_assignment=ctx.device_assignment)\n\u001b[0m\u001b[1;32m   2870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.pyc\u001b[0m in \u001b[0;36msplit_compile_and_shard\u001b[0;34m(computation, inputs, num_shards, input_shard_axes, outputs_from_all_shards, output_shard_axes, infeed_queue, device_assignment, name)\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minfeed_queue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfeed_queue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m       \u001b[0mdevice_assignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_assignment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   \u001b[0;31m# There must be at least one shard since num_shards > 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.pyc\u001b[0m in \u001b[0;36msplit_compile_and_replicate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mvscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_custom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcomputation_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0mvscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_use_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_use_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mmulti_tpu_eval_steps_on_single_shard\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmulti_tpu_eval_steps_on_single_shard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m     return training_loop.repeat(iterations_per_loop_var, single_tpu_eval_step,\n\u001b[0;32m-> 2862\u001b[0;31m                                 [_ZERO_LOSS])\n\u001b[0m\u001b[1;32m   2863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m   (compile_op, loss,) = tpu.split_compile_and_shard(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(n, body, inputs, infeed_queue, name)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   outputs = while_loop(\n\u001b[0;32m--> 208\u001b[0;31m       cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m   return control_flow_ops.while_loop(\n\u001b[0;32m--> 170\u001b[0;31m       condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3554\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3556\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 3087\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   3088\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   3021\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mbody_wrapper\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mdequeue_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdequeue_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# If the computation only returned one value, make it a tuple.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mbody_wrapper\u001b[0;34m(i, *args)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbody_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(total_loss)\u001b[0m\n\u001b[1;32m   1422\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m       \u001b[0mtpu_estimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_estimator_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TPUEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         raise RuntimeError(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0m_add_item_to_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_CTX_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m     \u001b[0mestimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m     if (running_on_cpu and\n\u001b[1;32m   1595\u001b[0m         isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\n",
            "\u001b[0;32m<ipython-input-10-5ebb4ee1571f>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           eval_metrics=eval_metrics,scaffold_fn=scaffold_fn)\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, mode, predictions, loss, train_op, eval_metrics, export_outputs, scaffold_fn, host_call, training_hooks, evaluation_hooks, prediction_hooks)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhost_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mhost_calls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host_call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0m_OutfeedHostCall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mtraining_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(host_calls)\u001b[0m\n\u001b[1;32m   1652\u001b[0m               \u001b[0;34m'In TPUEstimatorSpec.{}, length of tensors {} does not match '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m               'method args of the function, which takes {}.'.format(\n\u001b[0;32m-> 1654\u001b[0;31m                   name, len(host_call[1]), len(fn_args)))\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: In TPUEstimatorSpec.eval_metrics, length of tensors 3 does not match method args of the function, which takes 4."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIwoEwmrk6ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqh11HPVAsvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####seq 128, Small BERT, Batchsize 32 for train, 8 for dev and test\n",
        "# 1)I0423 16:57:01.507206 140076901431168 basic_session_run_hooks.py:249] loss = 0.40919852, step = 46\n",
        "# 2)I0423 18:08:31.116359 140500355729280 basic_session_run_hooks.py:249] loss = 0.7756149, step = 92\n",
        "# 4)Loss for final step: 0.6891643.\n",
        "# 5)Loss for final step: 0.9730371\n",
        "# 6)Loss for final step: 0.4775733\n",
        "# 7)Loss for final step: 1.2802429.\n",
        "# 8)Loss for final step: 0.509207\n",
        "# 9)loss = 0.29262337, step = 414\n",
        "# 10)Loss for final step: 0.47267017\n",
        "\n",
        "\n",
        "# 2)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 92\n",
        "#   loss = 0.79767215\n",
        "# 3)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 138\n",
        "#   loss = 0.79767215\n",
        "# 4)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.86550355\n",
        "#   global_step = 184\n",
        "#   loss = 0.83540887\n",
        "# 5)***** Eval results *****\n",
        "#   eval_accuracy = 0.5964286\n",
        "#   eval_loss = 0.88053304\n",
        "#   global_step = 230\n",
        "#   loss = 0.91362196\n",
        "# 6)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.90498805\n",
        "#   global_step = 276\n",
        "#   loss = 1.0806552\n",
        "# 7)***** Eval results *****\n",
        "#   eval_accuracy = 0.56785715\n",
        "#   eval_loss = 0.92742974\n",
        "#   global_step = 322\n",
        "#   loss = 1.0180835\n",
        "# 8)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9436406\n",
        "#   global_step = 368\n",
        "#   loss = 0.9721719\n",
        "# 9)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9714315\n",
        "#   global_step = 414\n",
        "#   loss = 0.5798999\n",
        "# 10)***** Eval results *****\n",
        "#   eval_accuracy = 0.54285717\n",
        "#   eval_loss = 1.0072392\n",
        "#   global_step = 460\n",
        "#   loss = 1.053762\n",
        "  \n",
        "#   DEV Info:\n",
        "# 1)[[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "# 2)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 3)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 4)[[  0   8  42]\n",
        "#  [  0  16 100]\n",
        "#  [  0  17 157]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        50\n",
        "#      Neutral       0.39      0.14      0.20       116\n",
        "#     Positive       0.53      0.90      0.66       174\n",
        "\n",
        "#    micro avg       0.51      0.51      0.51       340\n",
        "#    macro avg       0.31      0.35      0.29       340\n",
        "# weighted avg       0.40      0.51      0.41       340\n",
        "# 5)[[  0  17   6]\n",
        "#  [  0  40  57]\n",
        "#  [  0  35 129]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.41      0.42        97\n",
        "#     Positive       0.67      0.79      0.72       164\n",
        "\n",
        "#    micro avg       0.60      0.60      0.60       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.60      0.56       284\n",
        "# 6)[[  0  19   4]\n",
        "#  [  0  44  53]\n",
        "#  [  0  42 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.45      0.44        97\n",
        "#     Positive       0.68      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.58      0.56       284\n",
        "# 7)[[  0  18   5]\n",
        "#  [  0  46  51]\n",
        "#  [  0  47 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.41      0.47      0.44        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.36      0.40      0.38       284\n",
        "# weighted avg       0.53      0.57      0.55       284\n",
        "# 8)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  53 111]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.68      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "# 9)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  54 110]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.67      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "# 10)[[  0  18   5]\n",
        "#  [  0  51  46]\n",
        "#  [  0  62 102]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.53      0.45        97\n",
        "#     Positive       0.67      0.62      0.64       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.54      0.52       284\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0h7wMprmvyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Seq:256, small BERT, batch size 32 for train, 8 for test and dev\n",
        "# 3) {'loss': 0.79650426, 'eval_accuracy': 0.58214283, 'eval_loss': 0.86125755, 'global_step': 138}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# 4) {'loss': 0.73700047, 'eval_accuracy': 0.5928571, 'eval_loss': 0.8223035, 'global_step': 184}\n",
        "# [[  0  11  12]\n",
        "#  [  0  20  77]\n",
        "#  [  0  17 147]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.21      0.28        97\n",
        "#     Positive       0.62      0.90      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.35      0.37      0.34       284\n",
        "# weighted avg       0.50      0.59      0.52       284\n",
        "\n",
        "# 5) {'loss': 0.71594626, 'eval_accuracy': 0.5928571, 'eval_loss': 0.82239425, 'global_step': 230}\n",
        "# [[  0  19   4]\n",
        "#  [  0  51  46]\n",
        "#  [  0  48 116]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.53      0.47        97\n",
        "#     Positive       0.70      0.71      0.70       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.55      0.59      0.57       284\n",
        "\n",
        "# 6) {'loss': 0.7227276, 'eval_accuracy': 0.5857143, 'eval_loss': 0.8479867, 'global_step': 276}\n",
        "# [[  0  21   2]\n",
        "#  [  2  51  44]\n",
        "#  [  1  49 114]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.53      0.47        97\n",
        "#     Positive       0.71      0.70      0.70       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.56      0.58      0.57       284\n",
        "\n",
        "# 7){'loss': 0.83304703, 'eval_accuracy': 0.5642857, 'eval_loss': 0.90589315, 'global_step': 322}\n",
        "# [[  0  19   4]\n",
        "#  [  4  40  53]\n",
        "#  [  3  42 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.41      0.40        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.38      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "\n",
        "# 8){'loss': 0.84049994, 'eval_accuracy': 0.575, 'eval_loss': 0.93640614, 'global_step': 368}\n",
        "# [[  2  18   3]\n",
        "#  [  5  42  50]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.22      0.09      0.12        23\n",
        "#      Neutral       0.40      0.43      0.42        97\n",
        "#     Positive       0.69      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.44      0.41      0.42       284\n",
        "# weighted avg       0.55      0.57      0.56       284\n",
        "\n",
        "# 9){'loss': 0.8601472, 'eval_accuracy': 0.5642857, 'eval_loss': 0.95250976, 'global_step': 414}\n",
        "# [[  0  18   5]\n",
        "#  [  4  37  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.38      0.38        97\n",
        "#     Positive       0.67      0.74      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "\n",
        "\n",
        "# 10){'loss': 0.9091368, 'eval_accuracy': 0.5535714, 'eval_loss': 0.9715489, 'global_step': 460}\n",
        "# [[  0  19   4]\n",
        "#  [  4  37  56]\n",
        "#  [  1  44 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.37      0.38      0.38        97\n",
        "#     Positive       0.66      0.73      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.34      0.37      0.36       284\n",
        "# weighted avg       0.51      0.55      0.53       284\n",
        "\n",
        "# 11){'loss': 0.9756337, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0129306, 'global_step': 506}\n",
        "# [[  3  16   4]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.39      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 12){'loss': 0.9551747, 'eval_accuracy': 0.56785715, 'eval_loss': 1.0222387, 'global_step': 552}\n",
        "# [[  3  17   3]\n",
        "#  [  5  38  54]\n",
        "#  [  2  43 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.30      0.13      0.18        23\n",
        "#      Neutral       0.39      0.39      0.39        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.45      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 13){'loss': 1.0276382, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0547612, 'global_step': 598}\n",
        "# [[  3  17   3]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 14){'loss': 1.0036505, 'eval_accuracy': 0.55714285, 'eval_loss': 1.0697843, 'global_step': 644}\n",
        "# [[  3  17   3]\n",
        "#  [  7  36  54]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.37      0.37      0.37        97\n",
        "#     Positive       0.67      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.43      0.41      0.41       284\n",
        "# weighted avg       0.54      0.55      0.54       284\n",
        "\n",
        "# 15){'loss': 1.0295725, 'eval_accuracy': 0.5642857, 'eval_loss': 1.0809377, 'global_step': 690}\n",
        "# [[  3  17   3]\n",
        "#  [  5  39  53]\n",
        "#  [  1  46 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.40      0.39        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 16){'loss': 1.0889318, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1056138, 'global_step': 736}\n",
        "# [[  3  17   3]\n",
        "#  [  8  33  56]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.42      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 17){'loss': 1.1312778, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1440269, 'global_step': 782}\n",
        "# [[  3  17   3]\n",
        "#  [  9  33  55]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 18){'loss': 1.1436831, 'eval_accuracy': 0.55, 'eval_loss': 1.1613237, 'global_step': 828}\n",
        "# [[  3  16   4]\n",
        "#  [  8  35  54]\n",
        "#  [  2  45 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.54       284\n",
        "\n",
        "# 19){'loss': 1.1817628, 'eval_accuracy': 0.55, 'eval_loss': 1.1834545, 'global_step': 874}\n",
        "# [[  3  16   4]\n",
        "#  [  8  34  55]\n",
        "#  [  1  45 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.36      0.35      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.53       284\n",
        "\n",
        "# 20){'loss': 1.1921012, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1976833, 'global_step': 920}\n",
        "# [[  3  16   4]\n",
        "#  [  9  35  53]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.53      0.54      0.53       284\n",
        "\n",
        "# 21){'loss': 1.2514663, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2244278, 'global_step': 966}\n",
        "# [[  3  16   4]\n",
        "#  [ 11  32  54]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.19      0.13      0.15        23\n",
        "#      Neutral       0.34      0.33      0.33        97\n",
        "#     Positive       0.66      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.40      0.39      0.39       284\n",
        "# weighted avg       0.51      0.53      0.52       284\n",
        "\n",
        "# 22){'loss': 1.2630298, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2527977, 'global_step': 1012}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284\n",
        "\n",
        "# 23){'loss': 1.3330169, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2815608, 'global_step': 1058}\n",
        "# [[  6  13   4]\n",
        "#  [ 14  31  52]\n",
        "#  [  9  42 113]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.26      0.23        23\n",
        "#      Neutral       0.36      0.32      0.34        97\n",
        "#     Positive       0.67      0.69      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.41      0.42      0.42       284\n",
        "# weighted avg       0.53      0.53      0.53       284\n",
        "\n",
        "# 24){'loss': 1.3111317, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2898273, 'global_step': 1104}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqSe326lwHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Larg BERT seq:256, batchsize 8 for train, dev , test\n",
        "# {'loss': 1.0679293, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8965841, 'global_step': 93}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0362501, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88540924, 'global_step': 186}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0290864, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8812368, 'global_step': 279}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0271251, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88323754, 'global_step': 372}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.02654, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8804489, 'global_step': 465}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# SAME FOR 10 EPOCHS!!!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPSn-7_J05J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy2D-6ZBJ10t",
        "colab_type": "code",
        "outputId": "2cf4abd1-d66d-4384-d04b-01b532d628e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZunWbyH8xrEb",
        "colab_type": "code",
        "outputId": "48e21d5a-c62e-4273-8fb4-4658380d0c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_train['DOCUMENT'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idaqpVbZz57p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}