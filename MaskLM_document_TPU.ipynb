{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskLM_document_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/MaskLM_document_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "outputId": "58745a15-1c45-441c-d1d7-35a3244240cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python2.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "('TPU address is', 'grpc://10.96.13.98:8470')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1120 00:14:09.010605 139937064892288 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3486968103806664664),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16159888889617227047),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10553681643051528714),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9551362916639775973),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12329192859028265861),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16618443022023782064),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11737272850591448828),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8326117815147279132),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2937183748583611634),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15140196551814490080),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13978879623824509872)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "outputId": "9a94d673-8b16-4978-fb1a-21bdbcf6c185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "# import python modules defined by BERT\n",
        "from bert import modeling\n",
        "# import optimization\n",
        "# import run_classifier\n",
        "from bert import run_classifier_with_tfhub\n",
        "# import tokenization\n",
        "\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1120 00:14:12.451973 139937064892288 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "4c785012-98a6-4e63-c72c-5bc83e5b9558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/aug_19_models/mask_lm/weighted/last3layers'.format(BUCKET)\n",
        "# V2_augment/doc_level/v2/smallBERT-docLevel-seq512/experiment1'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "# !gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/aug_19_models/mask_lm/weighted/last3layers *****\n",
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import modeling\n",
        "# import run_classifier\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  pre_tvars = tf.trainable_variables()\n",
        "#   print('all trainable variables: >>',pre_tvars)\n",
        "  tvars = pre_tvars\n",
        "  tvars = [item for item in pre_tvars if not '/layer_0/'  in item.name and not '/layer_1/'  in item.name and not '/layer_2/'  in item.name\n",
        "          and not '/layer_3/'  in item.name and not '/layer_4/'  in item.name and not '/layer_5/'  in item.name and not '/layer_6/'  in item.name \n",
        "          and not '/layer_7/'  in item.name and not '/layer_8/'  in item.name ]\n",
        "           #and not '/layer_9/' in item.name]\n",
        "#   and not '/layer_10/'  in item.name \n",
        "#           and not '/layer_11/'  in item.name ]\n",
        "  print('excluded trainable variables: >>',tvars)\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_TRAIN_EPOCHS = 10.0  ## Activate if ** is Not ACTIVATED\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 50\n",
        "SAVE_SUMMARY_STEPS = 20\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "outputId": "3297ecac-8417-4771-fd7a-908e66402824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# data_train = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_train_v3.csv', encoding='latin-1')[:-1]\n",
        "# data_augment = pd.read_csv('/content/mask_lm_postprocess.csv', encoding='latin-1')[:-1]\n",
        "# data_dev = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_fixed_test_v3.csv', encoding='latin-1')\n",
        "\n",
        "\n",
        "# data_train = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_train_v3.csv'), encoding='latin-1')\n",
        "# # data_augment = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_postprocess.csv'), encoding='latin-1')\n",
        "# data_dev = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_dev_v3.csv'), encoding='latin-1')\n",
        "# data_test= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_random_test_v3.csv'), encoding='latin-1')\n",
        "# data_test_fixed= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_fixed_test_v3.csv'), encoding='latin-1')\n",
        "\n",
        "data_pref = 'gs://bert_example/data_aug19/masked_lm/mask_lm_combined_shuffled_3Dec_7Dec_aug19_reindex_%s.csv'\n",
        "\n",
        "data_train = pd.read_csv(tf.gfile.GFile(data_pref % str('train')), encoding='latin-1')\n",
        "data_dev   = pd.read_csv(tf.gfile.GFile(data_pref % 'dev'), encoding='latin-1')\n",
        "data_test  = pd.read_csv(tf.gfile.GFile(data_pref % 'random_test'), encoding='latin-1')\n",
        "data_test_fixed= pd.read_csv(tf.gfile.GFile(data_pref % 'fixed_test'), encoding='latin-1')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(df):\n",
        "#   print('df length>>',len(df['DOCUMENT']))\n",
        "  data = {}\n",
        "  df['DOCUMENT'] = df['DOCUMENT'].str.replace('\\[TGT\\]','tgt')\n",
        "  data[\"sentence\"] = df['DOCUMENT']\n",
        "  data[\"label\"] =df[\"LABEL\"]\n",
        "  data['doc_id'] = df[\"DOCUMENT_INDEX\"]\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None):\n",
        "  df_new = load_directory_data(df[:index])\n",
        "#   print(df_new)\n",
        "  true_df = df_new[df_new['label'] == True]\n",
        "  false_df = df_new[df_new['label'] == False]\n",
        "#   print('true_df>>>',len(true_df))\n",
        "#   true_df[\"polarity\"] = 1\n",
        "#   false_df[\"polarity\"] = 0\n",
        "  return pd.concat([true_df, false_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train = load_dataset(data_train)\n",
        "# train_augment = load_dataset(data_augment)\n",
        "# train = pd.concat([train, train_augment]).sample(frac=1).reset_index(drop=True)\n",
        "dev = load_dataset(data_dev)\n",
        "test = load_dataset(data_test)\n",
        "test_fixed = load_dataset(data_test_fixed)\n",
        "\n",
        "print('train set length: %d,dev set length: %d, test set lenght: %d,  fixed test: %d'%\n",
        "      (len(train),len(dev),len(test),len(test_fixed)))\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set length: 45944,dev set length: 7187, test set lenght: 8130,  fixed test: 12604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMNSsUHdsbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ae2b7a9e-1b5f-41cc-d645-fb291110e026"
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0,1]\n",
        "use_tpu = True\n",
        "print(len(train[train['label']==1]))\n",
        "print(len(train[train['label']==0]))\n",
        "print(len(dev[dev['label']==1]))\n",
        "print(len(dev[dev['label']==0]))\n",
        "\n",
        "# a = 1./len(dev[dev['label']==0])\n",
        "# b = 1./len(dev[dev['label']==1])\n",
        "# print(a/(a+b) , b/(a+b))\n",
        "\n",
        "# a = 1./len(train[train['label']==0])\n",
        "# b = 1./len(train[train['label']==1])\n",
        "# print(a/(a+b) , b/(a+b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29916\n",
            "16028\n",
            "4626\n",
            "2561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLEWLUG0Fmi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import tokenization\n",
        "from bert import run_classifier\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/uncased_L-12_H-768_A-12', 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "outputId": "56d89d3b-1cc0-4bfe-9a5c-4bbb46bde8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_InputExamples_fixed = test_fixed.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_features_fixed = run_classifier.convert_examples_to_features(test_InputExamples_fixed, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# ## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1120 00:14:29.758840 139937064892288 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOesPBlKTv94",
        "colab_type": "code",
        "outputId": "e0b53876-8bb3-42aa-86cf-0f5e3bfd5e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pickle\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "try:\n",
        "  train_features = pickle.load(open('train_features_large.dms','rb'))\n",
        "except Exception as e: \n",
        "    print('can not load train features, creating train features: %s'%str(e))\n",
        "#     train_features = pickle.load(tf.gfile.GFile('gs://bert_example/mask_lm/models/V2_augment/doc_leve/last_2/smallBERT-docLevel-seq512/data/train_features_large.dms', \"rb\"))\n",
        "\n",
        "\n",
        "  \n",
        "    train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    with open('train_features_large','wb') as f:\n",
        "      pickle.dump(train_features,f)\n",
        "try:\n",
        "\n",
        "  dev_features = pickle.load(open('dev_features_large.dsm','rb'))\n",
        "except Exception as e:\n",
        "\n",
        "   \n",
        "    print('can not load from GC, creating dev features')\n",
        "    dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    with open('dev_features_large','wb') as f:\n",
        "      pickle.dump(dev_features,f)\n",
        "print(len(train_features))\n",
        "print(len(dev_features))\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_fixed_features = run_classifier.convert_examples_to_features(test_InputExamples_fixed, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1120 00:19:31.960284 139937064892288 run_classifier.py:774] Writing example 0 of 45944\n",
            "I1120 00:19:31.985311 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:19:31.986937 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:19:31.988997 139937064892288 run_classifier.py:464] tokens: [CLS] tgt . ( michael st ##ra ##vat ##o for the washington post ) the democratic congressional campaign committee is taking fire from the left after months of relative peace thanks to a document put online the night of feb . 22 a ##¢ ##aa a collection of opposition research on tgt a progressive candidate in texas ##a ##¢ ##aa ##s competitive 7th congressional district . as tgt was attending a campaign event the dc ##cc posted a short but brutal collection of hits on mo ##ser that would be likely used against her if she won the democratic nomination . the 7th district is one of 23 where hillary clinton out ##pol ##led donald trump in 2016 while voters sent a republican congressman to washington . a ##¢ ##aa ##de ##mo ##cratic voters need to hear that tgt is not going to change washington a ##¢ ##aa the committee wrote . a ##¢ ##aa tgt is a washington insider who beg ##rud ##gingly moved to houston to run for congress . a ##¢ ##aa mo ##ser responded later thursday night saying in a statement that the dc ##cca ##¢ ##aa ##s tactics represented a ##¢ ##aa ##w ##hy people hate politics ##a ##¢ ##aa and hurt the effort to take congress back from republicans . a ##¢ ##aa ##we ##a ##¢ ##aar ##e used to tough talk here in texas but it ##a ##¢ ##aa ##s disappointing to hear it from washington operatives a ##¢ ##aa she said . tgt did relocate to her native houston last year and began running for congress with her move back home a ##¢ ##aa after launching daily action a campaign tool for progressive ##s frustrated by trump a ##¢ ##aa as part of the story . the dc ##cc which has seen plenty of candidates brought down by residence issues came to view her as une ##le ##ctable . a ##¢ ##aa ##vot ##ers in houston have organized for over a year to hold [ republican rep . john ab ##ney ] cu ##lb ##erson accountable and win this clinton district a ##¢ ##aa said dc ##cc communications director meredith kelly . tgt tgt as a general election candidate and would rob voters of their opportunity to flip texas ##a ##¢ ##aa ##s 7th in november . a ##¢ ##aa the dc ##cca ##¢ ##aa ##s memo hit mo ##ser for paying revolution messaging the firm where her husband arun cha ##ud ##har ##y had worked since 2012 to produce her tv ads . but its main attack lines were backed up by an article mo ##ser wrote for washington ##ian in 2014 where she joked that she a ##¢ ##aa ##d a ##¢ ##aa ##so ##one ##r have my teeth pulled out without an ##esthesia ##a ##¢ ##aa than move to her grandparents ##a ##¢ ##aa home in paris tex . paris is a small city more than 300 miles from the 7th district ; the dc ##cc warned that voters would not process the nu ##ance . both mo ##ser a [SEP]\n",
            "I1120 00:19:31.990304 139937064892288 run_classifier.py:465] input_ids: 101 1 1012 1006 2745 2358 2527 22879 2080 2005 1996 2899 2695 1007 1996 3537 7740 3049 2837 2003 2635 2543 2013 1996 2187 2044 2706 1997 5816 3521 4283 2000 1037 6254 2404 3784 1996 2305 1997 13114 1012 2570 1037 29645 11057 1037 3074 1997 4559 2470 2006 1 1037 6555 4018 1999 3146 2050 29645 11057 2015 6975 5504 7740 2212 1012 2004 1 2001 7052 1037 3049 2724 1996 5887 9468 6866 1037 2460 2021 12077 3074 1997 4978 2006 9587 8043 2008 2052 2022 3497 2109 2114 2014 2065 2016 2180 1996 3537 6488 1012 1996 5504 2212 2003 2028 1997 2603 2073 18520 7207 2041 18155 3709 6221 8398 1999 2355 2096 7206 2741 1037 3951 12295 2000 2899 1012 1037 29645 11057 3207 5302 17510 7206 2342 2000 2963 2008 1 2003 2025 2183 2000 2689 2899 1037 29645 11057 1996 2837 2626 1012 1037 29645 11057 1 2003 1037 2899 25297 2040 11693 28121 28392 2333 2000 5395 2000 2448 2005 3519 1012 1037 29645 11057 9587 8043 5838 2101 9432 2305 3038 1999 1037 4861 2008 1996 5887 16665 29645 11057 2015 9887 3421 1037 29645 11057 2860 10536 2111 5223 4331 2050 29645 11057 1998 3480 1996 3947 2000 2202 3519 2067 2013 10643 1012 1037 29645 11057 8545 2050 29645 26526 2063 2109 2000 7823 2831 2182 1999 3146 2021 2009 2050 29645 11057 2015 15640 2000 2963 2009 2013 2899 25631 1037 29645 11057 2016 2056 1012 1 2106 20102 2000 2014 3128 5395 2197 2095 1998 2211 2770 2005 3519 2007 2014 2693 2067 2188 1037 29645 11057 2044 12106 3679 2895 1037 3049 6994 2005 6555 2015 10206 2011 8398 1037 29645 11057 2004 2112 1997 1996 2466 1012 1996 5887 9468 2029 2038 2464 7564 1997 5347 2716 2091 2011 5039 3314 2234 2000 3193 2014 2004 16655 2571 23576 1012 1037 29645 11057 22994 2545 1999 5395 2031 4114 2005 2058 1037 2095 2000 2907 1031 3951 16360 1012 2198 11113 5420 1033 12731 20850 18617 26771 1998 2663 2023 7207 2212 1037 29645 11057 2056 5887 9468 4806 2472 10635 5163 1012 1 1 2004 1037 2236 2602 4018 1998 2052 6487 7206 1997 2037 4495 2000 11238 3146 2050 29645 11057 2015 5504 1999 2281 1012 1037 29645 11057 1996 5887 16665 29645 11057 2015 24443 2718 9587 8043 2005 7079 4329 24732 1996 3813 2073 2014 3129 28217 15775 6784 8167 2100 2018 2499 2144 2262 2000 3965 2014 2694 14997 1012 2021 2049 2364 2886 3210 2020 6153 2039 2011 2019 3720 9587 8043 2626 2005 2899 2937 1999 2297 2073 2016 19700 2008 2016 1037 29645 11057 2094 1037 29645 11057 6499 5643 2099 2031 2026 4091 2766 2041 2302 2019 25344 2050 29645 11057 2084 2693 2000 2014 14472 2050 29645 11057 2188 1999 3000 16060 1012 3000 2003 1037 2235 2103 2062 2084 3998 2661 2013 1996 5504 2212 1025 1996 5887 9468 7420 2008 7206 2052 2025 2832 1996 16371 6651 1012 2119 9587 8043 1037 102\n",
            "I1120 00:19:31.991431 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:19:31.992707 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:31.993549 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:19:32.005109 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:19:32.010648 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:19:32.012691 139937064892288 run_classifier.py:464] tokens: [CLS] tgt ##turn ##ed 104 on monday jan . 1 2018 . tgt believes the secret to tgt longevity is tgt daily consumption of diet coke . ( photo : w ##zz ##m - tv grand rapids - kala ##ma ##zoo - battle creek mic ##h . ) grand rapids mic ##h . a tgt ##has been a resident at a michigan retirement community for the last 15 years . tgt moved into sentinel pointe retirement community in grand rapids mic ##h . when tgt was 89 . on monday tgt turned 104 . \" i ' m surprised that i ' m 104 \" said tgt ##on tuesday . it just doesn ' t seem like i should be that old . \" when asked when she and her family moved to grand rapids tgt ##co ##uld ##n ' t remember the exact year only saying that tgt was born in illinois in 1914 then moved to north dakota before eventually coming to michigan . \" when i was 100 i thought i ' d never be 104 ; i thought i ' d pass away by that time but it just didn ' t happen tgt ##sai ##d . \" then i turn 101 and nothing happens . maybe the reason why nothing has happened is because of tgt daily intake of tgt favorite beverage a diet coke . \" i drink it because i like it tgt ##sai ##d . tgt said [ mask ] was going shopping wednesday because tgt needed more diet coke . \" i have a bag full of empty diet coke cans that i need to return to buy more diet coke . tgt ##sai ##d tgt drinks at least one can of diet coke each day . on tuesday morning tgt ##cel ##eb ##rated tgt 104 ##th birthday in style with a cake and with the support of many of the members of tgt retirement community . [SEP]\n",
            "I1120 00:19:32.015865 139937064892288 run_classifier.py:465] input_ids: 101 1 22299 2098 9645 2006 6928 5553 1012 1015 2760 1012 1 7164 1996 3595 2000 1 26906 2003 1 3679 8381 1997 8738 14492 1012 1006 6302 1024 1059 13213 2213 1011 2694 2882 12775 1011 26209 2863 23221 1011 2645 3636 23025 2232 1012 1007 2882 12775 23025 2232 1012 1037 1 14949 2042 1037 6319 2012 1037 4174 5075 2451 2005 1996 2197 2321 2086 1012 1 2333 2046 16074 26119 5075 2451 1999 2882 12775 23025 2232 1012 2043 1 2001 6486 1012 2006 6928 1 2357 9645 1012 1000 1045 1005 1049 4527 2008 1045 1005 1049 9645 1000 2056 1 2239 9857 1012 2009 2074 2987 1005 1056 4025 2066 1045 2323 2022 2008 2214 1012 1000 2043 2356 2043 2016 1998 2014 2155 2333 2000 2882 12775 1 3597 21285 2078 1005 1056 3342 1996 6635 2095 2069 3038 2008 1 2001 2141 1999 4307 1999 4554 2059 2333 2000 2167 7734 2077 2776 2746 2000 4174 1012 1000 2043 1045 2001 2531 1045 2245 1045 1005 1040 2196 2022 9645 1025 1045 2245 1045 1005 1040 3413 2185 2011 2008 2051 2021 2009 2074 2134 1005 1056 4148 1 15816 2094 1012 1000 2059 1045 2735 7886 1998 2498 6433 1012 2672 1996 3114 2339 2498 2038 3047 2003 2138 1997 1 3679 13822 1997 1 5440 19645 1037 8738 14492 1012 1000 1045 4392 2009 2138 1045 2066 2009 1 15816 2094 1012 1 2056 1031 7308 1033 2001 2183 6023 9317 2138 1 2734 2062 8738 14492 1012 1000 1045 2031 1037 4524 2440 1997 4064 8738 14492 18484 2008 1045 2342 2000 2709 2000 4965 2062 8738 14492 1012 1 15816 2094 1 8974 2012 2560 2028 2064 1997 8738 14492 2169 2154 1012 2006 9857 2851 1 29109 15878 9250 1 9645 2705 5798 1999 2806 2007 1037 9850 1998 2007 1996 2490 1997 2116 1997 1996 2372 1997 1 5075 2451 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.018523 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.020246 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.021441 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:19:32.036592 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:19:32.038753 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:19:32.041567 139937064892288 run_classifier.py:464] tokens: [CLS] image copyright supplied image capt ##ion tgt lost tgt job when tgt told [ mask ] employer about tgt depression tgt was diagnosed with chronic depression in tgt early twenties : \" i ' d been bot ##tling up quite a bit through most of my teens . then it hit me a like a brick wall \" tgt said . tgt was so depressed tgt didn ' t leave the house for six months . things began to improve and when tgt was 23 years - old tgt got a job at a food and drink company . but tgt had weekly counsel ##ling to attend . \" every thursday i ' d have to take the afternoon off . so i spoke to my boss . in the meeting tgt was fine with it and said ' do what you need to do ' \" . but after the meeting tgt received an email being told tgt was going to be taken off tgt accounts and that tgt should think about seeking employment at a mental health charity as \" they might relate \" . the manager also told people in the office : \" it went round . it was horrible - i felt really exposed \" she said . working there for a few further months triggered hay ##ley to experience anxiety depression and \" massive trust issues \" . eventually she left . she started her own company in london which she still runs five years on : \" in the long - term it ' s given me awareness of mental health - and the awareness to make sure i was never going to make people feel the way i was made to feel . \" [SEP]\n",
            "I1120 00:19:32.043009 139937064892288 run_classifier.py:465] input_ids: 101 3746 9385 8127 3746 14408 3258 1 2439 1 3105 2043 1 2409 1031 7308 1033 11194 2055 1 6245 1 2001 11441 2007 11888 6245 1999 1 2220 18946 1024 1000 1045 1005 1040 2042 28516 15073 2039 3243 1037 2978 2083 2087 1997 2026 13496 1012 2059 2009 2718 2033 1037 2066 1037 5318 2813 1000 1 2056 1012 1 2001 2061 14777 1 2134 1005 1056 2681 1996 2160 2005 2416 2706 1012 2477 2211 2000 5335 1998 2043 1 2001 2603 2086 1011 2214 1 2288 1037 3105 2012 1037 2833 1998 4392 2194 1012 2021 1 2018 4882 9517 2989 2000 5463 1012 1000 2296 9432 1045 1005 1040 2031 2000 2202 1996 5027 2125 1012 2061 1045 3764 2000 2026 5795 1012 1999 1996 3116 1 2001 2986 2007 2009 1998 2056 1005 2079 2054 2017 2342 2000 2079 1005 1000 1012 2021 2044 1996 3116 1 2363 2019 10373 2108 2409 1 2001 2183 2000 2022 2579 2125 1 6115 1998 2008 1 2323 2228 2055 6224 6107 2012 1037 5177 2740 5952 2004 1000 2027 2453 14396 1000 1012 1996 3208 2036 2409 2111 1999 1996 2436 1024 1000 2009 2253 2461 1012 2009 2001 9202 1011 1045 2371 2428 6086 1000 2016 2056 1012 2551 2045 2005 1037 2261 2582 2706 13330 10974 3051 2000 3325 10089 6245 1998 1000 5294 3404 3314 1000 1012 2776 2016 2187 1012 2016 2318 2014 2219 2194 1999 2414 2029 2016 2145 3216 2274 2086 2006 1024 1000 1999 1996 2146 1011 2744 2009 1005 1055 2445 2033 7073 1997 5177 2740 1011 1998 1996 7073 2000 2191 2469 1045 2001 2196 2183 2000 2191 2111 2514 1996 2126 1045 2001 2081 2000 2514 1012 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.046782 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.048573 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.050163 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:19:32.078773 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:19:32.080388 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:19:32.082726 139937064892288 run_classifier.py:464] tokens: [CLS] conservative christians in america love tgt . while other republican presidential candidates , namely ben carson and mike hu ##ck ##abe ##e , are also extremely popular with many christians , particularly evangelical ##s , tgt continues to pull in more and more of the conservative christian vote . for the third year in the row , tgt won the values voter summit presidential straw poll , \" the largest gathering of evangelical voters from across the country . \" so why do christians gr ##avi ##tate toward tgt ? tgt is a passionate defender of religious liberty . no political leader is a stronger defender of religious liberty than cruz , who has pushed back strongly against the erosion of constitutional rights , battling \" liberal fascist ##s \" during a time when religious freedom has been tested in this country to a degree it has not seen in recent history . \" we were founded by men and women fleeing religious persecution , \" tgt said at the iowa faith and freedom coalition summit this year . tgt is una ##fra ##id to take on the ` anti - christian ' left . tgt is a fighter , repeatedly proving that tgt ' s not only willing to go to bat for christians , tgt will call out those whom tgt perceive ##s to be \" anti - christian \" - - particularly those on the radical left . \" today ' s democratic party has decided there is no room for christians in today ' s democratic party , tgt said in april . in an interview on sean han ##nity ' s show in october , tgt responded to the president ' s claim in the new york review of books that \" it seems as if folks who take religion the most seriously sometimes are also those who are suspicious of those not like them . \" tgt told han ##nity that obama ' s comment was indicative of the \" strong anti - american streak \" and \" anti ##pathy for christians \" among those , like obama , on the \" far left . \" tgt went on to praise america - - which is still a majority christian country - - as \" an incredible force of good in this world \" which has \" spilled more blood \" for freedom than any other country . \" and yet , the far - left , out of which barack obama arose , des ##pis ##es american leadership in the world , \" said tgt . tgt appears to be a genuine believer . by all accounts , tgt is not just \" pol ##itic ##king \" about tgt christian principles . one moment captured on a cell phone after a campaign event in october revealed just one example of many in which tgt demonstrated tgt genuine belief in the power of prayer and tgt pastoral inclination ##s - l ##rb - still shot from facebook post below - rr ##b - : [SEP]\n",
            "I1120 00:19:32.084773 139937064892288 run_classifier.py:465] input_ids: 101 4603 8135 1999 2637 2293 1 1012 2096 2060 3951 4883 5347 1010 8419 3841 9806 1998 3505 15876 3600 16336 2063 1010 2024 2036 5186 2759 2007 2116 8135 1010 3391 11295 2015 1010 1 4247 2000 4139 1999 2062 1998 2062 1997 1996 4603 3017 3789 1012 2005 1996 2353 2095 1999 1996 5216 1010 1 2180 1996 5300 14303 6465 4883 13137 8554 1010 1000 1996 2922 7215 1997 11295 7206 2013 2408 1996 2406 1012 1000 2061 2339 2079 8135 24665 18891 12259 2646 1 1029 1 2003 1037 13459 8291 1997 3412 7044 1012 2053 2576 3003 2003 1037 6428 8291 1997 3412 7044 2084 8096 1010 2040 2038 3724 2067 6118 2114 1996 14173 1997 6543 2916 1010 17773 1000 4314 14870 2015 1000 2076 1037 2051 2043 3412 4071 2038 2042 7718 1999 2023 2406 2000 1037 3014 2009 2038 2025 2464 1999 3522 2381 1012 1000 2057 2020 2631 2011 2273 1998 2308 14070 3412 14522 1010 1000 1 2056 2012 1996 5947 4752 1998 4071 6056 6465 2023 2095 1012 1 2003 14477 27843 3593 2000 2202 2006 1996 1036 3424 1011 3017 1005 2187 1012 1 2003 1037 4959 1010 8385 13946 2008 1 1005 1055 2025 2069 5627 2000 2175 2000 7151 2005 8135 1010 1 2097 2655 2041 2216 3183 1 23084 2015 2000 2022 1000 3424 1011 3017 1000 1011 1011 3391 2216 2006 1996 7490 2187 1012 1000 2651 1005 1055 3537 2283 2038 2787 2045 2003 2053 2282 2005 8135 1999 2651 1005 1055 3537 2283 1010 1 2056 1999 2258 1012 1999 2019 4357 2006 5977 7658 22758 1005 1055 2265 1999 2255 1010 1 5838 2000 1996 2343 1005 1055 4366 1999 1996 2047 2259 3319 1997 2808 2008 1000 2009 3849 2004 2065 12455 2040 2202 4676 1996 2087 5667 2823 2024 2036 2216 2040 2024 10027 1997 2216 2025 2066 2068 1012 1000 1 2409 7658 22758 2008 8112 1005 1055 7615 2001 24668 1997 1996 1000 2844 3424 1011 2137 9039 1000 1998 1000 3424 20166 2005 8135 1000 2426 2216 1010 2066 8112 1010 2006 1996 1000 2521 2187 1012 1000 1 2253 2006 2000 8489 2637 1011 1011 2029 2003 2145 1037 3484 3017 2406 1011 1011 2004 1000 2019 9788 2486 1997 2204 1999 2023 2088 1000 2029 2038 1000 13439 2062 2668 1000 2005 4071 2084 2151 2060 2406 1012 1000 1998 2664 1010 1996 2521 1011 2187 1010 2041 1997 2029 13857 8112 10375 1010 4078 18136 2229 2137 4105 1999 1996 2088 1010 1000 2056 1 1012 1 3544 2000 2022 1037 10218 24591 1012 2011 2035 6115 1010 1 2003 2025 2074 1000 14955 18291 6834 1000 2055 1 3017 6481 1012 2028 2617 4110 2006 1037 3526 3042 2044 1037 3049 2724 1999 2255 3936 2074 2028 2742 1997 2116 1999 2029 1 7645 1 10218 6772 1999 1996 2373 1997 7083 1998 1 13645 21970 2015 1011 1048 15185 1011 2145 2915 2013 9130 2695 2917 1011 25269 2497 1011 1024 102\n",
            "I1120 00:19:32.087033 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:19:32.089209 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.090607 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:19:32.113365 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:19:32.115825 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:19:32.118191 139937064892288 run_classifier.py:464] tokens: [CLS] travis kala ##nick has to get uber ' s bet on self - driving cars right . \" it starts with understanding that the world is going to go self - driving and autonomous \" he told business insider in an interview . \" so if that ' s happening what would happen if we weren ' t a part of that future ? if we weren ' t part of the autonomy thing ? then the future passes us by basically in a very ex ##ped ##iti ##ous and efficient way \" he said . these are big bold and aggressive moves characteristic of the uber that has already changed how millions get around in cities across the globe . but they also show the growing existent ##ial threat that kala ##nick feels if tgt tgt company doesn ' t wake up to the impending revolution in transportation . and tgt ' s not willing to ce ##de any ground to giants with hundreds of billions of dollars in the bank like apple or google . \" if we are not tied for first then the person who is in first or the entity that ' s in first then rolls out a ride - sharing network that is far cheaper or far higher - quality than uber ' s then uber is no longer a thing \" kala ##nick said . business insider spoke to kala ##nick about what an uber that relies on self - driving cars looks like a ##¢ ##aa and what will happen to all of those drivers who to ##te people around if this is its future . kala ##nick : i think it starts with understanding that the world is going to go self - driving and autonomous . because well a million fewer people are going to die a year . traffic in all cities will be gone . significantly reduced pollution and trillion ##s of hours will be given back to people a ##¢ ##aa quality of life goes way up . once you go \" all right there ' s a lot of upside ##s there \" and you have folks like the folks in mountain view [ california ] a few different companies working hard on this problem this thing is going to happen . tgt : that is the trillion - dollar question and i wish i had an answer for you on that one but i don ' t . what i know is that i can ' t be wrong . right ? i have to make sure that i ' m ready when it ' s ready or that i ' m making it ready . so i have to be tied for first at the least . kala ##nick : no i don ' t . i think we ' re catching up . look at some of the folks and how long they ' ve been working on it . lots of respect for an area that they ' ve pioneered and [SEP]\n",
            "I1120 00:19:32.120781 139937064892288 run_classifier.py:465] input_ids: 101 10001 26209 13542 2038 2000 2131 19169 1005 1055 6655 2006 2969 1011 4439 3765 2157 1012 1000 2009 4627 2007 4824 2008 1996 2088 2003 2183 2000 2175 2969 1011 4439 1998 8392 1000 2002 2409 2449 25297 1999 2019 4357 1012 1000 2061 2065 2008 1005 1055 6230 2054 2052 4148 2065 2057 4694 1005 1056 1037 2112 1997 2008 2925 1029 2065 2057 4694 1005 1056 2112 1997 1996 12645 2518 1029 2059 1996 2925 5235 2149 2011 10468 1999 1037 2200 4654 5669 25090 3560 1998 8114 2126 1000 2002 2056 1012 2122 2024 2502 7782 1998 9376 5829 8281 1997 1996 19169 2008 2038 2525 2904 2129 8817 2131 2105 1999 3655 2408 1996 7595 1012 2021 2027 2036 2265 1996 3652 25953 4818 5081 2008 26209 13542 5683 2065 1 1 2194 2987 1005 1056 5256 2039 2000 1996 17945 4329 1999 5193 1012 1998 1 1005 1055 2025 5627 2000 8292 3207 2151 2598 2000 7230 2007 5606 1997 25501 1997 6363 1999 1996 2924 2066 6207 2030 8224 1012 1000 2065 2057 2024 2025 5079 2005 2034 2059 1996 2711 2040 2003 1999 2034 2030 1996 9178 2008 1005 1055 1999 2034 2059 9372 2041 1037 4536 1011 6631 2897 2008 2003 2521 16269 2030 2521 3020 1011 3737 2084 19169 1005 1055 2059 19169 2003 2053 2936 1037 2518 1000 26209 13542 2056 1012 2449 25297 3764 2000 26209 13542 2055 2054 2019 19169 2008 16803 2006 2969 1011 4439 3765 3504 2066 1037 29645 11057 1998 2054 2097 4148 2000 2035 1997 2216 6853 2040 2000 2618 2111 2105 2065 2023 2003 2049 2925 1012 26209 13542 1024 1045 2228 2009 4627 2007 4824 2008 1996 2088 2003 2183 2000 2175 2969 1011 4439 1998 8392 1012 2138 2092 1037 2454 8491 2111 2024 2183 2000 3280 1037 2095 1012 4026 1999 2035 3655 2097 2022 2908 1012 6022 4359 10796 1998 23458 2015 1997 2847 2097 2022 2445 2067 2000 2111 1037 29645 11057 3737 1997 2166 3632 2126 2039 1012 2320 2017 2175 1000 2035 2157 2045 1005 1055 1037 2843 1997 14961 2015 2045 1000 1998 2017 2031 12455 2066 1996 12455 1999 3137 3193 1031 2662 1033 1037 2261 2367 3316 2551 2524 2006 2023 3291 2023 2518 2003 2183 2000 4148 1012 1 1024 2008 2003 1996 23458 1011 7922 3160 1998 1045 4299 1045 2018 2019 3437 2005 2017 2006 2008 2028 2021 1045 2123 1005 1056 1012 2054 1045 2113 2003 2008 1045 2064 1005 1056 2022 3308 1012 2157 1029 1045 2031 2000 2191 2469 2008 1045 1005 1049 3201 2043 2009 1005 1055 3201 2030 2008 1045 1005 1049 2437 2009 3201 1012 2061 1045 2031 2000 2022 5079 2005 2034 2012 1996 2560 1012 26209 13542 1024 2053 1045 2123 1005 1056 1012 1045 2228 2057 1005 2128 9105 2039 1012 2298 2012 2070 1997 1996 12455 1998 2129 2146 2027 1005 2310 2042 2551 2006 2009 1012 7167 1997 4847 2005 2019 2181 2008 2027 1005 2310 16193 1998 102\n",
            "I1120 00:19:32.123199 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:19:32.124417 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:19:32.125368 139937064892288 run_classifier.py:468] label: True (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can not load train features, creating train features: [Errno 2] No such file or directory: 'train_features_large.dms'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1120 00:22:02.623788 139937064892288 run_classifier.py:774] Writing example 10000 of 45944\n",
            "I1120 00:24:31.776660 139937064892288 run_classifier.py:774] Writing example 20000 of 45944\n",
            "I1120 00:27:05.898261 139937064892288 run_classifier.py:774] Writing example 30000 of 45944\n",
            "I1120 00:29:38.821140 139937064892288 run_classifier.py:774] Writing example 40000 of 45944\n",
            "I1120 00:32:34.991987 139937064892288 run_classifier.py:774] Writing example 0 of 7187\n",
            "I1120 00:32:35.015783 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:32:35.017024 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:32:35.018743 139937064892288 run_classifier.py:464] tokens: [CLS] with a flourish stil ##p sets his two - sided flag on fire and drops it in the trash can . a ##¢ ##aa ##i con ##si ##gn this flag to the waste bin of history ! a ##¢ ##aa stil ##p says as he is dramatically enveloped in white smoke . later at a nearby diner stil ##p smiles content ##edly and reflects . \" today went well \" he says before cho ##mp ##ing into a ve ##gg ##ie burger . this is hardly his first rodeo . he has been dubbed a ##¢ ##aa ##the p . t . barn ##um of pennsylvania politics ##a ##¢ ##aa and is regularly referred to by friends and foe ##s alike as a a ##¢ ##aa ##master of political theater . a ##¢ ##aa stil ##pa ##¢ ##aa ##w ##ho lives in the harrisburg area ##a ##¢ ##aa ##has made a name for himself through a variety of creative prop - laden media - court ##ing protest stunts . he ' s particularly well - known in the largely rural swat ##h between philadelphia and pittsburgh that folks call a ##¢ ##aa ##pen ##ns ##yl ##tu ##cky . \" but with his confederate / nazi flag - burning demonstrations stil ##p says a ##¢ ##aa ##you dona ##¢ ##aa ##t bring humor to this at all . when you have heather hey ##er dying at the hands of a neo - nazi people who are flying these flags to represent hate and white supremacy everything is 1 000 percent serious . a ##¢ ##aa a ##¢ ##aa ##all across pennsylvania you see the confederate flag on people ##a ##¢ ##aa ##s porch ##es so we confront them with the true meaning a ##¢ ##aa stil ##p says . a ##¢ ##aa ##it puts them on the defensive . you know show me why your confederate flag isn ##a ##¢ ##aa ##t equal to the nazi flag ? explain to me why . they say a ##¢ ##aa ##we can ##a ##¢ ##aa ##t help it if it ##a ##¢ ##aa ##s been adopted by the k ##kk and white su ##pre ##mac ##ist groups . a ##¢ ##aa but it ##a ##¢ ##aa ##s not my job to walk it back to where you want it to be . that ##a ##¢ ##aa ##s the present reality and it always was a symbol of hate . a ##¢ ##aa stil ##p says he was motivated to make and burn his two - sided flags after seeing white su ##pre ##mac ##ists carrying both flags during the charlotte ##sville a ##¢ ##aa ##uni ##te the right ##a ##¢ ##aa rally . he was also incense ##d by scott perry ##a ##¢ ##aa ##a us congressman from mid - state pennsylvania ##a ##¢ ##aa ##ech ##oin ##g trump in blaming a ##¢ ##aa ##bot ##h sides ##a ##¢ ##aa for the associated violence and hey ##era ##¢ ##aa ##s death . stil ##p a ##¢ ##aa ##s no stranger to flags . [SEP]\n",
            "I1120 00:32:35.019983 139937064892288 run_classifier.py:465] input_ids: 101 2007 1037 24299 25931 2361 4520 2010 2048 1011 11536 5210 2006 2543 1998 9010 2009 1999 1996 11669 2064 1012 1037 29645 11057 2072 9530 5332 16206 2023 5210 2000 1996 5949 8026 1997 2381 999 1037 29645 11057 25931 2361 2758 2004 2002 2003 12099 25407 1999 2317 5610 1012 2101 2012 1037 3518 15736 25931 2361 8451 4180 26207 1998 11138 1012 1000 2651 2253 2092 1000 2002 2758 2077 16480 8737 2075 2046 1037 2310 13871 2666 15890 1012 2023 2003 6684 2010 2034 18936 1012 2002 2038 2042 9188 1037 29645 11057 10760 1052 1012 1056 1012 8659 2819 1997 3552 4331 2050 29645 11057 1998 2003 5570 3615 2000 2011 2814 1998 22277 2015 11455 2004 1037 1037 29645 11057 8706 1997 2576 4258 1012 1037 29645 11057 25931 4502 29645 11057 2860 6806 3268 1999 1996 24569 2181 2050 29645 11057 14949 2081 1037 2171 2005 2370 2083 1037 3528 1997 5541 17678 1011 14887 2865 1011 2457 2075 6186 28465 1012 2002 1005 1055 3391 2092 1011 2124 1999 1996 4321 3541 25414 2232 2090 4407 1998 6278 2008 12455 2655 1037 29645 11057 11837 3619 8516 8525 17413 1012 1000 2021 2007 2010 8055 1013 6394 5210 1011 5255 13616 25931 2361 2758 1037 29645 11057 29337 24260 29645 11057 2102 3288 8562 2000 2023 2012 2035 1012 2043 2017 2031 9533 4931 2121 5996 2012 1996 2398 1997 1037 9253 1011 6394 2111 2040 2024 3909 2122 9245 2000 5050 5223 1998 2317 22006 2673 2003 1015 2199 3867 3809 1012 1037 29645 11057 1037 29645 11057 8095 2408 3552 2017 2156 1996 8055 5210 2006 2111 2050 29645 11057 2015 7424 2229 2061 2057 14323 2068 2007 1996 2995 3574 1037 29645 11057 25931 2361 2758 1012 1037 29645 11057 4183 8509 2068 2006 1996 5600 1012 2017 2113 2265 2033 2339 2115 8055 5210 3475 2050 29645 11057 2102 5020 2000 1996 6394 5210 1029 4863 2000 2033 2339 1012 2027 2360 1037 29645 11057 8545 2064 2050 29645 11057 2102 2393 2009 2065 2009 2050 29645 11057 2015 2042 4233 2011 1996 1047 19658 1998 2317 10514 28139 22911 2923 2967 1012 1037 29645 11057 2021 2009 2050 29645 11057 2015 2025 2026 3105 2000 3328 2009 2067 2000 2073 2017 2215 2009 2000 2022 1012 2008 2050 29645 11057 2015 1996 2556 4507 1998 2009 2467 2001 1037 6454 1997 5223 1012 1037 29645 11057 25931 2361 2758 2002 2001 12774 2000 2191 1998 6402 2010 2048 1011 11536 9245 2044 3773 2317 10514 28139 22911 5130 4755 2119 9245 2076 1996 5904 9337 1037 29645 11057 19496 2618 1996 2157 2050 29645 11057 8320 1012 2002 2001 2036 28647 2094 2011 3660 6890 2050 29645 11057 2050 2149 12295 2013 3054 1011 2110 3552 2050 29645 11057 15937 28765 2290 8398 1999 24114 1037 29645 11057 18384 2232 3903 2050 29645 11057 2005 1996 3378 4808 1998 4931 6906 29645 11057 2015 2331 1012 25931 2361 1037 29645 11057 2015 2053 7985 2000 9245 1012 102\n",
            "I1120 00:32:35.021744 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:32:35.023197 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.024262 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:32:35.053488 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:32:35.054714 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:32:35.057507 139937064892288 run_classifier.py:464] tokens: [CLS] ever since tgt and his patriots won a come - from - behind super bowl in february the nfl superstar has been living his best life leading up to the 2017 season . from goo ##fin ##g off with gr ##on ##k to ditch ##ing trump and even sumo wrestling in japan ( yes really ) here ' s everything tommy boy has been up to since . related love him or hate him tgt is football ' s winning ##est quarterback new england patriots ' star tied peyton manning for combined regular season and playoff wins and he could break the record next week tgt you just won super bowl 51 : what are you going to do now ? ! in lieu of a trip to disneyland tgt dec ##amp ##ed to laid ##back montana following the festivities in boston . that ' s where sports illustrated ' s peter king spent the sunday after super bowl li with the fresh champ at his mountain cabin where tgt did some skiing and reflected on tgt victory . there tgt told told king \" i have zero pain . i feel great . i feel 100 percent . \" brady himself didn ' t take a trip to the south of the border but his jersey did . cap ##ping off weeks of fever ##ed speculation and searching the nfl announced that tgt tgt prized super bowl ga ##rb was recovered in mexico the jersey stolen by a globe - tr ##otti ##ng newspaper man - slash - memorabilia dealer . the story has a happy ending though ! on april 2nd tgt finally got that sucker back and thus our long national nightmare was finally over . proving that brady ' s passion for playing at a top level knows no limits the star qb took took the greens of augusta with fellow athletic powerhouse jordan sp ##ie ##th for a round in advance of the masters . according to tgt sp ##ie ##th ' s golf prowess was no match for him . \" trying to beat @ jordan ##sp ##ie ##th in golf is like trying to arm wrestle @ the ##rock \" he posted under an ins ##tagram of the two before adding the hash ##tag ##s # dream ##bi ##g and # never ##gi ##ve ##up . tom you ' re a super bowl champ mana ##¢ ##a ##¦ isn ' t that enough ? ! tgt and tgt patriots co ##hort ##s continued their championship victory lap by paying the red sox a visit on the team ' s opening day . naturally gr ##on ##k had to gr ##on ##k it up and grab tgt ' s super bowl jersey an obvious lamp ##oon of the great jersey he ##ist of 2017 . usually no athlete bats an eye when they ' re invited to the white house to celebrate their monumental win whether the world series or super bowl . however when that white house is currently occupied a certain controversial [SEP]\n",
            "I1120 00:32:35.059135 139937064892288 run_classifier.py:465] input_ids: 101 2412 2144 1 1998 2010 11579 2180 1037 2272 1011 2013 1011 2369 3565 4605 1999 2337 1996 5088 18795 2038 2042 2542 2010 2190 2166 2877 2039 2000 1996 2418 2161 1012 2013 27571 16294 2290 2125 2007 24665 2239 2243 2000 14033 2075 8398 1998 2130 28193 4843 1999 2900 1006 2748 2428 1007 2182 1005 1055 2673 6838 2879 2038 2042 2039 2000 2144 1012 3141 2293 2032 2030 5223 2032 1 2003 2374 1005 1055 3045 4355 9074 2047 2563 11579 1005 2732 5079 17931 11956 2005 4117 3180 2161 1998 7808 5222 1998 2002 2071 3338 1996 2501 2279 2733 1 2017 2074 2180 3565 4605 4868 1024 2054 2024 2017 2183 2000 2079 2085 1029 999 1999 22470 1997 1037 4440 2000 25104 1 11703 16613 2098 2000 4201 5963 8124 2206 1996 21206 1999 3731 1012 2008 1005 1055 2073 2998 7203 1005 1055 2848 2332 2985 1996 4465 2044 3565 4605 5622 2007 1996 4840 24782 2012 2010 3137 6644 2073 1 2106 2070 12701 1998 7686 2006 1 3377 1012 2045 1 2409 2409 2332 1000 1045 2031 5717 3255 1012 1045 2514 2307 1012 1045 2514 2531 3867 1012 1000 10184 2370 2134 1005 1056 2202 1037 4440 2000 1996 2148 1997 1996 3675 2021 2010 3933 2106 1012 6178 4691 2125 3134 1997 9016 2098 12143 1998 6575 1996 5088 2623 2008 1 1 25695 3565 4605 11721 15185 2001 6757 1999 3290 1996 3933 7376 2011 1037 7595 1011 19817 21325 3070 3780 2158 1011 18296 1011 28663 11033 1012 1996 2466 2038 1037 3407 4566 2295 999 2006 2258 3416 1 2633 2288 2008 26476 2067 1998 2947 2256 2146 2120 10103 2001 2633 2058 1012 13946 2008 10184 1005 1055 6896 2005 2652 2012 1037 2327 2504 4282 2053 6537 1996 2732 26171 2165 2165 1996 15505 1997 13635 2007 3507 5188 24006 5207 11867 2666 2705 2005 1037 2461 1999 5083 1997 1996 5972 1012 2429 2000 1 11867 2666 2705 1005 1055 5439 26120 2001 2053 2674 2005 2032 1012 1000 2667 2000 3786 1030 5207 13102 2666 2705 1999 5439 2003 2066 2667 2000 2849 25579 1030 1996 16901 1000 2002 6866 2104 2019 16021 23091 1997 1996 2048 2077 5815 1996 23325 15900 2015 1001 3959 5638 2290 1998 1001 2196 5856 3726 6279 1012 3419 2017 1005 2128 1037 3565 4605 24782 24951 29645 2050 29649 3475 1005 1056 2008 2438 1029 999 1 1998 1 11579 2522 27794 2015 2506 2037 2528 3377 5001 2011 7079 1996 2417 9175 1037 3942 2006 1996 2136 1005 1055 3098 2154 1012 8100 24665 2239 2243 2018 2000 24665 2239 2243 2009 2039 1998 6723 1 1005 1055 3565 4605 3933 2019 5793 10437 7828 1997 1996 2307 3933 2002 2923 1997 2418 1012 2788 2053 8258 12236 2019 3239 2043 2027 1005 2128 4778 2000 1996 2317 2160 2000 8439 2037 15447 2663 3251 1996 2088 2186 2030 3565 4605 1012 2174 2043 2008 2317 2160 2003 2747 4548 1037 3056 6801 102\n",
            "I1120 00:32:35.061605 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:32:35.063133 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.064790 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:32:35.078654 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:32:35.079716 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:32:35.081182 139937064892288 run_classifier.py:464] tokens: [CLS] tgt ##on monday blasted net neutrality activists who protested outside tgt home with signs directed at tgt children saying the demonstration across ##es a line . a the protests followed tgt ##say ##ing last week that tgt would follow through on tgt pledge to repeal obama - era internet regulations . the move set up a showdown with consumer groups but the backlash recently reached pa ##i ' s own home - - with activists putting up cardboard signs that ask if this is the world he wants his children to \" inherit . \" one sign says \" they will come to know the truth . dad murdered democracy in cold blood . \" ai ##t certainly crosses a line with me a tgt ##to ##ld fox news ##a af ##ox & friends ##a on monday . af ##ami ##lies a ##¦ should remain out of it and stop hara ##ssing us at our homes . . . . it was a little nerve - rack ##ing especially for my wife whoa ##s not involved in this space . a but tgt ##ar ##gues the rules discourage the is ##ps from making investments in their network that would provide even better and faster online access . he also said monday that he thinks the five - member federal communications commission will vote 3 - 2 in favor of repeal ##ing the regulations . the vote is scheduled for dec . 14 . at ##he solution is not more regulations a he said . at ##he problem was this was slapping on more regulations . a [ mask ] has reportedly distributed tgt alternative plan to other commissioners . fellow commissioner mig ##non l . cl ##y ##burn has called tgt as proposals aa give ##away to the nation ##as largest communications companies at the expense of consumers and innovation . a [SEP]\n",
            "I1120 00:32:35.082767 139937064892288 run_classifier.py:465] input_ids: 101 1 2239 6928 18461 5658 21083 10134 2040 11456 2648 1 2188 2007 5751 2856 2012 1 2336 3038 1996 10467 2408 2229 1037 2240 1012 1037 1996 8090 2628 1 24322 2075 2197 2733 2008 1 2052 3582 2083 2006 1 16393 2000 21825 8112 1011 3690 4274 7040 1012 1996 2693 2275 2039 1037 24419 2007 7325 2967 2021 1996 25748 3728 2584 6643 2072 1005 1055 2219 2188 1011 1011 2007 10134 5128 2039 19747 5751 2008 3198 2065 2023 2003 1996 2088 2002 4122 2010 2336 2000 1000 22490 1012 1000 2028 3696 2758 1000 2027 2097 2272 2000 2113 1996 3606 1012 3611 7129 7072 1999 3147 2668 1012 1000 9932 2102 5121 7821 1037 2240 2007 2033 1037 1 3406 6392 4419 2739 2050 21358 11636 1004 2814 2050 2006 6928 1012 21358 10631 11983 1037 29649 2323 3961 2041 1997 2009 1998 2644 18820 18965 2149 2012 2256 5014 1012 1012 1012 1012 2009 2001 1037 2210 9113 1011 14513 2075 2926 2005 2026 2564 23281 2015 2025 2920 1999 2023 2686 1012 1037 2021 1 2906 22967 1996 3513 28085 1996 2003 4523 2013 2437 10518 1999 2037 2897 2008 2052 3073 2130 2488 1998 5514 3784 3229 1012 2002 2036 2056 6928 2008 2002 6732 1996 2274 1011 2266 2976 4806 3222 2097 3789 1017 1011 1016 1999 5684 1997 21825 2075 1996 7040 1012 1996 3789 2003 5115 2005 11703 1012 2403 1012 2012 5369 5576 2003 2025 2062 7040 1037 2002 2056 1012 2012 5369 3291 2001 2023 2001 22021 2006 2062 7040 1012 1037 1031 7308 1033 2038 7283 5500 1 4522 2933 2000 2060 12396 1012 3507 5849 19117 8540 1048 1012 18856 2100 8022 2038 2170 1 2004 10340 9779 2507 9497 2000 1996 3842 3022 2922 4806 3316 2012 1996 10961 1997 10390 1998 8144 1012 1037 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.083862 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.085851 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.087080 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:32:35.112479 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:32:35.113647 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:32:35.115374 139937064892288 run_classifier.py:464] tokens: [CLS] but tgt were so startling and so disturbing tgt parents felt compelled to do more . corey a who had never been in trouble or shown any suicidal tendencies a killed himself after being questioned about a disciplinary matter by authorities at school and his parents believe the tragedy was entirely prevent ##able . they decided if they wanted to truly honor corey as life they needed to do something that would educate students about their rights warn parents about potential risks and remind educators about their responsibilities . to achieve that goal they recently launched corey as goal a nonprofit organization dedicated to raising awareness of the constitutional rights of minors in school settings and providing education on how disciplinary practices in schools can better support the emotional well - being of students . they plan speaking engagements and a website filled with information on the topic to help spread their message . awe need to start telling people that this could happen in their schools a maureen wal ##gren said . awe need to get this information in front of them and hopefully influence change . a corey a junior at nape ##r ##ville north high school was summoned to administrative offices jan . 11 2017 to meet with dean stephen madden and nape ##r ##ville police officer brett he ##un who is assigned to the campus . the two questioned corey about an all ##ega ##tion that he had a video of a con ##sen ##su ##al sexual encounter with a female classmate on his phone and that he had played the recording for friends . he ##un and madden accused tgt and threatened tgt with placement on the state ' s sex offender registry according to a lawsuit filed earlier this year by the wal ##gren family . he ##un and madden spoke with corey for 18 minutes according to police reports before calling his parents . once they reached maureen wal ##gren he ##un talked to her on a speaker ##phone with tgt ##t ##gt told her that her son was being investigated for \" child pornography \" and could end up being placed on the sex offender registry maureen wal ##gren has said . the officer then told maureen wal ##gren he intended to download items from corey ' s phone and needed her consent according to the reports . wal ##gren a nurse said she would leave work immediately and be at the school within 50 minutes . after the phone call ended the officer and the dean walked corey to an office waiting area and told him to sit down . the two then went to speak with the school ##as principal leaving corey to wait for his mother . left by himself tgt ##walk ##ed out of the school a few minutes later and headed toward a downtown nape ##r ##ville parking garage . tgt climbed to the fifth floor and then ended tgt life by plum ##met ##ing 53 feet to the ground . tgt mother who had arrived at [SEP]\n",
            "I1120 00:32:35.117008 139937064892288 run_classifier.py:465] input_ids: 101 2021 1 2020 2061 19828 1998 2061 14888 1 3008 2371 15055 2000 2079 2062 1012 18132 1037 2040 2018 2196 2042 1999 4390 2030 3491 2151 26094 20074 1037 2730 2370 2044 2108 8781 2055 1037 17972 3043 2011 4614 2012 2082 1998 2010 3008 2903 1996 10576 2001 4498 4652 3085 1012 2027 2787 2065 2027 2359 2000 5621 3932 18132 2004 2166 2027 2734 2000 2079 2242 2008 2052 16957 2493 2055 2037 2916 11582 3008 2055 4022 10831 1998 10825 19156 2055 2037 10198 1012 2000 6162 2008 3125 2027 3728 3390 18132 2004 3125 1037 14495 3029 4056 2000 6274 7073 1997 1996 6543 2916 1997 18464 1999 2082 10906 1998 4346 2495 2006 2129 17972 6078 1999 2816 2064 2488 2490 1996 6832 2092 1011 2108 1997 2493 1012 2027 2933 4092 20583 1998 1037 4037 3561 2007 2592 2006 1996 8476 2000 2393 3659 2037 4471 1012 15180 2342 2000 2707 4129 2111 2008 2023 2071 4148 1999 2037 2816 1037 19167 24547 13565 2056 1012 15180 2342 2000 2131 2023 2592 1999 2392 1997 2068 1998 11504 3747 2689 1012 1037 18132 1037 3502 2012 23634 2099 3077 2167 2152 2082 2001 11908 2000 3831 4822 5553 1012 2340 2418 2000 3113 2007 4670 4459 24890 1998 23634 2099 3077 2610 2961 12049 2002 4609 2040 2003 4137 2000 1996 3721 1012 1996 2048 8781 18132 2055 2019 2035 29107 3508 2008 2002 2018 1037 2678 1997 1037 9530 5054 6342 2389 4424 8087 2007 1037 2931 23175 2006 2010 3042 1998 2008 2002 2018 2209 1996 3405 2005 2814 1012 2002 4609 1998 24890 5496 1 1998 5561 1 2007 11073 2006 1996 2110 1005 1055 3348 25042 15584 2429 2000 1037 9870 6406 3041 2023 2095 2011 1996 24547 13565 2155 1012 2002 4609 1998 24890 3764 2007 18132 2005 2324 2781 2429 2000 2610 4311 2077 4214 2010 3008 1012 2320 2027 2584 19167 24547 13565 2002 4609 5720 2000 2014 2006 1037 5882 9864 2007 1 2102 13512 2409 2014 2008 2014 2365 2001 2108 10847 2005 1000 2775 19378 1000 1998 2071 2203 2039 2108 2872 2006 1996 3348 25042 15584 19167 24547 13565 2038 2056 1012 1996 2961 2059 2409 19167 24547 13565 2002 3832 2000 8816 5167 2013 18132 1005 1055 3042 1998 2734 2014 9619 2429 2000 1996 4311 1012 24547 13565 1037 6821 2056 2016 2052 2681 2147 3202 1998 2022 2012 1996 2082 2306 2753 2781 1012 2044 1996 3042 2655 3092 1996 2961 1998 1996 4670 2939 18132 2000 2019 2436 3403 2181 1998 2409 2032 2000 4133 2091 1012 1996 2048 2059 2253 2000 3713 2007 1996 2082 3022 4054 2975 18132 2000 3524 2005 2010 2388 1012 2187 2011 2370 1 17122 2098 2041 1997 1996 2082 1037 2261 2781 2101 1998 3753 2646 1037 5116 23634 2099 3077 5581 7381 1012 1 6589 2000 1996 3587 2723 1998 2059 3092 1 2166 2011 22088 11368 2075 5187 2519 2000 1996 2598 1012 1 2388 2040 2018 3369 2012 102\n",
            "I1120 00:32:35.118139 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:32:35.120138 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.121134 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:32:35.139101 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:32:35.140196 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:32:35.141947 139937064892288 run_classifier.py:464] tokens: [CLS] a prosecutor said tgt admitted shooting long and another soldier \" because of what they had done to muslims in the past . \" an fbi - led joint terrorism task force based in the southern united states has been investigating tgt since tgt returned to the united states from yemen a law enforcement official said . tgt had been arrested and jailed in yemen at some point for using a somali passport the official said . the time of that arrest was not immediately clear . assistant u . s . attorney pat harris said no decision has been made on whether to pursue federal charges against tgt . \" we ' re consulting with a lot of people on what if any charges can be filed against tgt \" harris said . police sgt . cassandra davis said tuesday it wasn ' t known when tgt began planning the attack . deputy prosecutor scott duncan said tgt told investigators that \" tgt would have killed more soldiers had they been in the parking lot . \" long and [ mask ] were targeted as they stood outside the recruiting center smoking cigarettes . tgt did not speak during the brief hearing before judge alice light ##le . john re ##hra ##uer spokesman for the pu ##las ##ki county jail said the department was handling tgt as it does other high - profile cases . \" tgt is in some protective custody in a higher - security unit in a cell by tgt \" re ##hra ##uer said . tgt said tgt didn ' t know of any threats against tgt and said jail ##ers did not believe tgt was in any greater danger than previous high - profile people handled at the prison . a police report said tgt told investigators tgt observed two soldiers in uniform drove up to the recruiting center and began shooting . \" tgt saw them standing there and drove up and shot them \" lt . terry hastings told the associated press . \" that ' s what tgt said . \" police arrested tgt along a highway moments after the shooting ##s . police said an assault rifle and other weapons were found in tgt vehicle . in addition to the capital murder count tgt is accused of committing 16 counts of a terrorist ##ic act . thomas said most additional counts resulted from the gunfire occurring near other people . [SEP]\n",
            "I1120 00:32:35.143668 139937064892288 run_classifier.py:465] input_ids: 101 1037 12478 2056 1 4914 5008 2146 1998 2178 5268 1000 2138 1997 2054 2027 2018 2589 2000 7486 1999 1996 2627 1012 1000 2019 8495 1011 2419 4101 10130 4708 2486 2241 1999 1996 2670 2142 2163 2038 2042 11538 1 2144 1 2513 2000 1996 2142 2163 2013 13968 1037 2375 7285 2880 2056 1012 1 2018 2042 4727 1998 21278 1999 13968 2012 2070 2391 2005 2478 1037 16831 12293 1996 2880 2056 1012 1996 2051 1997 2008 6545 2001 2025 3202 3154 1012 3353 1057 1012 1055 1012 4905 6986 5671 2056 2053 3247 2038 2042 2081 2006 3251 2000 7323 2976 5571 2114 1 1012 1000 2057 1005 2128 10552 2007 1037 2843 1997 2111 2006 2054 2065 2151 5571 2064 2022 6406 2114 1 1000 5671 2056 1012 2610 17001 1012 15609 4482 2056 9857 2009 2347 1005 1056 2124 2043 1 2211 4041 1996 2886 1012 4112 12478 3660 7343 2056 1 2409 14766 2008 1000 1 2052 2031 2730 2062 3548 2018 2027 2042 1999 1996 5581 2843 1012 1000 2146 1998 1031 7308 1033 2020 9416 2004 2027 2768 2648 1996 14357 2415 9422 15001 1012 1 2106 2025 3713 2076 1996 4766 4994 2077 3648 5650 2422 2571 1012 2198 2128 13492 13094 14056 2005 1996 16405 8523 3211 2221 7173 2056 1996 2533 2001 8304 1 2004 2009 2515 2060 2152 1011 6337 3572 1012 1000 1 2003 1999 2070 9474 9968 1999 1037 3020 1011 3036 3131 1999 1037 3526 2011 1 1000 2128 13492 13094 2056 1012 1 2056 1 2134 1005 1056 2113 1997 2151 8767 2114 1 1998 2056 7173 2545 2106 2025 2903 1 2001 1999 2151 3618 5473 2084 3025 2152 1011 6337 2111 8971 2012 1996 3827 1012 1037 2610 3189 2056 1 2409 14766 1 5159 2048 3548 1999 6375 5225 2039 2000 1996 14357 2415 1998 2211 5008 1012 1000 1 2387 2068 3061 2045 1998 5225 2039 1998 2915 2068 1000 8318 1012 6609 12296 2409 1996 3378 2811 1012 1000 2008 1005 1055 2054 1 2056 1012 1000 2610 4727 1 2247 1037 3307 5312 2044 1996 5008 2015 1012 2610 2056 2019 6101 5883 1998 2060 4255 2020 2179 1999 1 4316 1012 1999 2804 2000 1996 3007 4028 4175 1 2003 5496 1997 16873 2385 9294 1997 1037 9452 2594 2552 1012 2726 2056 2087 3176 9294 4504 2013 1996 16978 10066 2379 2060 2111 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.145530 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.147196 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:32:35.148219 139937064892288 run_classifier.py:468] label: False (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can not load from GC, creating dev features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1120 00:33:54.353492 139937064892288 run_classifier.py:774] Writing example 0 of 8130\n",
            "I1120 00:33:54.380841 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:33:54.384027 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:33:54.387252 139937064892288 run_classifier.py:464] tokens: [CLS] but one billionaire investor and former trump adviser tgt ##was seemingly un ##ve ##xed having dumped a million shares tied to the steel industry a week before the president announced 25 percent for foreign - made steel . a feb . 22 sec filing shows tgt ##sol ##d off tgt $ 31 . 3 million stake in the mani ##tow ##oc company which is a leading global manufacturer of cranes for heavy construction based in mani ##tow ##oc wi ##s . according to the company ' s website . since trump ' s announcement thursday mani ##tow ##oc ' s stock has plum ##met ##ed to about $ 26 . ic ##ah ##n - who has had majority interest in several companies including motorola x ##ero ##x family dollar and pep boys - had sold his shares for about $ 32 to $ 34 each according to the filing . tgt ##had not actively traded any mani ##tow ##oc stock since january 2015 according to regulatory filing ##s . trump and ic ##ah ##n ' s history is one of friends turned foe ##s turned friends . it began in the early 1980s when trump tried to win tgt ##over with a helicopter ride . in 1988 when trump paid $ 11 million to host a heavyweight title fight between mike tyson and michael spin ##ks in atlantic city new jersey trump took ic ##ah ##n known at the time for his series of hostile corporate takeover ##s backstage to meet tyson . during the announcer ' s roll call of famous guests tgt ##was called trump ' s \" good friend \" according to the new yorker . in the early 1990s tgt ##head ##ed the deal that allowed trump to keep some of his power and ownership of his ta ##j mahal casino during its first bout with bankruptcy . trump ' s share of the casino about 50 percent was a generous one but tgt ##art ##iculate ##d that investors could still boot trump out . many involved in the deal thought tgt ##had out ##man ##eu ##vered trump according to the washington post ' s drew ha ##r ##well . by 2010 trump entertainment resorts was flooded with debt and tgt ##pus ##hed to take over saying trump ' s brand had become a \" disadvantage \" that may no longer have been \" synonymous with business ac ##umen high quality . . . and enormous success . \" tensions appeared to have eased by the time trump ran for president as he spoke fond ##ly of tgt ##cal ##ling him one of the \" great businessmen of the world \" smart enough to tackle u . s . negotiations with china . in return tgt ##end ##ors ##ed trump saying the country would be \" lucky \" to have him in the oval office ha ##r ##well reported . though tgt ##no longer advises trump in a formal role the two reportedly still talk . tgt ##res ##ign ##ed from tgt position as a \" [SEP]\n",
            "I1120 00:33:54.388478 139937064892288 run_classifier.py:465] input_ids: 101 2021 2028 22301 14316 1998 2280 8398 11747 1 17311 9428 4895 3726 19068 2383 14019 1037 2454 6661 5079 2000 1996 3886 3068 1037 2733 2077 1996 2343 2623 2423 3867 2005 3097 1011 2081 3886 1012 1037 13114 1012 2570 10819 15242 3065 1 19454 2094 2125 1 1002 2861 1012 1017 2454 8406 1999 1996 23624 18790 10085 2194 2029 2003 1037 2877 3795 7751 1997 27083 2005 3082 2810 2241 1999 23624 18790 10085 15536 2015 1012 2429 2000 1996 2194 1005 1055 4037 1012 2144 8398 1005 1055 8874 9432 23624 18790 10085 1005 1055 4518 2038 22088 11368 2098 2000 2055 1002 2656 1012 24582 4430 2078 1011 2040 2038 2018 3484 3037 1999 2195 3316 2164 29583 1060 10624 2595 2155 7922 1998 27233 3337 1011 2018 2853 2010 6661 2005 2055 1002 3590 2000 1002 4090 2169 2429 2000 1996 15242 1012 1 16102 2025 8851 7007 2151 23624 18790 10085 4518 2144 2254 2325 2429 2000 10738 15242 2015 1012 8398 1998 24582 4430 2078 1005 1055 2381 2003 2028 1997 2814 2357 22277 2015 2357 2814 1012 2009 2211 1999 1996 2220 3865 2043 8398 2699 2000 2663 1 7840 2007 1037 7739 4536 1012 1999 2997 2043 8398 3825 1002 2340 2454 2000 3677 1037 8366 2516 2954 2090 3505 19356 1998 2745 6714 5705 1999 4448 2103 2047 3933 8398 2165 24582 4430 2078 2124 2012 1996 2051 2005 2010 2186 1997 10420 5971 15336 2015 20212 2000 3113 19356 1012 2076 1996 14073 1005 1055 4897 2655 1997 3297 6368 1 17311 2170 8398 1005 1055 1000 2204 2767 1000 2429 2000 1996 2047 19095 1012 1999 1996 2220 4134 1 4974 2098 1996 3066 2008 3039 8398 2000 2562 2070 1997 2010 2373 1998 6095 1997 2010 11937 3501 27913 9270 2076 2049 2034 10094 2007 10528 1012 8398 1005 1055 3745 1997 1996 9270 2055 2753 3867 2001 1037 12382 2028 2021 1 8445 24153 2094 2008 9387 2071 2145 9573 8398 2041 1012 2116 2920 1999 1996 3066 2245 1 16102 2041 2386 13765 25896 8398 2429 2000 1996 2899 2695 1005 1055 3881 5292 2099 4381 1012 2011 2230 8398 4024 16511 2001 10361 2007 7016 1998 1 12207 9072 2000 2202 2058 3038 8398 1005 1055 4435 2018 2468 1037 1000 20502 1000 2008 2089 2053 2936 2031 2042 1000 22594 2007 2449 9353 27417 2152 3737 1012 1012 1012 1998 8216 3112 1012 1000 13136 2596 2000 2031 10987 2011 1996 2051 8398 2743 2005 2343 2004 2002 3764 13545 2135 1997 1 9289 2989 2032 2028 1997 1996 1000 2307 17353 1997 1996 2088 1000 6047 2438 2000 11147 1057 1012 1055 1012 7776 2007 2859 1012 1999 2709 1 10497 5668 2098 8398 3038 1996 2406 2052 2022 1000 5341 1000 2000 2031 2032 1999 1996 9242 2436 5292 2099 4381 2988 1012 2295 1 3630 2936 25453 8398 1999 1037 5337 2535 1996 2048 7283 2145 2831 1012 1 6072 23773 2098 2013 1 2597 2004 1037 1000 102\n",
            "I1120 00:33:54.394226 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:33:54.396018 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.397335 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:33:54.409101 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:33:54.410293 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:33:54.412086 139937064892288 run_classifier.py:464] tokens: [CLS] este ##r led ##eck ##a makes history winning olympic gold in both snow ##boarding and skiing with a gold medal in the snow ##board parallel giant slalom [ mask ] has become the first woman in winter olympics history to win a gold medal in two different sports at the same olympic games . on saturday she emerged victorious in the final run against germany ' s se ##lina joe ##rg finishing just 0 . 46 seconds ahead . joe ##rg took silver in the event followed by ramon ##a there ##sia ho ##fm ##eis ##ter also of germany who took bronze . a week earlier tgt ##sur ##pr ##ised spectators and herself by taking a gold medal a in skiing . tgt ##fin ##ished first in the women ' s super - g in alpine skiing finishing several spots ahead of the decorated american skier lindsey von ##n . but tgt ##is best known and has seen the most success as a snow ##board ##er where tgt ' s won world titles and competed in the winter games in soc ##hi four years ago . tgt ##is the third athlete to win gold in two events in the same winter games according to analytics company grace ##note . the last time it happened was in 1928 with johan [UNK] taking gold in the nordic combined and cross - country skiing . four years earlier thor ##lei ##f ha ##ug achieved the same thing the company says . the 22 - year - old led ##eck ##a earlier made olympic history just by being the first woman to compete in both the alpine skiing and snow ##boarding events . she first ski ##ed at age 2 and snow ##board ##ed at age 5 according to the new york times but refused to follow the conventional wisdom of her coaches to give up one sport in order to special ##ize . [SEP]\n",
            "I1120 00:33:54.413444 139937064892288 run_classifier.py:465] input_ids: 101 28517 2099 2419 11012 2050 3084 2381 3045 4386 2751 1999 2119 4586 21172 1998 12701 2007 1037 2751 3101 1999 1996 4586 6277 5903 5016 19617 1031 7308 1033 2038 2468 1996 2034 2450 1999 3467 3783 2381 2000 2663 1037 2751 3101 1999 2048 2367 2998 2012 1996 2168 4386 2399 1012 2006 5095 2016 6003 13846 1999 1996 2345 2448 2114 2762 1005 1055 7367 13786 3533 10623 5131 2074 1014 1012 4805 3823 3805 1012 3533 10623 2165 3165 1999 1996 2724 2628 2011 12716 2050 2045 8464 7570 16715 17580 3334 2036 1997 2762 2040 2165 4421 1012 1037 2733 3041 1 26210 18098 5084 12405 1998 2841 2011 2635 1037 2751 3101 1037 1999 12701 1012 1 16294 13295 2034 1999 1996 2308 1005 1055 3565 1011 1043 1999 10348 12701 5131 2195 7516 3805 1997 1996 7429 2137 21294 17518 3854 2078 1012 2021 1 2483 2190 2124 1998 2038 2464 1996 2087 3112 2004 1037 4586 6277 2121 2073 1 1005 1055 2180 2088 4486 1998 3879 1999 1996 3467 2399 1999 27084 4048 2176 2086 3283 1012 1 2483 1996 2353 8258 2000 2663 2751 1999 2048 2824 1999 1996 2168 3467 2399 2429 2000 25095 2194 4519 22074 1012 1996 2197 2051 2009 3047 2001 1999 4662 2007 13093 100 2635 2751 1999 1996 13649 4117 1998 2892 1011 2406 12701 1012 2176 2086 3041 15321 23057 2546 5292 15916 4719 1996 2168 2518 1996 2194 2758 1012 1996 2570 1011 2095 1011 2214 2419 11012 2050 3041 2081 4386 2381 2074 2011 2108 1996 2034 2450 2000 5566 1999 2119 1996 10348 12701 1998 4586 21172 2824 1012 2016 2034 8301 2098 2012 2287 1016 1998 4586 6277 2098 2012 2287 1019 2429 2000 1996 2047 2259 2335 2021 4188 2000 3582 1996 7511 9866 1997 2014 7850 2000 2507 2039 2028 4368 1999 2344 2000 2569 4697 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.414819 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.416704 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.417897 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:33:54.431946 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:33:54.433701 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:33:54.436168 139937064892288 run_classifier.py:464] tokens: [CLS] tgt will return from [ mask ] espn suspension on monday a little more than two weeks after the network sanctioned her for suggesting that nfl fans could boycott ad ##vert ##iser ##s and vendors associated with the dallas cowboys whose owner jerry jones has ordered players to stand for the national anthem or be bench ##ed . we haven ' t heard much at all from hill over the course of the suspension - she hasn ' t been active on twitter - but t ##m ##z bumped into her at los angeles international airport and got her to say a few words about what happened namely that she understands why she was punished by the network . \" so here ' s how this works : it doesn ' t really matter what i think . it matters to people but here ' s the reality : espn acted what they felt was right and you know i don ' t have any argument or qui ##bble with that . i would tell people absolutely after my donald trump t ##wee ##ts i deserved that suspension . i deserved it . like absolutely . i violated the policy ; i deserved that suspension \" she said . before the suspension hill had drawn the ir ##e of her espn bosses when she called trump a \" white su ##pre ##mac ##ist \" on twitter . she was warned about making such statements but was not punished at the time . hill also was asked about the speculation that she would leave espn . [SEP]\n",
            "I1120 00:33:54.437611 139937064892288 run_classifier.py:465] input_ids: 101 1 2097 2709 2013 1031 7308 1033 10978 8636 2006 6928 1037 2210 2062 2084 2048 3134 2044 1996 2897 14755 2014 2005 9104 2008 5088 4599 2071 17757 4748 16874 17288 2015 1998 17088 3378 2007 1996 5759 11666 3005 3954 6128 3557 2038 3641 2867 2000 3233 2005 1996 2120 11971 2030 2022 6847 2098 1012 2057 4033 1005 1056 2657 2172 2012 2035 2013 2940 2058 1996 2607 1997 1996 8636 1011 2016 8440 1005 1056 2042 3161 2006 10474 1011 2021 1056 2213 2480 19030 2046 2014 2012 3050 3349 2248 3199 1998 2288 2014 2000 2360 1037 2261 2616 2055 2054 3047 8419 2008 2016 19821 2339 2016 2001 14248 2011 1996 2897 1012 1000 2061 2182 1005 1055 2129 2023 2573 1024 2009 2987 1005 1056 2428 3043 2054 1045 2228 1012 2009 5609 2000 2111 2021 2182 1005 1055 1996 4507 1024 10978 6051 2054 2027 2371 2001 2157 1998 2017 2113 1045 2123 1005 1056 2031 2151 6685 2030 21864 11362 2007 2008 1012 1045 2052 2425 2111 7078 2044 2026 6221 8398 1056 28394 3215 1045 10849 2008 8636 1012 1045 10849 2009 1012 2066 7078 1012 1045 14424 1996 3343 1025 1045 10849 2008 8636 1000 2016 2056 1012 2077 1996 8636 2940 2018 4567 1996 20868 2063 1997 2014 10978 23029 2043 2016 2170 8398 1037 1000 2317 10514 28139 22911 2923 1000 2006 10474 1012 2016 2001 7420 2055 2437 2107 8635 2021 2001 2025 14248 2012 1996 2051 1012 2940 2036 2001 2356 2055 1996 12143 2008 2016 2052 2681 10978 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.439416 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.440773 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.441900 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:33:54.448563 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:33:54.450318 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:33:54.451611 139937064892288 run_classifier.py:464] tokens: [CLS] tgt ##is with ##holding support for the go ##p tax bill until [ mask ] sees changes that would help small businesses . that ##as the word monday from the montana law ##maker . tgt ##say ##s tgt wants to ensure that the tax bill doesn ##at put small businesses at a competitive disadvantage against large corporations . tgt says abe ##for ##e i can support this bill this improvement needs to be made . a tgt says tgt as optimistic and will continue working with tgt go ##p colleagues on the issue . tgt ##jo ##ins republican sen . ron johnson of wisconsin in expressing opposition to the bill in its current form . [SEP]\n",
            "I1120 00:33:54.454859 139937064892288 run_classifier.py:465] input_ids: 101 1 2483 2007 23410 2490 2005 1996 2175 2361 4171 3021 2127 1031 7308 1033 5927 3431 2008 2052 2393 2235 5661 1012 2008 3022 1996 2773 6928 2013 1996 8124 2375 8571 1012 1 24322 2015 1 4122 2000 5676 2008 1996 4171 3021 2987 4017 2404 2235 5661 2012 1037 6975 20502 2114 2312 11578 1012 1 2758 14863 29278 2063 1045 2064 2490 2023 3021 2023 7620 3791 2000 2022 2081 1012 1037 1 2758 1 2004 21931 1998 2097 3613 2551 2007 1 2175 2361 8628 2006 1996 3277 1012 1 5558 7076 3951 12411 1012 6902 3779 1997 5273 1999 14026 4559 2000 1996 3021 1999 2049 2783 2433 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.461738 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.463141 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.464236 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:33:54.490333 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:33:54.491548 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:33:54.494002 139937064892288 run_classifier.py:464] tokens: [CLS] tgt stood shoulder to shoulder with [ mask ] fierce ##st religious allies . flanked by a huge sign for tgt ' s senate campaign one supporter rail ##ed against the \" lgbt mafia \" and \" homosexual ##ist gay terrorism . \" another warned that \" homosexual so ##dom ##y \" destroys those who participate in it and the nations that allow it . and still another described same - sex marriage as \" a mirage \" because \" it ' s ph ##ony and fake . \" thursday ' s news conference was designed to send a powerful message to the political world that religious conservatives across america remain committed to moore a christian conservative and former judge whose alabama senate campaign has been rocked by mounting allegations of sexual misconduct . the event also revealed an aggressive strain of homo ##phobia rarely seen in mainstream politics a ##¢ ##aa in recent years at least . but in a senate campaign suddenly hyper focused on tgt ' s relationships with teenage girls decades ago tgt ' s hard ##line stance on gay rights and other lgbt issues has become little more than an after ##th ##ough ##t for many voters as election day approaches . tgt first caught the attention of many in the lgbt community after describing homosexual conduct as \" an inherent evil against which children must be protected \" in a 2002 child custody case involving a lesbian mother . in a 2005 television interview tgt said \" homosexual conduct should be illegal . \" tgt also said there ' s no difference between gay sex and sex with a cow horse or dog . tgt ' s stand a ##¢ ##aa combined with the fiery comments from tgt supporters a ##¢ ##aa un ##ner ##ved some in birmingham ' s relatively small lgbt community . indeed other lgbt activists suggested this week that open acceptance of tgt ' s anti - gay rhetoric ha ##rke ##ns to a dark and violent time in alabama history . tgt ' s democratic challenger doug jones is known best perhaps for prose ##cuting the men who bombed birmingham ' s 16th street baptist church a ##¢ ##aa a prosecution that came nearly 40 years after the 1963 crime that killed four black girls . racial tensions have lingered in the state even as the violence less ##ened . in 2000 alabama became the last state in the country to over ##turn its ban on inter ##rac ##ial marriage . she ' s not laughing at tgt . in contrast to many conservative politicians with national ambitions moore has made little attempt to change his tone on lgbt issues as equal rights for the gay community has earned increasing acceptance among mainstream america . tgt ' s hero status among many christian conservatives was cement ##ed in 2016 when as the chief justice of the alabama supreme court he refused to comply with a supreme court ruling that legal ##ized same - sex marriage nationwide . he was [SEP]\n",
            "I1120 00:33:54.495932 139937064892288 run_classifier.py:465] input_ids: 101 1 2768 3244 2000 3244 2007 1031 7308 1033 9205 3367 3412 6956 1012 15874 2011 1037 4121 3696 2005 1 1005 1055 4001 3049 2028 10129 4334 2098 2114 1996 1000 12010 13897 1000 1998 1000 15667 2923 5637 10130 1012 1000 2178 7420 2008 1000 15667 2061 9527 2100 1000 20735 2216 2040 5589 1999 2009 1998 1996 3741 2008 3499 2009 1012 1998 2145 2178 2649 2168 1011 3348 3510 2004 1000 1037 21483 1000 2138 1000 2009 1005 1055 6887 16585 1998 8275 1012 1000 9432 1005 1055 2739 3034 2001 2881 2000 4604 1037 3928 4471 2000 1996 2576 2088 2008 3412 11992 2408 2637 3961 5462 2000 5405 1037 3017 4603 1998 2280 3648 3005 6041 4001 3049 2038 2042 14215 2011 15986 9989 1997 4424 23337 1012 1996 2724 2036 3936 2019 9376 10178 1997 24004 24920 6524 2464 1999 7731 4331 1037 29645 11057 1999 3522 2086 2012 2560 1012 2021 1999 1037 4001 3049 3402 23760 4208 2006 1 1005 1055 6550 2007 9454 3057 5109 3283 1 1005 1055 2524 4179 11032 2006 5637 2916 1998 2060 12010 3314 2038 2468 2210 2062 2084 2019 2044 2705 10593 2102 2005 2116 7206 2004 2602 2154 8107 1012 1 2034 3236 1996 3086 1997 2116 1999 1996 12010 2451 2044 7851 15667 6204 2004 1000 2019 16112 4763 2114 2029 2336 2442 2022 5123 1000 1999 1037 2526 2775 9968 2553 5994 1037 11690 2388 1012 1999 1037 2384 2547 4357 1 2056 1000 15667 6204 2323 2022 6206 1012 1000 1 2036 2056 2045 1005 1055 2053 4489 2090 5637 3348 1998 3348 2007 1037 11190 3586 2030 3899 1012 1 1005 1055 3233 1037 29645 11057 4117 2007 1996 15443 7928 2013 1 6793 1037 29645 11057 4895 3678 7178 2070 1999 6484 1005 1055 4659 2235 12010 2451 1012 5262 2060 12010 10134 4081 2023 2733 2008 2330 9920 1997 1 1005 1055 3424 1011 5637 17871 5292 25074 3619 2000 1037 2601 1998 6355 2051 1999 6041 2381 1012 1 1005 1055 3537 12932 8788 3557 2003 2124 2190 3383 2005 12388 29163 1996 2273 2040 18897 6484 1005 1055 5767 2395 7550 2277 1037 29645 11057 1037 11537 2008 2234 3053 2871 2086 2044 1996 3699 4126 2008 2730 2176 2304 3057 1012 5762 13136 2031 17645 1999 1996 2110 2130 2004 1996 4808 2625 6675 1012 1999 2456 6041 2150 1996 2197 2110 1999 1996 2406 2000 2058 22299 2049 7221 2006 6970 22648 4818 3510 1012 2016 1005 1055 2025 5870 2012 1 1012 1999 5688 2000 2116 4603 8801 2007 2120 19509 5405 2038 2081 2210 3535 2000 2689 2010 4309 2006 12010 3314 2004 5020 2916 2005 1996 5637 2451 2038 3687 4852 9920 2426 7731 2637 1012 1 1005 1055 5394 3570 2426 2116 3017 11992 2001 11297 2098 1999 2355 2043 2004 1996 2708 3425 1997 1996 6041 4259 2457 2002 4188 2000 14037 2007 1037 4259 2457 6996 2008 3423 3550 2168 1011 3348 3510 9053 1012 2002 2001 102\n",
            "I1120 00:33:54.497154 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:33:54.499043 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:33:54.500297 139937064892288 run_classifier.py:468] label: True (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45944\n",
            "7187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1120 00:35:40.853589 139937064892288 run_classifier.py:774] Writing example 0 of 12604\n",
            "I1120 00:35:40.897443 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:35:40.898987 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:35:40.904114 139937064892288 run_classifier.py:464] tokens: [CLS] tgt this week slapped substantial tariffs on imports of large washing machines and solar cells . the move drew particular attention because president trump ##a ##¢ ##aa ##s protection ##ist rhetoric had not to date been matched with significant new protection . do the new tariffs mark the start of a trade war ? just how serious is the situation ? the military has a way of addressing such questions : def ##con a scale ranging from 5 ( tran ##quil ) to 1 ( most alarm ##ing ) is meant to portray a ##¢ ##aa ##de ##fen ##se conditions . a ##¢ ##aa we do not have a corresponding measure in trade but given the concerns about where tgt is heading we need one . so ##a ##¢ ##aa ##¦ this has been our recent norm . certain practices to imp ##ede trade have gained fairly wide acceptance . prominent among these are anti - dumping ( ad ) and counter ##va ##iling duty ( cv ##d ) procedures . these are permitted under the w ##to and result in substantial amounts of protection . but in the united states the white house has no role in the process . while tgt has tried to take credit for some of these cases they are normally launched by businesses and decided by the u . s . international trade commission ( it ##c ) and the commerce department . nominally the president could not stop a case even if he wanted to . that may be unfortunate for consumers but it also means that new tariffs applied this way are not usually taken as evidence of a country ##a ##¢ ##aa ##s aggressive policy . . this has been our recent norm . certain practices to imp ##ede trade have gained fairly wide acceptance . prominent among these are anti - dumping ( ad ) and counter ##va ##iling duty ( cv ##d ) procedures . these are permitted under the w ##to and result in substantial amounts of protection . but in the united states the white house has no role in the process . while tgt has tried to take credit for some of these cases they are normally launched by businesses and decided by the u . s . international trade commission ( it ##c ) and the commerce department . nominally the president could not stop a case even if he wanted to . that may be unfortunate for consumers but it also means that new tariffs applied this way are not usually taken as evidence of a country ##a ##¢ ##aa ##s aggressive policy . tr ##ad ##con 3 . discretion ##ary unusual protection . the president also has discretion over the breadth of the protection applied including which export ##ing countries might be exempt from the new tariffs . president trump notably included mexico in the new tariffs even though the it ##c had determined that mexican washing machine tariffs posed no threat to u . s . industry . . here is where [SEP]\n",
            "I1120 00:35:40.908363 139937064892288 run_classifier.py:465] input_ids: 101 1 2023 2733 11159 6937 26269 2006 17589 1997 2312 12699 6681 1998 5943 4442 1012 1996 2693 3881 3327 3086 2138 2343 8398 2050 29645 11057 2015 3860 2923 17871 2018 2025 2000 3058 2042 10349 2007 3278 2047 3860 1012 2079 1996 2047 26269 2928 1996 2707 1997 1037 3119 2162 1029 2074 2129 3809 2003 1996 3663 1029 1996 2510 2038 1037 2126 1997 12786 2107 3980 1024 13366 8663 1037 4094 7478 2013 1019 1006 25283 26147 1007 2000 1015 1006 2087 8598 2075 1007 2003 3214 2000 17279 1037 29645 11057 3207 18940 3366 3785 1012 1037 29645 11057 2057 2079 2025 2031 1037 7978 5468 1999 3119 2021 2445 1996 5936 2055 2073 1 2003 5825 2057 2342 2028 1012 2061 2050 29645 11057 29649 2023 2038 2042 2256 3522 13373 1012 3056 6078 2000 17727 14728 3119 2031 4227 7199 2898 9920 1012 4069 2426 2122 2024 3424 1011 23642 1006 4748 1007 1998 4675 3567 16281 4611 1006 26226 2094 1007 8853 1012 2122 2024 7936 2104 1996 1059 3406 1998 2765 1999 6937 8310 1997 3860 1012 2021 1999 1996 2142 2163 1996 2317 2160 2038 2053 2535 1999 1996 2832 1012 2096 1 2038 2699 2000 2202 4923 2005 2070 1997 2122 3572 2027 2024 5373 3390 2011 5661 1998 2787 2011 1996 1057 1012 1055 1012 2248 3119 3222 1006 2009 2278 1007 1998 1996 6236 2533 1012 24207 1996 2343 2071 2025 2644 1037 2553 2130 2065 2002 2359 2000 1012 2008 2089 2022 15140 2005 10390 2021 2009 2036 2965 2008 2047 26269 4162 2023 2126 2024 2025 2788 2579 2004 3350 1997 1037 2406 2050 29645 11057 2015 9376 3343 1012 1012 2023 2038 2042 2256 3522 13373 1012 3056 6078 2000 17727 14728 3119 2031 4227 7199 2898 9920 1012 4069 2426 2122 2024 3424 1011 23642 1006 4748 1007 1998 4675 3567 16281 4611 1006 26226 2094 1007 8853 1012 2122 2024 7936 2104 1996 1059 3406 1998 2765 1999 6937 8310 1997 3860 1012 2021 1999 1996 2142 2163 1996 2317 2160 2038 2053 2535 1999 1996 2832 1012 2096 1 2038 2699 2000 2202 4923 2005 2070 1997 2122 3572 2027 2024 5373 3390 2011 5661 1998 2787 2011 1996 1057 1012 1055 1012 2248 3119 3222 1006 2009 2278 1007 1998 1996 6236 2533 1012 24207 1996 2343 2071 2025 2644 1037 2553 2130 2065 2002 2359 2000 1012 2008 2089 2022 15140 2005 10390 2021 2009 2036 2965 2008 2047 26269 4162 2023 2126 2024 2025 2788 2579 2004 3350 1997 1037 2406 2050 29645 11057 2015 9376 3343 1012 19817 4215 8663 1017 1012 19258 5649 5866 3860 1012 1996 2343 2036 2038 19258 2058 1996 25291 1997 1996 3860 4162 2164 2029 9167 2075 3032 2453 2022 11819 2013 1996 2047 26269 1012 2343 8398 5546 2443 3290 1999 1996 2047 26269 2130 2295 1996 2009 2278 2018 4340 2008 4916 12699 3698 26269 13686 2053 5081 2000 1057 1012 1055 1012 3068 1012 1012 2182 2003 2073 102\n",
            "I1120 00:35:40.914307 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:35:40.919348 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:40.921821 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:35:40.932735 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:35:40.933980 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:35:40.935628 139937064892288 run_classifier.py:464] tokens: [CLS] african - american congress ##men will skip mississippi civil rights event to avoid trump two democratic representatives john lewis and ben ##nie thompson say they will not attend the long - awaited opening on saturday of two museums dedicated to mississippi ' s history and civil rights struggle because of the planned appearance of president trump . \" tgt ' s attendance and tgt hurt ##ful policies are an insult to the people portrayed in this civil rights museum . the struggles represented in this museum ex ##em ##plify the truth of what really happened in mississippi . tgt ' s di ##spar ##aging comments about women the disabled immigrants and national football league players di ##sr ##es ##pe ##ct the efforts of fan ##nie lou ham ##er aaron henry [ mask ] robert clark james chan ##ey andrew goodman michael sc ##h ##wer ##ner and countless others who have given their all for mississippi to be a better place . \" soon thereafter former navy secretary ray may ##bus who is white t ##wee ##ted that he would not attend the museum opening either citing tgt ' s planned speech . \" his support of white su ##pre ##mac ##ists and racism exact opposite of what museum is about \" said may ##bus a former democratic governor of mississippi . tgt was invited to speak at the ceremony marking the bi ##cent ##ennial of mississippi ' s admission to the union by republican gov . phil bryant who criticized trump ' s det ##rac ##tors . lewis was to have been one of the main speakers at the event along with my ##rl ##ie ever ##s the widow of assassinated mississippi naacp leader [ mask ] . she told the new york times she hopes that tgt will \" learn something \" by attending the museum opening . [SEP]\n",
            "I1120 00:35:40.936899 139937064892288 run_classifier.py:465] input_ids: 101 3060 1011 2137 3519 3549 2097 13558 5900 2942 2916 2724 2000 4468 8398 2048 3537 4505 2198 4572 1998 3841 8034 5953 2360 2027 2097 2025 5463 1996 2146 1011 19605 3098 2006 5095 1997 2048 9941 4056 2000 5900 1005 1055 2381 1998 2942 2916 5998 2138 1997 1996 3740 3311 1997 2343 8398 1012 1000 1 1005 1055 5270 1998 1 3480 3993 6043 2024 2019 15301 2000 1996 2111 6791 1999 2023 2942 2916 2688 1012 1996 11785 3421 1999 2023 2688 4654 6633 28250 1996 3606 1997 2054 2428 3047 1999 5900 1012 1 1005 1055 4487 27694 16594 7928 2055 2308 1996 9776 7489 1998 2120 2374 2223 2867 4487 21338 2229 5051 6593 1996 4073 1997 5470 8034 10223 10654 2121 7158 2888 1031 7308 1033 2728 5215 2508 9212 3240 4080 14514 2745 8040 2232 13777 3678 1998 14518 2500 2040 2031 2445 2037 2035 2005 5900 2000 2022 1037 2488 2173 1012 1000 2574 6920 2280 3212 3187 4097 2089 8286 2040 2003 2317 1056 28394 3064 2008 2002 2052 2025 5463 1996 2688 3098 2593 8951 1 1005 1055 3740 4613 1012 1000 2010 2490 1997 2317 10514 28139 22911 5130 1998 14398 6635 4500 1997 2054 2688 2003 2055 1000 2056 2089 8286 1037 2280 3537 3099 1997 5900 1012 1 2001 4778 2000 3713 2012 1996 5103 10060 1996 12170 13013 22929 1997 5900 1005 1055 9634 2000 1996 2586 2011 3951 18079 1012 6316 12471 2040 6367 8398 1005 1055 20010 22648 6591 1012 4572 2001 2000 2031 2042 2028 1997 1996 2364 7492 2012 1996 2724 2247 2007 2026 12190 2666 2412 2015 1996 7794 1997 16370 5900 26155 3003 1031 7308 1033 1012 2016 2409 1996 2047 2259 2335 2016 8069 2008 1 2097 1000 4553 2242 1000 2011 7052 1996 2688 3098 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:40.938110 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:40.939291 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:40.940783 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:35:40.964356 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:35:40.965703 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:35:40.967505 139937064892288 run_classifier.py:464] tokens: [CLS] it ' s not a final ruling but the supreme court is letting the trump administration fully enforce a ban on travel to the united states by residents of six mostly muslim countries . challenges to the policy are winding through the federal courts and the justices themselves ultimately are expected to rule on whether the ban is legal . it applies to travelers from chad iran libya somalia syria and yemen . the justices offered no explanation for their action monday . the trump administration had said that blocking the full ban was causing \" ir ##re ##para ##ble harm \" because the policy is based on legitimate national security and foreign policy concerns . the order indicates that the high court might eventually approve the latest version of tgt . lower courts have continued to find problems with it . a presidential spokesman hogan gi ##dley said the white house was \" not surprised \" that the court permitted \" immediate enforcement of the president ' s proclamation limiting travel from countries presenting heightened risks of terrorism . \" opponents of this and previous versions of tgt say they show a bias against muslims . they say that was reinforced most recently by tgt s re ##t ##wee ##ts of anti - muslim videos . \" tgt s anti - muslim prejudice is no secret . tgt has repeatedly confirmed it including just last week on twitter . it ' s unfortunate that tgt can move forward for now but this order does not address the merits of our claims \" said omar ja ##d ##wat director of the american civil liberties union ' s immigrants ' rights project . the ac ##lu is representing some opponents of tgt . just two justices ruth bad ##er gin ##sburg and sonia soto ##may ##or noted their disagreement with court orders allowing the latest policy to take full effect . the new policy is not expected to cause the chaos that ensued at airports when tgt ##roll ##ed out tgt first ban without warning in january . for people from the six countries covered by [ mask ] lower courts had said those with a claim of a \" bon ##a fide \" relationship with someone in the united states could not be kept out of the country . grandparents cousins and other relatives were among those courts said could not be excluded . the courts were borrowing language the supreme court itself came up with last summer to allow partial enforcement of an earlier version of tgt . now those relationships will no longer provide a blanket exemption from tgt ##al ##th ##ough visa officials can make exceptions on a case - by - case basis . in lawsuits filed in hawaii and maryland federal courts said the updated travel ban violated federal immigration law . the travel policy also applies to travelers from north korea and to some venezuelan government officials and their families but the lawsuits did not challenge those restrictions . a temporary ban on refugees [SEP]\n",
            "I1120 00:35:40.968883 139937064892288 run_classifier.py:465] input_ids: 101 2009 1005 1055 2025 1037 2345 6996 2021 1996 4259 2457 2003 5599 1996 8398 3447 3929 16306 1037 7221 2006 3604 2000 1996 2142 2163 2011 3901 1997 2416 3262 5152 3032 1012 7860 2000 1996 3343 2024 12788 2083 1996 2976 5434 1998 1996 19867 3209 4821 2024 3517 2000 3627 2006 3251 1996 7221 2003 3423 1012 2009 12033 2000 15183 2013 9796 4238 12917 14717 7795 1998 13968 1012 1996 19867 3253 2053 7526 2005 2037 2895 6928 1012 1996 8398 3447 2018 2056 2008 10851 1996 2440 7221 2001 4786 1000 20868 2890 28689 3468 7386 1000 2138 1996 3343 2003 2241 2006 11476 2120 3036 1998 3097 3343 5936 1012 1996 2344 7127 2008 1996 2152 2457 2453 2776 14300 1996 6745 2544 1997 1 1012 2896 5434 2031 2506 2000 2424 3471 2007 2009 1012 1037 4883 14056 14851 21025 14232 2056 1996 2317 2160 2001 1000 2025 4527 1000 2008 1996 2457 7936 1000 6234 7285 1997 1996 2343 1005 1055 16413 14879 3604 2013 3032 10886 21106 10831 1997 10130 1012 1000 7892 1997 2023 1998 3025 4617 1997 1 2360 2027 2265 1037 13827 2114 7486 1012 2027 2360 2008 2001 11013 2087 3728 2011 1 1055 2128 2102 28394 3215 1997 3424 1011 5152 6876 1012 1000 1 1055 3424 1011 5152 18024 2003 2053 3595 1012 1 2038 8385 4484 2009 2164 2074 2197 2733 2006 10474 1012 2009 1005 1055 15140 2008 1 2064 2693 2830 2005 2085 2021 2023 2344 2515 2025 4769 1996 22617 1997 2256 4447 1000 2056 13192 14855 2094 24281 2472 1997 1996 2137 2942 18271 2586 1005 1055 7489 1005 2916 2622 1012 1996 9353 7630 2003 5052 2070 7892 1997 1 1012 2074 2048 19867 7920 2919 2121 18353 9695 1998 16244 22768 27871 2953 3264 2037 18185 2007 2457 4449 4352 1996 6745 3343 2000 2202 2440 3466 1012 1996 2047 3343 2003 2025 3517 2000 3426 1996 8488 2008 18942 2012 13586 2043 1 28402 2098 2041 1 2034 7221 2302 5432 1999 2254 1012 2005 2111 2013 1996 2416 3032 3139 2011 1031 7308 1033 2896 5434 2018 2056 2216 2007 1037 4366 1997 1037 1000 14753 2050 26000 1000 3276 2007 2619 1999 1996 2142 2163 2071 2025 2022 2921 2041 1997 1996 2406 1012 14472 12334 1998 2060 9064 2020 2426 2216 5434 2056 2071 2025 2022 12421 1012 1996 5434 2020 23733 2653 1996 4259 2457 2993 2234 2039 2007 2197 2621 2000 3499 7704 7285 1997 2019 3041 2544 1997 1 1012 2085 2216 6550 2097 2053 2936 3073 1037 8768 19621 2013 1 2389 2705 10593 9425 4584 2064 2191 11790 2006 1037 2553 1011 2011 1011 2553 3978 1012 1999 20543 6406 1999 7359 1998 5374 2976 5434 2056 1996 7172 3604 7221 14424 2976 7521 2375 1012 1996 3604 3343 2036 12033 2000 15183 2013 2167 4420 1998 2000 2070 15332 2231 4584 1998 2037 2945 2021 1996 20543 2106 2025 4119 2216 9259 1012 1037 5741 7221 2006 8711 102\n",
            "I1120 00:35:40.970129 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:35:40.971291 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:40.972310 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:35:40.996409 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:35:40.997770 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:35:40.999596 139937064892288 run_classifier.py:464] tokens: [CLS] atlanta ( ap ) a ##¢ ##aa when tgt sw ##oop ##s into atlanta for a vip seat at college football ##a ##¢ ##aa ##s biggest game tgt a ##¢ ##aa ##ll enjoy the southern hospitality of a city tgt di ##spar ##aged a year ago as a ##¢ ##aa ##fall ##ing apart ##a ##¢ ##aa and a ##¢ ##aa ##cr ##ime in ##fest ##ed . a ##¢ ##aa the insults t ##wee ##ted by tgt a week before tgt inauguration may seem like ancient history to most americans who follow the president ##a ##¢ ##aa ##s vol ##umi ##nous stream of online in ##ve ##ctive . but atlanta hasn ##a ##¢ ##aa ##t forgotten . a ##¢ ##aa ##it was very personal because we live here and we love it a ##¢ ##aa said stephanie lange ##r a 40 - year - old east atlanta resident whose anger over the president a ##¢ ##aa ##s put ##down ##s was re ##kind ##led by news of his pending visit . a ##¢ ##aa ##i guess it ##a ##¢ ##aa ##s just ironic that he a ##¢ ##aa ##s coming here a ##¢ ##aa she said . a ##¢ ##aa ##i guess things must have tgt plans to be among 72 000 attending the college football playoff championship game monday evening between georgia and alabama . the game is being held at mercedes - benz stadium the new $ 1 . 5 billion home field of the atlanta falcons . the stadium sits in the heart of downtown atlanta a place tgt targeted in a pair of t ##wee ##ts jan . 14 last year . tgt was punching back at democratic rep . [ mask ] . the atlanta congressman a civil rights hero had said he would skip trump ##a ##¢ ##aa ##s inauguration after allegations of russian election med ##dling caused him to doubt tgt a ##¢ ##aa ##as a legitimate president . a ##¢ ##aa tgt t ##wee ##ted : a ##¢ ##aa ##con ##gre ##ss ##man [ mask ] should spend more time on fixing and helping his district which is in horrible shape and falling apart ( not to mention crime in ##fest ##ed ) rather than falsely complaining about the election results . a ##¢ ##aa lewis ##a ##¢ ##aa 5th district seat covers most of the city of atlanta including downtown . tgt a ##¢ ##aa ##s attack gal ##vani ##zed residents to post photos of their neighborhoods and other twitter re ##bu ##ttal ##s under the hash ##tag # defend ##the ##5 ##th . a ##¢ ##aa ##for tgt to di ##spar ##age someone like [ mask ] is absolutely lu ##dic ##rous a ##¢ ##aa said calvin who figures tgt wants to come to the big game for the attention rather than any real interest in football . though atlanta leans democratic both georgia and alabama are solid a ##¢ ##aa ##trum ##p country ##a ##¢ ##aa states that strongly backed the president in the 2016 election . in a political [SEP]\n",
            "I1120 00:35:41.001028 139937064892288 run_classifier.py:465] input_ids: 101 5865 1006 9706 1007 1037 29645 11057 2043 1 25430 18589 2015 2046 5865 2005 1037 21722 2835 2012 2267 2374 2050 29645 11057 2015 5221 2208 1 1037 29645 11057 3363 5959 1996 2670 15961 1997 1037 2103 1 4487 27694 18655 1037 2095 3283 2004 1037 29645 11057 13976 2075 4237 2050 29645 11057 1998 1037 29645 11057 26775 14428 1999 14081 2098 1012 1037 29645 11057 1996 23862 1056 28394 3064 2011 1 1037 2733 2077 1 17331 2089 4025 2066 3418 2381 2000 2087 4841 2040 3582 1996 2343 2050 29645 11057 2015 5285 12717 18674 5460 1997 3784 1999 3726 15277 1012 2021 5865 8440 2050 29645 11057 2102 6404 1012 1037 29645 11057 4183 2001 2200 3167 2138 2057 2444 2182 1998 2057 2293 2009 1037 29645 11057 2056 11496 21395 2099 1037 2871 1011 2095 1011 2214 2264 5865 6319 3005 4963 2058 1996 2343 1037 29645 11057 2015 2404 7698 2015 2001 2128 18824 3709 2011 2739 1997 2010 14223 3942 1012 1037 29645 11057 2072 3984 2009 2050 29645 11057 2015 2074 19313 2008 2002 1037 29645 11057 2015 2746 2182 1037 29645 11057 2016 2056 1012 1037 29645 11057 2072 3984 2477 2442 2031 1 3488 2000 2022 2426 5824 2199 7052 1996 2267 2374 7808 2528 2208 6928 3944 2090 4108 1998 6041 1012 1996 2208 2003 2108 2218 2012 10793 1011 17770 3346 1996 2047 1002 1015 1012 1019 4551 2188 2492 1997 1996 5865 14929 1012 1996 3346 7719 1999 1996 2540 1997 5116 5865 1037 2173 1 9416 1999 1037 3940 1997 1056 28394 3215 5553 1012 2403 2197 2095 1012 1 2001 19477 2067 2012 3537 16360 1012 1031 7308 1033 1012 1996 5865 12295 1037 2942 2916 5394 2018 2056 2002 2052 13558 8398 2050 29645 11057 2015 17331 2044 9989 1997 2845 2602 19960 14423 3303 2032 2000 4797 1 1037 29645 11057 3022 1037 11476 2343 1012 1037 29645 11057 1 1056 28394 3064 1024 1037 29645 11057 8663 17603 4757 2386 1031 7308 1033 2323 5247 2062 2051 2006 15887 1998 5094 2010 2212 2029 2003 1999 9202 4338 1998 4634 4237 1006 2025 2000 5254 4126 1999 14081 2098 1007 2738 2084 23123 17949 2055 1996 2602 3463 1012 1037 29645 11057 4572 2050 29645 11057 4833 2212 2835 4472 2087 1997 1996 2103 1997 5865 2164 5116 1012 1 1037 29645 11057 2015 2886 14891 27760 5422 3901 2000 2695 7760 1997 2037 11681 1998 2060 10474 2128 8569 28200 2015 2104 1996 23325 15900 1001 6985 10760 2629 2705 1012 1037 29645 11057 29278 1 2000 4487 27694 4270 2619 2066 1031 7308 1033 2003 7078 11320 14808 13288 1037 29645 11057 2056 11130 2040 4481 1 4122 2000 2272 2000 1996 2502 2208 2005 1996 3086 2738 2084 2151 2613 3037 1999 2374 1012 2295 5865 12671 3537 2119 4108 1998 6041 2024 5024 1037 29645 11057 24456 2361 2406 2050 29645 11057 2163 2008 6118 6153 1996 2343 1999 1996 2355 2602 1012 1999 1037 2576 102\n",
            "I1120 00:35:41.002317 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1120 00:35:41.003541 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:41.004549 139937064892288 run_classifier.py:468] label: False (id = 0)\n",
            "I1120 00:35:41.021617 139937064892288 run_classifier.py:461] *** Example ***\n",
            "I1120 00:35:41.023696 139937064892288 run_classifier.py:462] guid: None\n",
            "I1120 00:35:41.025571 139937064892288 run_classifier.py:464] tokens: [CLS] tgt announced on friday that tgt is dec ##ert ##ifying but not withdrawing from or re ##writing the iran nuclear agreement . while [ mask ] delivered b ##list ##ering criticism of iran ' s \" fan ##atic ##al regime \" and argued the country has violated the spirit of the 2015 agreement that aims to prevent iran from obtaining a nuclear weapon tgt left the deal ' s fate up to congress . \" we cannot and will not make this certification \" tgt said during a speech at the white house . \" we will not continue down a path whose predictable conclusion is more violence more terror and the very real threat of iran ' s nuclear breakout . tgt has effectively given congress three options : do not subject iran to new sanctions and leave the agreement as is ; apply sanctions and withdraw from the deal ; or rene ##go ##tia ##te the deal . but tgt threatened to un ##ila ##tera ##lly withdraw from the agreement if congress is unable to agree on a solution which will require 60 votes and bi ##partisan support in the senate . tgt campaigned on tgt promise to \" rip up \" the agreement which was negotiated by the obama administration in an effort to curb iran ' s nuclear weapons program and is formally known as the joint comprehensive plan of action . but on july 17 the president rec ##ert ##ified the jc ##po ##a a ##¢ ##aa which is required by congress every 90 days a ##¢ ##aa for the second time after he was warned by his top aide ##s and cabinet officials that withdrawing would threaten us national security interests and told that while the deal is imperfect it provides crucial benefits for the us and its allies . in august the guardian reported that the tgt white house was pushing intelligence analysts to provide justification for declaring iran in violation of the tenants of the deal . that pressure reportedly reminded some analysts of the run - up to the invasion of iraq in 2003 . \" they told me there was a sense of rev ##ulsion . there was a sense of da ##a ##© ##ja ##a vu \" said ned price a former cia analyst who served as special adviser to former president barack obama . \" there was a sense of ' we ' ve seen this movie before . ' \" experts argue that trump ' s motivation ##s for scrap ##ping the deal are more political than strategic . [SEP]\n",
            "I1120 00:35:41.027024 139937064892288 run_classifier.py:465] input_ids: 101 1 2623 2006 5958 2008 1 2003 11703 8743 11787 2021 2025 21779 2013 2030 2128 18560 1996 4238 4517 3820 1012 2096 1031 7308 1033 5359 1038 9863 7999 6256 1997 4238 1005 1055 1000 5470 12070 2389 6939 1000 1998 5275 1996 2406 2038 14424 1996 4382 1997 1996 2325 3820 2008 8704 2000 4652 4238 2013 11381 1037 4517 5195 1 2187 1996 3066 1005 1055 6580 2039 2000 3519 1012 1000 2057 3685 1998 2097 2025 2191 2023 10618 1000 1 2056 2076 1037 4613 2012 1996 2317 2160 1012 1000 2057 2097 2025 3613 2091 1037 4130 3005 21425 7091 2003 2062 4808 2062 7404 1998 1996 2200 2613 5081 1997 4238 1005 1055 4517 25129 1012 1 2038 6464 2445 3519 2093 7047 1024 2079 2025 3395 4238 2000 2047 17147 1998 2681 1996 3820 2004 2003 1025 6611 17147 1998 10632 2013 1996 3066 1025 2030 10731 3995 10711 2618 1996 3066 1012 2021 1 5561 2000 4895 11733 14621 9215 10632 2013 1996 3820 2065 3519 2003 4039 2000 5993 2006 1037 5576 2029 2097 5478 3438 4494 1998 12170 26053 2490 1999 1996 4001 1012 1 16196 2006 1 4872 2000 1000 10973 2039 1000 1996 3820 2029 2001 13630 2011 1996 8112 3447 1999 2019 3947 2000 13730 4238 1005 1055 4517 4255 2565 1998 2003 6246 2124 2004 1996 4101 7721 2933 1997 2895 1012 2021 2006 2251 2459 1996 2343 28667 8743 7810 1996 29175 6873 2050 1037 29645 11057 2029 2003 3223 2011 3519 2296 3938 2420 1037 29645 11057 2005 1996 2117 2051 2044 2002 2001 7420 2011 2010 2327 14895 2015 1998 5239 4584 2008 21779 2052 15686 2149 2120 3036 5426 1998 2409 2008 2096 1996 3066 2003 29238 2009 3640 10232 6666 2005 1996 2149 1998 2049 6956 1012 1999 2257 1996 6697 2988 2008 1996 1 2317 2160 2001 6183 4454 18288 2000 3073 19777 2005 13752 4238 1999 11371 1997 1996 14665 1997 1996 3066 1012 2008 3778 7283 6966 2070 18288 1997 1996 2448 1011 2039 2000 1996 5274 1997 5712 1999 2494 1012 1000 2027 2409 2033 2045 2001 1037 3168 1997 7065 23316 1012 2045 2001 1037 3168 1997 4830 2050 29652 3900 2050 24728 1000 2056 12311 3976 1037 2280 9915 12941 2040 2366 2004 2569 11747 2000 2280 2343 13857 8112 1012 1000 2045 2001 1037 3168 1997 1005 2057 1005 2310 2464 2023 3185 2077 1012 1005 1000 8519 7475 2008 8398 1005 1055 14354 2015 2005 15121 4691 1996 3066 2024 2062 2576 2084 6143 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:41.028338 139937064892288 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:41.029582 139937064892288 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1120 00:35:41.030631 139937064892288 run_classifier.py:468] label: True (id = 1)\n",
            "I1120 00:37:27.204272 139937064892288 run_classifier.py:774] Writing example 10000 of 12604\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnRhr2sAzrYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = train['label']\n",
        "test_label = test['label']\n",
        "test_fixed_label = test_fixed['label']\n",
        "dev_label = dev['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6TrW6rjlZZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.head(500).to_csv(open('train_500.csv','w'),encoding='latin-1')\n",
        "data_train = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2x0zox7vWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_selective -f '\\bdata_test_fixed\\b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz3_JA2X6eAc",
        "colab_type": "code",
        "outputId": "ddf77932-9825-433d-debc-74d5dd72658e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# reset_selective -f data_train\n",
        "# who_ls\n",
        "import sys\n",
        "ipython_vars =[]# ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
        "\n",
        "\n",
        "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('train', 753257168),\n",
              " ('test_fixed', 146768632),\n",
              " ('data_test', 117534962),\n",
              " ('test', 117469922),\n",
              " ('data_dev', 70042071),\n",
              " ('dev', 69984575),\n",
              " ('train_InputExamples', 1837864),\n",
              " ('test_InputExamples_fixed', 504264),\n",
              " ('train_features', 406504),\n",
              " ('test_InputExamples', 325304),\n",
              " ('dev_InputExamples', 287584),\n",
              " ('lines', 253640),\n",
              " ('test_features_fixed', 111008),\n",
              " ('test_fixed_features', 111008),\n",
              " ('test_features', 69168),\n",
              " ('dev_features', 61440),\n",
              " ('train_label', 46048),\n",
              " ('test_fixed_label', 12708),\n",
              " ('test_label', 8234),\n",
              " ('dev_label', 7291),\n",
              " ('auth_info', 1048),\n",
              " ('AdamWeightDecayOptimizer', 904),\n",
              " ('Out', 280),\n",
              " ('In', 200),\n",
              " ('f', 144),\n",
              " ('data_pref', 132),\n",
              " ('create_optimizer', 120),\n",
              " ('load_dataset', 120),\n",
              " ('load_directory_data', 120),\n",
              " ('CONFIG_FILE', 109),\n",
              " ('INIT_CHECKPOINT', 108),\n",
              " ('OUTPUT_DIR', 97),\n",
              " ('VOCAB_FILE', 97),\n",
              " ('path', 97),\n",
              " ('e', 96),\n",
              " ('BERT_MODEL_HUB', 92),\n",
              " ('BERT_PRETRAINED_DIR', 92),\n",
              " ('label_list', 88),\n",
              " ('get_ipython', 80),\n",
              " ('absolute_import', 72),\n",
              " ('division', 72),\n",
              " ('ipython_vars', 72),\n",
              " ('print_function', 72),\n",
              " ('exit', 64),\n",
              " ('f_in', 64),\n",
              " ('f_out', 64),\n",
              " ('quit', 64),\n",
              " ('session', 64),\n",
              " ('tf', 64),\n",
              " ('tokenizer', 64),\n",
              " ('tpu_cluster_resolver', 64),\n",
              " ('BERT_MODEL', 60),\n",
              " ('TPU_ADDRESS', 60),\n",
              " ('auth', 56),\n",
              " ('hub', 56),\n",
              " ('modeling', 56),\n",
              " ('pd', 56),\n",
              " ('run_classifier', 56),\n",
              " ('run_classifier_with_tfhub', 56),\n",
              " ('tokenization', 56),\n",
              " ('BUCKET', 49),\n",
              " ('DATA_COLUMN', 45),\n",
              " ('line', 43),\n",
              " ('LABEL_COLUMN', 42),\n",
              " ('data_train', 37),\n",
              " ('DO_LOWER_CASE', 24),\n",
              " ('EVAL_BATCH_SIZE', 24),\n",
              " ('ITERATIONS_PER_LOOP', 24),\n",
              " ('LEARNING_RATE', 24),\n",
              " ('MAX_SEQ_LENGTH', 24),\n",
              " ('NUM_TPU_CORES', 24),\n",
              " ('NUM_TRAIN_EPOCHS', 24),\n",
              " ('PREDICT_BATCH_SIZE', 24),\n",
              " ('SAVE_CHECKPOINTS_STEPS', 24),\n",
              " ('SAVE_SUMMARY_STEPS', 24),\n",
              " ('TRAIN_BATCH_SIZE', 24),\n",
              " ('WARMUP_PROPORTION', 24),\n",
              " ('num_train_steps', 24),\n",
              " ('num_warmup_steps', 24),\n",
              " ('use_tpu', 24)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Sj7di-oaUB",
        "colab_type": "code",
        "outputId": "1bd67947-2900-44b6-8bb8-d0b09cbef002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEcWYt6jjaf",
        "colab_type": "code",
        "outputId": "cc89915d-64d6-4fc7-efae-51b5a94db07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_w = 1./len(dev[dev['label']==0])\n",
        "pos_w = 1./len(dev[dev['label']==1])\n",
        "\n",
        "class_weights_arr = [neg_w/(neg_w+pos_w),pos_w/(neg_w+pos_w)]\n",
        "# class_weights_arr = [0.6,0.4]\n",
        "trainable = True\n",
        "print(class_weights_arr)\n",
        "# class_weights_arr = [0.5,0.5]\n",
        "def create_model(bert_config,is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels,use_one_hot_embeddings):#, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "    \n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB, tags=tags, trainable=trainable)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "###### TO DO add weights\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "    print(one_hot_labels.get_shape())\n",
        "        \n",
        "    one_hot_labels = one_hot_labels * class_weights_arr\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "       \n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels,init_checkpoint, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu,use_one_hot_embeddings):# bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) =create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,use_one_hot_embeddings)\n",
        "\n",
        "    \n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "     \n",
        "      train_op = create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,scaffold_fn=scaffold_fn)\n",
        "#       output_spec = tf.Print(output_spec,[tf_loss,total_loss],message='tfloss, total loss')\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predictions,weights=is_real_example)\n",
        "        loss = tf.metrics.mean(per_example_loss,weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics,scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities},scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6436621678029776, 0.35633783219702236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "4eff6f43-ba9f-49f2-c9f5-12793786f1f6"
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "### Activate it if ** part is not activated \n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True\n",
        "#   bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=get_run_config(OUTPUT_DIR),\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE, \n",
        "# )\n",
        "# #####################################################################\n",
        "## No Error\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    keep_checkpoint_max=15,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "# model_fn = run_classifier.model_fn_builder(\n",
        "#     bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#     num_labels=len(label_list),\n",
        "#     init_checkpoint=INIT_CHECKPOINT,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     num_train_steps=num_train_steps,\n",
        "#     num_warmup_steps=num_warmup_steps,\n",
        "#     use_tpu=use_tpu,\n",
        "#     use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "\n",
        "# estimator_from_tfhub._export_to_tpu = False\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1120 00:37:57.598301 139937064892288 estimator.py:1994] Estimator's model_fn (<function model_fn at 0x7f45330e41b8>) includes params argument, but params are not passed to Estimator.\n",
            "I1120 00:37:57.606750 139937064892288 estimator.py:212] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.96.13.98:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 15, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f453286f650>, '_model_dir': 'gs://bert_example/aug_19_models/mask_lm/weighted/last3layers', '_protocol': None, '_save_checkpoints_steps': 50, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=2871, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f4535ed95d0>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.96.13.98:8470', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': u'grpc://10.96.13.98:8470'}\n",
            "I1120 00:37:57.608508 139937064892288 tpu_context.py:220] _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "def model_train(estimator,train_features=train_features):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_features)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  print('start running estimator')\n",
        "#   estimator._export_to_tpu = False\n",
        "  md = estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "  return md\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator,eval_examples=None,eval_features=dev_features):\n",
        "  # Eval the model.\n",
        "#   eval_examples = dev_InputExamples#processor.get_dev_examples(TASK_DATA_DIR)\n",
        "#   eval_features = run_classifier.convert_examples_to_features(\n",
        "#       eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "#   with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "  print(\"***** Eval results *****\")\n",
        "  for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "#       writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "      \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# labels = [1,0]\n",
        "# def model_predict(estimator,prediction_examples):\n",
        "#   # Make predictions on a subset of eval examples\n",
        "# #   prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "#   input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#   predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "#   predictions = estimator.predict(predict_input_fn)\n",
        "#   return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n",
        "def model_predict(estimator,input_features,input_examples,checkpoint_path=None):\n",
        "  # Make predictions on a subset of eval examples\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmU7E5v1-k0",
        "colab_type": "code",
        "outputId": "02e449cd-7563-460f-86f2-6057f55aa1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "model_train(estimator,train_features=train_features)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started training at 2019-11-20 00:45:54.601040 *****\n",
            "  Num examples = 45944\n",
            "  Batch size = 16\n",
            "start running estimator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 00:47:06.452250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.455482 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.457176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.458959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.467869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.479541 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.488974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.496045 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.499067 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.523791 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.527118 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.534768 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.538186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.545877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.549019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.568387 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.571590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.581841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.585083 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.595128 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.599123 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.608743 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.611717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.623051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.626230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.639640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.643160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.650221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.654248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.661731 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.664555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.682265 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.685348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.695935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.699109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.708868 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.712785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.725483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.728724 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.742732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.746524 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.759268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.762723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.771559 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.774894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.783488 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.786988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.805985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.810400 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.820926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.824990 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.835321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.839857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.850804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.854145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.865312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.869234 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.881721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.885682 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.893836 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.898396 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.905759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.910543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.929325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.934499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.944822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.952294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.964709 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.971427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.984261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.987329 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.997327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.000361 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.014003 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.019433 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.027420 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.030415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.039308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.044294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.066303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.072509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.084403 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.088108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.100001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.106352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.115931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.118985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.129012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.132602 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.144649 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.150039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.157325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.160701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.168260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.171353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.190819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.194103 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.204610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.208528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.218489 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.222845 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.234654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.237935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.248831 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.252840 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.265762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.270019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.279839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.284789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.292675 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.297959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.316926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.322321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.332916 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.338063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.347532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.352493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.362513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.365628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.376537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.379569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.391937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.396004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.403213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.406794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.415011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.420222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.437892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.443226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.454775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.458633 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.468672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.473350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.483948 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.489160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.500077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.504893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.515845 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.520277 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.527190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.530498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.538841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.541732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.560194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.566298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.575567 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.581056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.590034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.594640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.604532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.609858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.620807 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.623712 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.637423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.642066 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.649070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.651973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.659409 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.662235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.680149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.685887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.695431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.698368 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.709012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.712786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.723077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.726491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.737307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.741360 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.753245 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.756434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.763151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.766386 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.773128 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.776937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.799168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.802826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.814016 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.818070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.828474 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.832029 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.841564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.847275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.857438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.861495 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.874052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.877383 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.885274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.888611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.897595 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.902851 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.922281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.926961 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.935964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.940334 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.948817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.952838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.964879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.968794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.980025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.983618 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.013928 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.018141 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.030590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.034264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.043570 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.047849 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.057929 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2, 2)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Finished training at 2019-11-20 01:16:37.241792 *****\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7f45841ac750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYlSED2fBRC",
        "colab_type": "code",
        "outputId": "3e327972-aaa3-454d-9b2b-4bfe3bd2a783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(2871,28711,2871):\n",
        "  print('evaluating epoch: %d'%(i/2871))\n",
        "  pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%(i))\n",
        "  true_label = list(dev['label'])\n",
        "  labels_val = []\n",
        "  for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:47:28.787249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.790396 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.792452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.794035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.802660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.817955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.828022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.834934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.838350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.859389 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.863677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.870687 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.873877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.880351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.883647 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.898747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.902481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.907816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.911048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.923171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.928765 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.937701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.943145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.950933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.956157 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.969923 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.978048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.985508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.989025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.997312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.004142 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.019869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.024197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.032008 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.036660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.046578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.051168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.062581 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.066585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.072592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.076169 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.090017 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.093298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.100430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.105372 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.111139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.114346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.128572 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.132019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.138322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.141443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.150531 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.157283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.166249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.170824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.177256 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.180814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.193212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.196654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.204540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.208347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.215353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.224827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.242418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.245723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.253703 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.257051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.268870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.273817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.284069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.287444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.294183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.297977 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.310322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.316067 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.324261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.327534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.335838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.340183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.355703 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.361763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.368443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.372056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.383213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.389069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.399044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.402662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.409616 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.412920 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.426013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.429620 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.437378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.440814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.448359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.451776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.468242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.475833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.483829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.487832 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.502295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.507153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.518322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.522069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.529012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.533859 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.546773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.550590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.559021 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.562767 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.570251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.574206 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.594854 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.598810 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.606642 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.610394 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.620537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.624944 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.635377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.638952 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.645781 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.650337 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.661539 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.664653 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.673044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.678447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.685918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.689810 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.712374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.716037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.724725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.728424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.740662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.745049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.754981 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.758418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.765229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.768821 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.780751 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.784037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.792352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.796909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.805011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.810626 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.826512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.831137 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.838324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.844048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.854213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.859766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.870229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.874914 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.880867 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.887382 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.900549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.905014 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.911880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.917714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.923114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.926564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.941056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.944214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.950956 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.954425 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.965111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.969530 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.979716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.983645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.990230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.994235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.006059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.011517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.018224 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.023145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.029627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.034590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.052274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.055679 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.062586 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.066078 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.075015 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.079499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.089519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.092791 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.099193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.104516 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.116210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.120918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.129050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.133644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.140139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.143578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.160268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.164052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.172250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.177248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.186944 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.191446 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.200984 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.204220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.209817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.215373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.245521 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.248842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.262046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.266490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.275964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.279378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.290585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[1845  716]\n",
            " [ 198 4428]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.90      0.72      0.80      2561\n",
            "        True       0.86      0.96      0.91      4626\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      7187\n",
            "   macro avg       0.88      0.84      0.85      7187\n",
            "weighted avg       0.88      0.87      0.87      7187\n",
            "\n",
            "evaluating epoch: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:48:19.019186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.021737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.023825 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.026325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.035711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.049695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.059983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.067075 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.070759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.090147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.093648 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.101763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.105839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.112973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.116389 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.133280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.136547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.143063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.146508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.154819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.158998 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.169044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.172378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.178148 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.181437 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.192035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.195255 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.201947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.205233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.210951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.214437 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.234663 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.239279 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.245198 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.250842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.259402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.264053 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.274190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.277753 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.287298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.293478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.306188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.309705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.319638 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.328664 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.336915 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.344824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.360824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.364444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.373541 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.377573 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.387904 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.394697 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.406285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.410387 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.417963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.422152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.435156 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.439065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.447762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.452063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.458621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.463650 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.479578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.483344 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.489901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.493424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.503652 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.508755 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.518580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.523046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.527992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.532133 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.543536 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.547434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.555197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.558837 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.566355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.570667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.585453 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.589138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.599081 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.604537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.615053 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.621597 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.631779 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.638858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.645999 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.650038 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.663073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.667880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.675574 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.679724 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.686757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.690813 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.707283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.710967 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.716655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.721307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.733099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.739129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.749313 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.755878 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.762757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.768285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.779367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.784713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.793138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.797138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.803785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.806988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.822051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.825109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.830507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.834974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.843622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.848306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.857130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.860238 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.866050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.869611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.881197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.884351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.891777 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.896505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.902630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.905801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.921888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.925062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.930486 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.934881 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.944094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.948190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.958085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.961297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.967756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.973305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.984427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.988593 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.995927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.001976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.009650 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.012808 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.028059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.031997 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.037534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.040971 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.051068 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.054933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.064385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.068676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.075407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.078504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.089684 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.092811 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.100311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.104012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.110090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.113193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.128366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.132162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.138659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.142000 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.151134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.154987 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.166858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.171168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.176630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.180172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.194931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.199290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.206485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.210180 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.216193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.219429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.235737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.239090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.244478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.249212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.259183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.262407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.273452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.277082 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.283431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.286508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.298690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.303507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.310795 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.318521 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.324934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.328974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.344392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.347785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.354927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.358776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.369050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.372654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.383884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.387111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.392720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.397152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.433366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.439994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.453010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.456310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.466039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.469822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.479497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452ee5d960> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1619  942]\n",
            " [ 192 4434]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.63      0.74      2561\n",
            "        True       0.82      0.96      0.89      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.86      0.80      0.81      7187\n",
            "weighted avg       0.85      0.84      0.83      7187\n",
            "\n",
            "evaluating epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:49:10.288819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.291101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.293690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.295897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.305232 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.317280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.327111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.335154 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.338538 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.359611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.363595 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.371049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.374728 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.381145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.384030 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.397344 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.400250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.405236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.408226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.416923 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.420490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.429230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.432130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.437271 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.440218 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.450609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.453484 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.459960 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.462877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.469172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.471961 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.485008 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.487906 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.494828 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.498310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.506942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.510481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.519994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.523001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.528630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.531614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.543152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.546050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.552428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.555193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.563950 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.568547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.582684 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.586478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.592421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.595189 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.604609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.610555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.620152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.623434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.628375 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.632694 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.643954 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.646781 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.654098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.658333 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.665580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.668515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.683809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.687170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.693672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.698785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.708551 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.713167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.723421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.728051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.735033 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.737942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.749864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.754869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.762689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.766381 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.772824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.775599 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.789901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.794784 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.800327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.803375 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.812917 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.816370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.825207 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.827970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.834414 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.837152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.848747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.851624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.859226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.862114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.869082 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.872716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.887080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.891129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.896895 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.901405 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.911592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.916026 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.926103 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.929708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.936023 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.940370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.952945 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.956562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.964184 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.967765 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.974666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.978761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.993254 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.996491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.004419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.007504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.017303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.022105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.036252 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.041697 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.048439 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.051803 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.064306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.067287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.076292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.079144 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.086350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.090295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.105643 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.110217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.116611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.119390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.130094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.135270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.148336 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.151623 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.160240 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.163429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.177789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.182334 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.190639 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.194880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.202603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.207355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.226963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.233158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.241102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.245090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.254965 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.258933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.270185 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.273964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.281660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.285907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.300740 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.306523 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.315099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.319195 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.326491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.329711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.346421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.350222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.358479 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.363100 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.379878 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.385600 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.402430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.408004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.415761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.419074 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.432256 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.436794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.445007 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.448662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.456299 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.459202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.480307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.484108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.490310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.494016 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.507842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.511594 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.523654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.526576 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.531976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.536294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.549062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.552968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.561696 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.565031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.572158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.575789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.590436 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.593566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.600035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.602996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.613980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.620434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.632200 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.636723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.643409 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.647084 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.679289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.682908 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.697369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.701884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.713614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.717010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.726483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f45240b2050> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1780  781]\n",
            " [ 341 4285]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.84      0.70      0.76      2561\n",
            "        True       0.85      0.93      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.84      0.81      0.82      7187\n",
            "weighted avg       0.84      0.84      0.84      7187\n",
            "\n",
            "evaluating epoch: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:50:04.071634 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.073693 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.079722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.081476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.090557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.103039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.112876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.120563 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.125227 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.144861 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.148040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.155658 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.158662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.166938 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.170159 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.186652 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.191235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.197262 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.200197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.210074 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.213879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.224395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.227689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.233896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.237493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.249689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.254393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.261898 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.265741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.275010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.278862 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.295241 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.298330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.304287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.307432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.317178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.321820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.331443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.334397 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.340826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.343837 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.359877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.364274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.372205 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.375513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.383279 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.386173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.402271 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.405432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.412014 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.415498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.425714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.430263 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.444659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.450686 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.458220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.462613 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.475615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.478902 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.486670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.489959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.497395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.500762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.518076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.522243 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.529545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.535866 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.547017 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.552035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.562448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.565892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.574139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.577363 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.588422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.594589 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.604772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.608577 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.614749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.618297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.633310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.636121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.642447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.646543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.656308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.660938 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.670384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.673151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.679150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.682771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.696526 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.701874 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.708183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.711009 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.717318 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.720978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.735296 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.738900 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.744414 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.748285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.758233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.761996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.776230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.780996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.787163 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.792357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.806324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.809815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.817034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.820760 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.828972 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.833123 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.848031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.851197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.858335 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.862107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.872060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.876497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.888897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.892235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.897300 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.902657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.915354 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.918554 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.925734 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.928858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.935359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.940870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.956042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.959491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.966274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.969870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.980530 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.985557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.995630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.999104 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.007179 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.011423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.025283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.028352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.036412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.041073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.048763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.053138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.070640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.074590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.080229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.084291 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.094424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.098304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.109292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.112306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.119393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.122551 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.135013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.138431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.146024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.149561 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.158838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.162303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.177560 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.180557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.187210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.190629 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.201090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.204941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.215795 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.218924 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.224073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.228007 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.241039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.247241 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.255173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.258220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.265826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.269031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.286953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.292197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.298758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.303934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.314858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.320812 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.331764 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.337147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.347232 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.351171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.365998 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.370656 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.379607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.384149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.393143 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.396502 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.413512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.418350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.425575 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.429790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.440041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.445575 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.456552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.463741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.471496 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.475115 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.514086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.517411 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.530797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.535216 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.544512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.549422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.559757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f4529924190> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1865  696]\n",
            " [ 518 4108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.78      0.73      0.75      2561\n",
            "        True       0.86      0.89      0.87      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.82      0.81      0.81      7187\n",
            "weighted avg       0.83      0.83      0.83      7187\n",
            "\n",
            "evaluating epoch: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:50:51.857301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.860691 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.862374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.864186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.873210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.887892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.898049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.904660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.907741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.927335 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.930273 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.937400 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.940355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.946971 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.949959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.963972 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.969202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.974725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.977721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.986057 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.989914 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.999443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.002501 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.007827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.011518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.024049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.027642 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.035092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.038059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.043771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.046585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.060324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.065545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.070771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.073658 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.082108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.085659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.096323 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.100706 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.106159 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.110304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.120275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.123173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.129722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.134675 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.139929 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.143480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.158721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.161715 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.167526 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.171787 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.183896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.188147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.197561 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.200793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.206685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.210424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.220793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.223700 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.231050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.233846 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.240353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.244102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.258857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.262490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.268004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.272979 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.284024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.287659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.296401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.299412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.304986 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.308758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.319761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.325006 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.333879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.339705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.345942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.350203 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.364720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.370534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.376511 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.380995 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.390062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.394937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.404139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.408282 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.416305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.420342 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.432766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.436705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.445172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.448517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.454143 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.457941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.474035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.477243 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.483165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.488311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.499268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.503385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.512790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.515783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.521042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.525369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.536813 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.541167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.547183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.549983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.556114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.559035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.574130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.577853 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.583373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.586406 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.600147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.604295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.613851 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.616771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.622430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.625392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.637794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.640930 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.647964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.652060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.657994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.662031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.678046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.683777 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.692384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.695518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.705509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.709072 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.718559 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.722379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.727691 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.730557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.741935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.744755 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.752072 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.755433 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.762222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.765165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.780519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.783504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.788894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.791713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.800884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.804440 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.812966 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.817357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.822338 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.826673 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.837614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.841026 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.847651 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.850852 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.856611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.860430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.876746 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.880681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.886055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.888931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.898719 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.903296 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.912499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.915359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.920977 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.925945 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.937011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.940474 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.947305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.950109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.956069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.959050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.973557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.976450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.981982 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.985013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.994402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.998156 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.007373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.010220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.013925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.016947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.032582 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.035547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.042418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.045260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.051528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.054341 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.074110 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.079302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.084796 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.090653 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.102257 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.107665 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.117986 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.122392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.128834 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.133280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.166167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.170264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.183767 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.187108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.196702 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.201117 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.209532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452ab7e9b0> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1730  831]\n",
            " [ 436 4190]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.80      0.68      0.73      2561\n",
            "        True       0.83      0.91      0.87      4626\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      7187\n",
            "   macro avg       0.82      0.79      0.80      7187\n",
            "weighted avg       0.82      0.82      0.82      7187\n",
            "\n",
            "evaluating epoch: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:51:48.618505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.624773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.626554 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.628412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.637659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.646815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.655992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.662627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.665936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.685934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.689476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.696855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.700267 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.707086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.710392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.723676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.728085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.734549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.738714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.747786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.752655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.763163 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.766907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.774162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.777749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.790023 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.793648 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.801670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.805609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.812443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.816040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.838596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.845196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.853307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.858859 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.871226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.877258 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.890557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.895662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.904041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.908809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.920448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.924717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.931545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.935255 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.941178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.945272 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.959109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.962251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.969214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.972901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.982136 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.986936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.996088 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.999183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.007107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.011893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.023792 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.027398 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.035731 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.039911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.048830 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.053378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.069483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.075089 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.080888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.084515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.094158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.099756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.109833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.113616 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.119685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.125402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.137876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.142251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.150373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.154450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.163270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.176431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.195888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.201626 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.207515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.212713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.222832 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.228827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.239316 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.244574 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.251069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.256660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.271194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.276917 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.283797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.287681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.293476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.298968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.315236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.318427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.325535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.328775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.340452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.346132 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.360196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.364144 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.370806 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.374293 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.386262 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.389609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.396080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.400331 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.406374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.410824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.429828 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.432790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.439050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.444052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.455820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.462244 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.472310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.476747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.481021 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.486532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.497622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.502192 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.508991 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.512881 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.519166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.522073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.539992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.544121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.550514 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.554594 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.564266 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.569472 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.578537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.582484 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.588535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.592823 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.603384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.607569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.615197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.619359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.626365 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.631174 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.643785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.648657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.654212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.657829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.667711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.671936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.681020 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.684432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.690371 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.693875 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.705873 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.709242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.720206 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.723630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.729976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.733328 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.749090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.752758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.759783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.763395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.774552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.779171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.791167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.796428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.804580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.808710 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.821600 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.825187 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.833259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.837013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.845004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.848543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.865314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.873310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.880740 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.884497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.895196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.899416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.909776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.913431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.919230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.925211 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.937480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.942822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.950278 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.955873 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.963092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.969780 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.985438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.991089 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.997920 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.003134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.013701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.019325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.029819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.034392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.041049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.045763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.079298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.083734 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.097304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.102674 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.111438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.116194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.125428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452732d190> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1673  888]\n",
            " [ 333 4293]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.83      0.65      0.73      2561\n",
            "        True       0.83      0.93      0.88      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.83      0.79      0.80      7187\n",
            "weighted avg       0.83      0.83      0.82      7187\n",
            "\n",
            "evaluating epoch: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:52:40.265716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.267649 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.269099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.270385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.278759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.293327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.303056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.309864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.312892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.332667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.336772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.344342 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.347754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.355297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.358426 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.372209 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.376018 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.383105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.386146 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.395289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.399599 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.408899 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.412050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.418725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.421974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.433178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.436203 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.443783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.446685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.452970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.455707 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.471095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.476095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.482858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.485953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.495934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.500107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.509390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.513370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.519752 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.522830 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.534440 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.538055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.546165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.549427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.556603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.560379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.578783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.581746 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.588855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.591742 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.601150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.604829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.614501 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.617302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.623553 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.626235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.637528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.640275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.648166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.650943 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.658480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.661838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.677719 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.680670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.686234 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.689708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.699804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.704034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.714394 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.717264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.723102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.726418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.740045 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.742963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.750415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.753621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.759964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.763509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.777441 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.781301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.787446 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.791096 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.800717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.805624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.814062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.817436 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.823585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.826800 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.839170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.843194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.851401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.854562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.864377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.867801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.884251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.887846 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.894192 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.897049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.907111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.910888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.921962 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.924931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.933723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.939268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.951549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.955183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.962723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.965748 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.972717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.980741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.995419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.998641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.005155 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.009727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.025619 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.030826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.043893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.048413 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.054105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.058326 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.071116 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.074259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.081361 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.085242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.092535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.095385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.111695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.114732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.119925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.123491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.134571 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.138216 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.147378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.150149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.156495 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.159219 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.170809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.173722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.180370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.183712 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.189853 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.192583 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.210630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.213821 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.219346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.223617 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.233429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.237087 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.246798 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.249876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.256002 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.259668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.271770 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.276079 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.284049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.287492 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.294351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.298168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.312505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.316518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.322926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.326958 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.336330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.340620 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.351783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.355098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.362399 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.365727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.378931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.382721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.390932 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.394140 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.400093 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.403060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.416786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.419713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.426177 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.429416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.438669 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.442423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.451540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.454289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.460768 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.463610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.475022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.478660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.485435 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.489366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.495980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.500124 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.514300 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.517677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.524974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.528975 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.538882 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.543567 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.553188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.557540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.564627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.568701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.597951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.602978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.615381 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.620497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.628942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.633281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.642055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451e73ca00> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1886  675]\n",
            " [ 571 4055]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.77      0.74      0.75      2561\n",
            "        True       0.86      0.88      0.87      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.81      0.81      0.81      7187\n",
            "weighted avg       0.83      0.83      0.83      7187\n",
            "\n",
            "evaluating epoch: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:53:27.240442 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.243529 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.247287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.249231 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.260756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.274105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.285186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.292542 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.295955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.317713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.321244 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.328744 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.332235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.339138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.344301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.357508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.362529 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.368685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.371773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.380228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.384035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.393233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.396332 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.401598 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.405497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.416644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.420087 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.427747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.431385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.437427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.440922 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.458425 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.465049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.472049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.477802 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.487715 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.494055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.503479 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.506833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.514506 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.517759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.531934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.537631 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.544481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.548213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.554965 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.559149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.574815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.578083 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.585537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.589627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.604373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.608911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.620024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.625610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.631618 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.636786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.648602 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.654438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.662097 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.665512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.672909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.675954 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.691348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.694407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.699615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.702879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.713624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.717968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.727427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.730390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.736588 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.741507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.752963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.757550 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.765193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.768603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.776055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.779217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.795726 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.798985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.805115 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.808578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.818773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.823596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.833307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.836369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.842422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.845793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.859184 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.862385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.869759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.873001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.879702 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.882610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.897022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.901146 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.906580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.910099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.920667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.924491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.934351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.938214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.943888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.947139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.958892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.962032 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.969317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.972373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.980094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.983607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.998897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.002306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.008711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.013187 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.023302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.030889 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.041596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.046303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.051645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.054974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.070290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.074655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.081662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.086102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.092809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.095833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.112260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.115350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.120348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.124969 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.135236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.139321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.150362 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.153547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.158638 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.162261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.174215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.177246 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.184202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.188514 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.195357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.198352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.216077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.222641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.229804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.235280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.244657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.250248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.259779 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.265240 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.272155 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.277664 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.290193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.296847 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.303311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.307801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.314013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.322839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.341890 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.347493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.354247 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.357345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.368259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.373095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.383347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.387799 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.395924 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.399672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.416894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.420628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.429065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.432662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.439879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.443330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.463037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.467816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.474978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.479444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.489401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.496710 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.509727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.513233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.518886 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.522983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.534488 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.537508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.544893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.548863 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.555135 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.561307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.576173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.579898 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.587142 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.590967 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.604101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.609049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.619076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.623666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.630029 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.633857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.664513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.668865 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.685376 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.688988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.698379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.701883 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.711785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451ae54320> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1795  766]\n",
            " [ 414 4212]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.81      0.70      0.75      2561\n",
            "        True       0.85      0.91      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.83      0.81      0.81      7187\n",
            "weighted avg       0.83      0.84      0.83      7187\n",
            "\n",
            "evaluating epoch: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:54:20.631896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.634183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.635942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.638317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.646512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.660676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.670666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.677151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.680270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.699136 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.702785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.709415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.713391 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.719109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.722668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.736784 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.740044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.745690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.748714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.756797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.760668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.769737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.772711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.778176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.781213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.792236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.795416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.802624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.806025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.812031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.815004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.828583 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.831435 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.838222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.841377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.850975 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.856754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.865314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.868114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.874644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.877839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.891766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.894934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.905056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.908706 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.915843 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.919943 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.934562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.938622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.946363 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.950485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.962754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.968592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.980176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.983951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.989645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.992876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.004736 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.009953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.017261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.022120 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.028229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.034991 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.053122 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.057888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.063193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.067095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.077392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.082221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.090615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.095101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.100640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.105042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.117588 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.122137 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.130194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.133933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.140686 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.144864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.159336 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.163445 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.170713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.174258 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.184294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.188628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.202228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.207670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.215125 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.219700 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.231292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.234848 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.241868 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.245095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.251497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.254856 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.272085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.279092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.285887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.289346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.302887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.307949 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.318088 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.321417 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.328196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.331976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.343797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.347537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.354558 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.357641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.364450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.367907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.387862 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.391894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.397286 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.400188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.408698 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.412498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.422449 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.425939 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.431839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.436516 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.447560 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.450606 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.457911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.461030 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.466552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.469494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.484086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.487162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.493217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.496315 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.505290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.509259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.517741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.521809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.527358 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.531430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.545408 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.550689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.557677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.563448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.571120 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.574973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.592633 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.597121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.603308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.607161 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.616852 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.621041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.630070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.633181 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.638636 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.641452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.652896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.656491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.663347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.666100 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.672849 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.675612 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.691749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.696367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.702080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.705936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.715487 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.720585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.730170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.734246 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.739912 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.743607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.755215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.759511 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.766994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.770663 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.778131 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.782330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.797708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.801867 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.808507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.812369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.822621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.827733 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.837347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.841769 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.849273 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.852955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.867615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.871176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.880419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.884248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.891129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.896747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.911070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.914737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.920794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.926275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.936098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.940676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.949947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.953720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.960572 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.964485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.996165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.000281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.014442 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.018073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.028736 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.032519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.041603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451d07ecd0> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1630  931]\n",
            " [ 282 4344]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.85      0.64      0.73      2561\n",
            "        True       0.82      0.94      0.88      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.84      0.79      0.80      7187\n",
            "weighted avg       0.83      0.83      0.82      7187\n",
            "\n",
            "evaluating epoch: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:55:13.477834 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.479759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.481285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.482909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.491228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.505517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.516432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.523371 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.526566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.545430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.548402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.554681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.557816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.564826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.567754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.583065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.586215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.591299 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.594506 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.603667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.607311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.616362 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.619277 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.624445 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.627918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.641195 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.645199 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.652498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.655476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.662281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.665250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.681931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.687179 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.693422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.696568 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.707257 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.711314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.721312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.725239 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.731153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.737531 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.754182 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.757498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.765841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.770411 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.777970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.781249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.798732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.802290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.809520 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.822345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.836865 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.841887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.852044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.855855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.862494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.866548 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.878447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.882392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.889918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.893022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.901326 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.905913 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.926084 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.930227 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.938517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.943286 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.955132 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.960527 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.972109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.976475 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.983378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.987374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.999603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.003941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.011518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.015064 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.025665 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.029661 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.045475 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.049040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.057166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.060566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.072221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.077193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.088330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.092173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.099076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.103348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.115406 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.123080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.134345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.139343 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.146888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.150108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.166775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.171788 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.179903 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.185028 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.195671 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.200113 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.214555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.220221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.225127 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.230494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.240952 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.243792 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.251050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.253855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.260310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.263226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.278076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.281153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.287060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.291397 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.300738 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.305413 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.314681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.318823 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.327925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.330770 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.342796 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.345747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.352814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.355927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.362379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.365226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.379295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.385085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.391108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.394270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.404412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.408548 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.418283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.422022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.428787 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.432018 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.443955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.447112 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.455205 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.458534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.466449 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.469553 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.485738 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.489820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.497174 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.500315 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.509931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.513809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.523498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.526979 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.533751 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.537379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.551985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.555569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.564538 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.568118 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.576201 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.580130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.596432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.600150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.607134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.610831 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.620956 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.625441 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.635716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.639319 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.646343 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.650563 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.663815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.668566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.675621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.678985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.686789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.690980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.706228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.709160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.715537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.718499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.728070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.731587 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.741182 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.744060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.748025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.753747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.768403 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.774772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.784641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.788367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.795212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.799352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.816113 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.821352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.827899 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.831317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.842695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.847393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.857613 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.860564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.867119 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.871105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.904002 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.907902 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.922360 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.925679 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.935153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.940270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.949819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[1814  747]\n",
            " [ 397 4229]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.82      0.71      0.76      2561\n",
            "        True       0.85      0.91      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.84      0.81      0.82      7187\n",
            "weighted avg       0.84      0.84      0.84      7187\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUkPORA68JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd = model_predict(estimator,test_features_fixed,test_InputExamples_fixed)\n",
        "true_label = list(test_fixed['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQReX-Hf0WiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(1,13):\n",
        "  \n",
        "#   print('processing epoch: %d'%i)\n",
        "#   pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/saved_models/model.ckpt-%d'%(i*3000))\n",
        "#   true_label = list(dev['label'])\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#       labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpz2FhG3_sS",
        "colab_type": "code",
        "outputId": "0a2b0d58-4c7e-4236-8af2-7ab20d131a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas\n",
        "pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%(1392*3))\n",
        "true_label = list(dev['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "      labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "print(labels_val)\n",
        "print(dev)\n",
        "dev['predicted'] = pandas.Series(labels_val)\n",
        "dev.to_csv('/content/dev_predicted.csv', encoding='utf8')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[ 969  259]\n",
            " [ 194 2058]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.83      0.79      0.81      1228\n",
            "        True       0.89      0.91      0.90      2252\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      3480\n",
            "   macro avg       0.86      0.85      0.86      3480\n",
            "weighted avg       0.87      0.87      0.87      3480\n",
            "\n",
            "[1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1]\n",
            "      doc_id  label                                           sentence\n",
            "0       1862   True  -LRB- tgt has called on the international comm...\n",
            "1       1906   True  But as the Finals have progressed  and Bryant'...\n",
            "2       1842  False  tgthas drawn national attention -- and calls f...\n",
            "3       1867   True  -LRB- tgt said Mass in the chapel of the Casa ...\n",
            "4       1920   True  tgt's grass-court winning streak ended at 14 w...\n",
            "5       1852  False  Disgruntled wide receiver Martavis Bryant foll...\n",
            "6       2085  False  tgta Hall of Fame broadcaster known as much fo...\n",
            "7       2074   True  After tgtreiterated  tgt  vow on Monday to ben...\n",
            "8       2004  False  WEST PALM BEACH  Fla. (Reuters) -  U.S. Presid...\n",
            "9       1978  False  President Donald Trump said  he  was surprised...\n",
            "10      2057  False  New Poll: In Swing Districts  Voters Strongly ...\n",
            "11      2096   True  Forty years ago this month   tgtbecame the fir...\n",
            "12      2035   True  CLOSE  tgtArrived in Myanmar Time\\nEthnic Kach...\n",
            "13      2083   True  Mother Jones scribe tgthas a different approac...\n",
            "14      1875  False  If you bought a Bitcoin in early 2017  when on...\n",
            "15      1875  False  If you bought a Bitcoin in early 2017  when on...\n",
            "16      2011  False  The idea that the Knicks would somehow turn  t...\n",
            "17      1845  False  NAIROBI   Kenya ( AP ) â Kenya ' s attorney ...\n",
            "18      1914   True  An economic downturn has a way of bringing out...\n",
            "19      2007   True  [MASK]are winning the internet over with  tgt ...\n",
            "20      1934  False  A Chicago police officer who faces potential f...\n",
            "21      1896  False  When President [MASK] goes to Guadalajara  Mex...\n",
            "22      1972  False  In a tweet on Monday   tgt  claimed  he  had g...\n",
            "23      2018  False  Lawyers for tgtshown last month  acknowleged T...\n",
            "24      1968  False  Reports surfaced last week that  tgttried to p...\n",
            "25      1853   True  ' I Came   I Saw   I Selfied ': How Instagram ...\n",
            "26      1924   True  Having just wrapped up a visit to China on the...\n",
            "27      1892  False  -- Many of my stadium visits have dovetailed w...\n",
            "28      2103   True  FILE - In this Dec. 17   2017  file photo  tgt...\n",
            "29      1852  False  Disgruntled wide receiver Martavis Bryant foll...\n",
            "...      ...    ...                                                ...\n",
            "3450    1946   True  But [MASK] were so startling and so disturbing...\n",
            "3451    1965   True  tgtwas expecting a different sort of reception...\n",
            "3452    2069  False  Virginiaâs DeâAndre  Hunter  did his part ...\n",
            "3453    1952   True  The  tgtstory continues to reverberate  as new...\n",
            "3454    2082  False  More than decade ago  Shah and a friend  tgt  ...\n",
            "3455    1969  False  Washington (CNN) White House press secretary S...\n",
            "3456    1859   True  tgtgot the surprise of a lifetime when  tgt  s...\n",
            "3457    2083  False  Mother Jones scribe tgthas a different approac...\n",
            "3458    2049   True  UPDATE: tgttells Rolling Stone that an erroneo...\n",
            "3459    1890   True  Italian former leftist guerrilla tgt  leaves t...\n",
            "3460    2082   True  More than decade ago  Shah and a friend  tgt  ...\n",
            "3461    1854   True  The Harvey Weinsteins of the world aren ' t al...\n",
            "3462    1998   True  tgt arrives to celebrate Mass at the Las Palma...\n",
            "3463    1952   True  The  tgtstory continues to reverberate  as new...\n",
            "3464    1951  False  Who  Pat Shurmur? No. No way Kyle learned that...\n",
            "3465    2072   True  City ethics officials  are looking into whethe...\n",
            "3466    2033   True  tgthasn't been on the road much in 2017  but t...\n",
            "3467    2035  False  CLOSE  tgtArrived in Myanmar Time\\nEthnic Kach...\n",
            "3468    1998   True  tgt arrives to celebrate Mass at the Las Palma...\n",
            "3469    2060  False  In the wake of the Harvey Weinstein revelation...\n",
            "3470    2051   True  Seven women Democratic Senators turned on  tgt...\n",
            "3471    2048  False  Meet tgt .  tgt 're all turning 36  and so is ...\n",
            "3472    2070   True  tgt Tarly   told TV Guide that characters are ...\n",
            "3473    1892  False  -- Many of my stadium visits have dovetailed w...\n",
            "3474    1850  False  What Went Wrong In Charlottesville ? Almost Ev...\n",
            "3475    1955   True  tgtsaid in a series of tweets on Sunday that  ...\n",
            "3476    2079   True  tgtsaid the Corker plan would basically be imp...\n",
            "3477    2103   True  FILE - In this Dec. 17   2017  file photo  tgt...\n",
            "3478    1865   True  God is the patient and merciful artisan of our...\n",
            "3479    1889   True  Paul Manafort  Rick Gates Back In Court For Ba...\n",
            "\n",
            "[3480 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GMYpmM-Q_B",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascfbXXbzEpw",
        "colab_type": "code",
        "outputId": "3b443645-20f0-4a63-9812-e3219a444502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# ## Run it if you want to train for a range of epochs and see the validation error and save the prediction on than\n",
        "# ## ** Part Name **\n",
        "# # mds = []\n",
        "# # evs = []\n",
        "# pds_dev = []\n",
        "# pds_tr = []\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) \n",
        "\n",
        "# for i in range(1,11):\n",
        "#   print('----------------------- Starting Epoch %d-----------------------'%i)\n",
        "\n",
        "#   NUM_TRAIN_EPOCHS = i\n",
        "  \n",
        "#   num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "#   num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# #   model_fn = model_fn_builder(\n",
        "# #   num_labels=len(label_list),\n",
        "# #   learning_rate=LEARNING_RATE,\n",
        "# #   num_train_steps=num_train_steps,\n",
        "# #   num_warmup_steps=num_warmup_steps,\n",
        "# #   use_tpu=True,\n",
        "# #   bert_hub_module_handle=BERT_MODEL_HUB)\n",
        "\n",
        "#   model_fn = model_fn_builder(\n",
        "#   bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#   num_labels=len(label_list),\n",
        "#   init_checkpoint=INIT_CHECKPOINT,\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps,\n",
        "#   use_tpu=True,\n",
        "#   use_one_hot_embeddings=True)\n",
        "  \n",
        "  \n",
        "#   estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=run_config,\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "  \n",
        "#   model_train(estimator_from_tfhub)\n",
        "# #   ev = model_eval(estimator_from_tfhub)\n",
        "\n",
        "# #   print(' -------------------- Train Prediction --------------------')\n",
        "# #   pd = model_predict(estimator_from_tfhub,train_features,train_InputExamples)\n",
        "# #   true_label = list(train['label'])\n",
        "# #   pds_tr.append(pd)\n",
        "# #   labels_val = []\n",
        "# #   for item in pd:\n",
        "# #     labels_val.append(np.argmax(item[1]))\n",
        "# #   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# #   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "#   print(' -------------------- Dev Prediction --------------------')\n",
        "#   pd = model_predict(estimator_from_tfhub,dev_features[:3100],dev_InputExamples[:3100])\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   pds_dev.append(pd)\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for tr,dv in zip(pds_tr,pds_dev):\n",
        "#   labels_val = []\n",
        "#   true_label = list(train['label'])\n",
        "#   for item in tr:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "#   labels_val = []\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   for item in dv:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------- Starting Epoch 1-----------------------\n",
            "***** Started training at 2019-05-15 17:57:14.080193 *****\n",
            "  Num examples = 122150\n",
            "  Batch size = 8\n",
            "start running estimator\n",
            "(1, 2)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_dvH1MmpAr3",
        "colab_type": "code",
        "outputId": "ae794af2-b7a8-4188-b4ef-8dc97e42c452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2520
        }
      },
      "source": [
        "# prd = model_predict(estimator,train_features,train_InputExamples)\n",
        "# prd = [item for item in predictions]\n",
        "len(prd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9efd420be1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_InputExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# prd = [item for item in predictions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-3ccd6d9b41c7>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(estimator, input_features, input_examples)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mpredict_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2498\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.pyc\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2492\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m           yield_single_examples=yield_single_examples):\n\u001b[0m\u001b[1;32m   2495\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mscaffold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 config=self._session_config),\n\u001b[0;32m--> 627\u001b[0;31m             hooks=all_hooks) as mon_sess:\n\u001b[0m\u001b[1;32m    628\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpreds_evaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    932\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    933\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \"\"\"\n\u001b[1;32m   1121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1312\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n\nCaused by op u'save_2/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-9efd420be1df>\", line 1, in <module>\n    prd = model_predict(estimator,train_features,train_InputExamples)\n  File \"<ipython-input-45-3ccd6d9b41c7>\", line 18, in model_predict\n    return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2494, in predict\n    yield_single_examples=yield_single_examples):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 627, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 934, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1122, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1127, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 805, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 562, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 3033, in _finalize\n    wrapped_finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 217, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 604, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 507, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 385, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khuNEnMw2wiv",
        "colab_type": "code",
        "outputId": "83f8b377-31f6-4976-8cf4-d26796869d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "len(a)\n",
        "# import numpy as np\n",
        "# from sklearn import metrics\n",
        "\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(dev['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(np.argmax(item[1]))\n",
        "# true_label = list(train['label'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  48]\n",
            " [  0 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        48\n",
            "        True       0.76      1.00      0.86       152\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       200\n",
            "   macro avg       0.38      0.50      0.43       200\n",
            "weighted avg       0.58      0.76      0.66       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "outputId": "f4559586-e7a1-4674-c430-a3b426c101f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed['sentiment'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-985837170ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_from_tfhub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_InputExamples_fixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabels_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fixed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_InputExamples_fixed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzzg7MRSk5E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5SpMUg7b9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.logging.set_verbosity(tf.logging.DEBUG) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test['sentiment'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKRWIAgn_YAE",
        "colab_type": "code",
        "outputId": "a5acf1b3-8717-4321-fd7c-b4804432240e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# len(labels_val)\n",
        "# len(test_InputExamples)\n",
        "# len(predictions)\n",
        "model_eval(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started evaluation at 2019-05-09 17:10:37.771525 *****\n",
            "  Num examples = 3479\n",
            "  Batch size = 8\n",
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-904242ebce51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_from_tfhub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-2dd67f62c231>\u001b[0m in \u001b[0;36mmodel_eval\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       drop_remainder=True)\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'***** Finished evaluation at {} *****'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0moutput_eval_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval_results.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m   def predict(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.pyc\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2474\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m   def _actual_eval(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 493\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    494\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    495\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1424\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1460\u001b[0;31m         features, labels, model_fn_lib.ModeKeys.EVAL, config)\n\u001b[0m\u001b[1;32m   1461\u001b[0m     eval_metric_ops = _verify_and_create_loss_metric(\n\u001b[1;32m   1462\u001b[0m         estimator_spec.eval_metric_ops, estimator_spec.loss)\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   2249\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m       return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n\u001b[0;32m-> 2251\u001b[0;31m                                                       config)\n\u001b[0m\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn_for_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config, params)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           compile_op, total_loss, host_calls, scaffold, eval_hooks = (\n\u001b[0;32m-> 2651\u001b[0;31m               _eval_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n\u001b[0m\u001b[1;32m   2652\u001b[0m           \u001b[0miterations_per_loop_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_or_get_iterations_per_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m           mean_loss = math_ops.div(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_eval_on_tpu_system\u001b[0;34m(ctx, model_fn_wrapper, dequeue_fn)\u001b[0m\n\u001b[1;32m   2867\u001b[0m       \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2868\u001b[0m       \u001b[0moutputs_from_all_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2869\u001b[0;31m       device_assignment=ctx.device_assignment)\n\u001b[0m\u001b[1;32m   2870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.pyc\u001b[0m in \u001b[0;36msplit_compile_and_shard\u001b[0;34m(computation, inputs, num_shards, input_shard_axes, outputs_from_all_shards, output_shard_axes, infeed_queue, device_assignment, name)\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minfeed_queue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfeed_queue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m       \u001b[0mdevice_assignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_assignment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   \u001b[0;31m# There must be at least one shard since num_shards > 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.pyc\u001b[0m in \u001b[0;36msplit_compile_and_replicate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mvscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_custom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcomputation_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0mvscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_use_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_use_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mmulti_tpu_eval_steps_on_single_shard\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmulti_tpu_eval_steps_on_single_shard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m     return training_loop.repeat(iterations_per_loop_var, single_tpu_eval_step,\n\u001b[0;32m-> 2862\u001b[0;31m                                 [_ZERO_LOSS])\n\u001b[0m\u001b[1;32m   2863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m   (compile_op, loss,) = tpu.split_compile_and_shard(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(n, body, inputs, infeed_queue, name)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   outputs = while_loop(\n\u001b[0;32m--> 208\u001b[0;31m       cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m   return control_flow_ops.while_loop(\n\u001b[0;32m--> 170\u001b[0;31m       condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3554\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3556\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 3087\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   3088\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   3021\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mbody_wrapper\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mdequeue_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdequeue_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# If the computation only returned one value, make it a tuple.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.pyc\u001b[0m in \u001b[0;36mbody_wrapper\u001b[0;34m(i, *args)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbody_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_convert_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(total_loss)\u001b[0m\n\u001b[1;32m   1422\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m       \u001b[0mtpu_estimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_estimator_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TPUEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         raise RuntimeError(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0m_add_item_to_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_CTX_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m     \u001b[0mestimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m     if (running_on_cpu and\n\u001b[1;32m   1595\u001b[0m         isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\n",
            "\u001b[0;32m<ipython-input-10-5ebb4ee1571f>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           eval_metrics=eval_metrics,scaffold_fn=scaffold_fn)\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, mode, predictions, loss, train_op, eval_metrics, export_outputs, scaffold_fn, host_call, training_hooks, evaluation_hooks, prediction_hooks)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhost_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mhost_calls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host_call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0m_OutfeedHostCall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mtraining_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(host_calls)\u001b[0m\n\u001b[1;32m   1652\u001b[0m               \u001b[0;34m'In TPUEstimatorSpec.{}, length of tensors {} does not match '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m               'method args of the function, which takes {}.'.format(\n\u001b[0;32m-> 1654\u001b[0;31m                   name, len(host_call[1]), len(fn_args)))\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: In TPUEstimatorSpec.eval_metrics, length of tensors 3 does not match method args of the function, which takes 4."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIwoEwmrk6ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqh11HPVAsvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####seq 128, Small BERT, Batchsize 32 for train, 8 for dev and test\n",
        "# 1)I0423 16:57:01.507206 140076901431168 basic_session_run_hooks.py:249] loss = 0.40919852, step = 46\n",
        "# 2)I0423 18:08:31.116359 140500355729280 basic_session_run_hooks.py:249] loss = 0.7756149, step = 92\n",
        "# 4)Loss for final step: 0.6891643.\n",
        "# 5)Loss for final step: 0.9730371\n",
        "# 6)Loss for final step: 0.4775733\n",
        "# 7)Loss for final step: 1.2802429.\n",
        "# 8)Loss for final step: 0.509207\n",
        "# 9)loss = 0.29262337, step = 414\n",
        "# 10)Loss for final step: 0.47267017\n",
        "\n",
        "\n",
        "# 2)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 92\n",
        "#   loss = 0.79767215\n",
        "# 3)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 138\n",
        "#   loss = 0.79767215\n",
        "# 4)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.86550355\n",
        "#   global_step = 184\n",
        "#   loss = 0.83540887\n",
        "# 5)***** Eval results *****\n",
        "#   eval_accuracy = 0.5964286\n",
        "#   eval_loss = 0.88053304\n",
        "#   global_step = 230\n",
        "#   loss = 0.91362196\n",
        "# 6)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.90498805\n",
        "#   global_step = 276\n",
        "#   loss = 1.0806552\n",
        "# 7)***** Eval results *****\n",
        "#   eval_accuracy = 0.56785715\n",
        "#   eval_loss = 0.92742974\n",
        "#   global_step = 322\n",
        "#   loss = 1.0180835\n",
        "# 8)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9436406\n",
        "#   global_step = 368\n",
        "#   loss = 0.9721719\n",
        "# 9)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9714315\n",
        "#   global_step = 414\n",
        "#   loss = 0.5798999\n",
        "# 10)***** Eval results *****\n",
        "#   eval_accuracy = 0.54285717\n",
        "#   eval_loss = 1.0072392\n",
        "#   global_step = 460\n",
        "#   loss = 1.053762\n",
        "  \n",
        "#   DEV Info:\n",
        "# 1)[[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "# 2)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 3)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 4)[[  0   8  42]\n",
        "#  [  0  16 100]\n",
        "#  [  0  17 157]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        50\n",
        "#      Neutral       0.39      0.14      0.20       116\n",
        "#     Positive       0.53      0.90      0.66       174\n",
        "\n",
        "#    micro avg       0.51      0.51      0.51       340\n",
        "#    macro avg       0.31      0.35      0.29       340\n",
        "# weighted avg       0.40      0.51      0.41       340\n",
        "# 5)[[  0  17   6]\n",
        "#  [  0  40  57]\n",
        "#  [  0  35 129]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.41      0.42        97\n",
        "#     Positive       0.67      0.79      0.72       164\n",
        "\n",
        "#    micro avg       0.60      0.60      0.60       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.60      0.56       284\n",
        "# 6)[[  0  19   4]\n",
        "#  [  0  44  53]\n",
        "#  [  0  42 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.45      0.44        97\n",
        "#     Positive       0.68      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.58      0.56       284\n",
        "# 7)[[  0  18   5]\n",
        "#  [  0  46  51]\n",
        "#  [  0  47 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.41      0.47      0.44        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.36      0.40      0.38       284\n",
        "# weighted avg       0.53      0.57      0.55       284\n",
        "# 8)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  53 111]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.68      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "# 9)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  54 110]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.67      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "# 10)[[  0  18   5]\n",
        "#  [  0  51  46]\n",
        "#  [  0  62 102]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.53      0.45        97\n",
        "#     Positive       0.67      0.62      0.64       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.54      0.52       284\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0h7wMprmvyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Seq:256, small BERT, batch size 32 for train, 8 for test and dev\n",
        "# 3) {'loss': 0.79650426, 'eval_accuracy': 0.58214283, 'eval_loss': 0.86125755, 'global_step': 138}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# 4) {'loss': 0.73700047, 'eval_accuracy': 0.5928571, 'eval_loss': 0.8223035, 'global_step': 184}\n",
        "# [[  0  11  12]\n",
        "#  [  0  20  77]\n",
        "#  [  0  17 147]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.21      0.28        97\n",
        "#     Positive       0.62      0.90      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.35      0.37      0.34       284\n",
        "# weighted avg       0.50      0.59      0.52       284\n",
        "\n",
        "# 5) {'loss': 0.71594626, 'eval_accuracy': 0.5928571, 'eval_loss': 0.82239425, 'global_step': 230}\n",
        "# [[  0  19   4]\n",
        "#  [  0  51  46]\n",
        "#  [  0  48 116]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.53      0.47        97\n",
        "#     Positive       0.70      0.71      0.70       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.55      0.59      0.57       284\n",
        "\n",
        "# 6) {'loss': 0.7227276, 'eval_accuracy': 0.5857143, 'eval_loss': 0.8479867, 'global_step': 276}\n",
        "# [[  0  21   2]\n",
        "#  [  2  51  44]\n",
        "#  [  1  49 114]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.53      0.47        97\n",
        "#     Positive       0.71      0.70      0.70       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.56      0.58      0.57       284\n",
        "\n",
        "# 7){'loss': 0.83304703, 'eval_accuracy': 0.5642857, 'eval_loss': 0.90589315, 'global_step': 322}\n",
        "# [[  0  19   4]\n",
        "#  [  4  40  53]\n",
        "#  [  3  42 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.41      0.40        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.38      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "\n",
        "# 8){'loss': 0.84049994, 'eval_accuracy': 0.575, 'eval_loss': 0.93640614, 'global_step': 368}\n",
        "# [[  2  18   3]\n",
        "#  [  5  42  50]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.22      0.09      0.12        23\n",
        "#      Neutral       0.40      0.43      0.42        97\n",
        "#     Positive       0.69      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.44      0.41      0.42       284\n",
        "# weighted avg       0.55      0.57      0.56       284\n",
        "\n",
        "# 9){'loss': 0.8601472, 'eval_accuracy': 0.5642857, 'eval_loss': 0.95250976, 'global_step': 414}\n",
        "# [[  0  18   5]\n",
        "#  [  4  37  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.38      0.38        97\n",
        "#     Positive       0.67      0.74      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "\n",
        "\n",
        "# 10){'loss': 0.9091368, 'eval_accuracy': 0.5535714, 'eval_loss': 0.9715489, 'global_step': 460}\n",
        "# [[  0  19   4]\n",
        "#  [  4  37  56]\n",
        "#  [  1  44 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.37      0.38      0.38        97\n",
        "#     Positive       0.66      0.73      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.34      0.37      0.36       284\n",
        "# weighted avg       0.51      0.55      0.53       284\n",
        "\n",
        "# 11){'loss': 0.9756337, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0129306, 'global_step': 506}\n",
        "# [[  3  16   4]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.39      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 12){'loss': 0.9551747, 'eval_accuracy': 0.56785715, 'eval_loss': 1.0222387, 'global_step': 552}\n",
        "# [[  3  17   3]\n",
        "#  [  5  38  54]\n",
        "#  [  2  43 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.30      0.13      0.18        23\n",
        "#      Neutral       0.39      0.39      0.39        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.45      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 13){'loss': 1.0276382, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0547612, 'global_step': 598}\n",
        "# [[  3  17   3]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 14){'loss': 1.0036505, 'eval_accuracy': 0.55714285, 'eval_loss': 1.0697843, 'global_step': 644}\n",
        "# [[  3  17   3]\n",
        "#  [  7  36  54]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.37      0.37      0.37        97\n",
        "#     Positive       0.67      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.43      0.41      0.41       284\n",
        "# weighted avg       0.54      0.55      0.54       284\n",
        "\n",
        "# 15){'loss': 1.0295725, 'eval_accuracy': 0.5642857, 'eval_loss': 1.0809377, 'global_step': 690}\n",
        "# [[  3  17   3]\n",
        "#  [  5  39  53]\n",
        "#  [  1  46 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.40      0.39        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 16){'loss': 1.0889318, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1056138, 'global_step': 736}\n",
        "# [[  3  17   3]\n",
        "#  [  8  33  56]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.42      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 17){'loss': 1.1312778, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1440269, 'global_step': 782}\n",
        "# [[  3  17   3]\n",
        "#  [  9  33  55]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 18){'loss': 1.1436831, 'eval_accuracy': 0.55, 'eval_loss': 1.1613237, 'global_step': 828}\n",
        "# [[  3  16   4]\n",
        "#  [  8  35  54]\n",
        "#  [  2  45 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.54       284\n",
        "\n",
        "# 19){'loss': 1.1817628, 'eval_accuracy': 0.55, 'eval_loss': 1.1834545, 'global_step': 874}\n",
        "# [[  3  16   4]\n",
        "#  [  8  34  55]\n",
        "#  [  1  45 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.36      0.35      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.53       284\n",
        "\n",
        "# 20){'loss': 1.1921012, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1976833, 'global_step': 920}\n",
        "# [[  3  16   4]\n",
        "#  [  9  35  53]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.53      0.54      0.53       284\n",
        "\n",
        "# 21){'loss': 1.2514663, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2244278, 'global_step': 966}\n",
        "# [[  3  16   4]\n",
        "#  [ 11  32  54]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.19      0.13      0.15        23\n",
        "#      Neutral       0.34      0.33      0.33        97\n",
        "#     Positive       0.66      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.40      0.39      0.39       284\n",
        "# weighted avg       0.51      0.53      0.52       284\n",
        "\n",
        "# 22){'loss': 1.2630298, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2527977, 'global_step': 1012}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284\n",
        "\n",
        "# 23){'loss': 1.3330169, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2815608, 'global_step': 1058}\n",
        "# [[  6  13   4]\n",
        "#  [ 14  31  52]\n",
        "#  [  9  42 113]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.26      0.23        23\n",
        "#      Neutral       0.36      0.32      0.34        97\n",
        "#     Positive       0.67      0.69      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.41      0.42      0.42       284\n",
        "# weighted avg       0.53      0.53      0.53       284\n",
        "\n",
        "# 24){'loss': 1.3111317, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2898273, 'global_step': 1104}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqSe326lwHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Larg BERT seq:256, batchsize 8 for train, dev , test\n",
        "# {'loss': 1.0679293, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8965841, 'global_step': 93}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0362501, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88540924, 'global_step': 186}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0290864, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8812368, 'global_step': 279}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0271251, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88323754, 'global_step': 372}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.02654, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8804489, 'global_step': 465}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# SAME FOR 10 EPOCHS!!!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPSn-7_J05J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy2D-6ZBJ10t",
        "colab_type": "code",
        "outputId": "2cf4abd1-d66d-4384-d04b-01b532d628e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZunWbyH8xrEb",
        "colab_type": "code",
        "outputId": "48e21d5a-c62e-4273-8fb4-4658380d0c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data_train['DOCUMENT'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idaqpVbZz57p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}