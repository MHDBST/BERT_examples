{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskLM_document_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/MaskLM_document_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "outputId": "e109e257-0af8-475c-e1ca-a43d81658b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "('TPU address is', 'grpc://10.42.39.162:8470')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1202 16:26:01.364530 140711381813120 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4342573636133850899),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16832590660377547623),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8895006221416704662),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10620825723298713799),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5541778135749699100),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 14112253399283000349),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 18383500680014699473),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4432500483059906639),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14420789982731503478),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 13201499332437650128),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 11237675425894323480)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "outputId": "ebd2985a-d07f-422d-e1cf-7f49bcab4421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "# import python modules defined by BERT\n",
        "from bert import modeling\n",
        "# import optimization\n",
        "# import run_classifier\n",
        "from bert import run_classifier_with_tfhub\n",
        "# import tokenization\n",
        "\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1202 16:26:05.794640 140711381813120 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "300eaa54-7262-4a10-e208-9d5762491feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/aug_19_models/mask_lm/weighted/last3layers'.format(BUCKET)\n",
        "# V2_augment/doc_level/v2/smallBERT-docLevel-seq512/experiment1'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "# !gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/aug_19_models/mask_lm/weighted/last3layers *****\n",
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import modeling\n",
        "# import run_classifier\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  pre_tvars = tf.trainable_variables()\n",
        "#   print('all trainable variables: >>',pre_tvars)\n",
        "  tvars = pre_tvars\n",
        "  # tvars = [item for item in pre_tvars if not '/layer_0/'  in item.name and not '/layer_1/'  in item.name and not '/layer_2/'  in item.name\n",
        "  #         and not '/layer_3/'  in item.name and not '/layer_4/'  in item.name and not '/layer_5/'  in item.name and not '/layer_6/'  in item.name \n",
        "  #         and not '/layer_7/'  in item.name and not '/layer_8/'  in item.name ]\n",
        "  #          #and not '/layer_9/' in item.name]\n",
        "#   and not '/layer_10/'  in item.name \n",
        "#           and not '/layer_11/'  in item.name ]\n",
        "  print('excluded trainable variables: >>',tvars)\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_TRAIN_EPOCHS = 15.0  ## Activate if ** is Not ACTIVATED\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 50\n",
        "SAVE_SUMMARY_STEPS = 20\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "outputId": "ee913729-bf23-4abf-fc6b-35e8942b082c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# data_train = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_train_v3.csv', encoding='latin-1')[:-1]\n",
        "# data_augment = pd.read_csv('/content/mask_lm_postprocess.csv', encoding='latin-1')[:-1]\n",
        "# data_dev = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/mask_lm_3Dec_7Dec_reindex_fixed_test_v3.csv', encoding='latin-1')\n",
        "\n",
        "\n",
        "# data_train = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_train_v3.csv'), encoding='latin-1')\n",
        "# # data_augment = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_postprocess.csv'), encoding='latin-1')\n",
        "# data_dev = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_dev_v3.csv'), encoding='latin-1')\n",
        "# data_test= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_random_test_v3.csv'), encoding='latin-1')\n",
        "# data_test_fixed= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/mask_lm_3Dec_7Dec_reindex_fixed_test_v3.csv'), encoding='latin-1')\n",
        "\n",
        "data_pref = 'gs://bert_example/data_aug19/masked_lm/mask_lm_combined_shuffled_3Dec_7Dec_aug19_reindex_%s.csv'\n",
        "\n",
        "data_train = pd.read_csv(tf.gfile.GFile(data_pref % str('train')), encoding='latin-1')\n",
        "data_dev   = pd.read_csv(tf.gfile.GFile(data_pref % 'dev'), encoding='latin-1')\n",
        "data_test  = pd.read_csv(tf.gfile.GFile(data_pref % 'random_test'), encoding='latin-1')\n",
        "data_test_fixed= pd.read_csv(tf.gfile.GFile(data_pref % 'fixed_test'), encoding='latin-1')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(df):\n",
        "#   print('df length>>',len(df['DOCUMENT']))\n",
        "  data = {}\n",
        "  df['DOCUMENT'] = df['DOCUMENT'].str.replace('\\[TGT\\]','tgt')\n",
        "  data[\"sentence\"] = df['DOCUMENT']\n",
        "  data[\"label\"] =df[\"LABEL\"]\n",
        "  data['doc_id'] = df[\"DOCUMENT_INDEX\"]\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None):\n",
        "  df_new = load_directory_data(df[:index])\n",
        "#   print(df_new)\n",
        "  true_df = df_new[df_new['label'] == True]\n",
        "  false_df = df_new[df_new['label'] == False]\n",
        "#   print('true_df>>>',len(true_df))\n",
        "#   true_df[\"polarity\"] = 1\n",
        "#   false_df[\"polarity\"] = 0\n",
        "  return pd.concat([true_df, false_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train = load_dataset(data_train)\n",
        "# train_augment = load_dataset(data_augment)\n",
        "# train = pd.concat([train, train_augment]).sample(frac=1).reset_index(drop=True)\n",
        "dev = load_dataset(data_dev)\n",
        "test = load_dataset(data_test)\n",
        "test_fixed = load_dataset(data_test_fixed)\n",
        "\n",
        "print('train set length: %d,dev set length: %d, test set lenght: %d,  fixed test: %d'%\n",
        "      (len(train),len(dev),len(test),len(test_fixed)))\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set length: 45944,dev set length: 7187, test set lenght: 8130,  fixed test: 12604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMNSsUHdsbY",
        "colab_type": "code",
        "outputId": "9f739f45-cd20-47e9-8822-d0f0f9f49a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0,1]\n",
        "use_tpu = True\n",
        "print(len(train[train['label']==1]))\n",
        "print(len(train[train['label']==0]))\n",
        "print(len(dev[dev['label']==1]))\n",
        "print(len(dev[dev['label']==0]))\n",
        "\n",
        "# a = 1./len(dev[dev['label']==0])\n",
        "# b = 1./len(dev[dev['label']==1])\n",
        "# print(a/(a+b) , b/(a+b))\n",
        "\n",
        "# a = 1./len(train[train['label']==0])\n",
        "# b = 1./len(train[train['label']==1])\n",
        "# print(a/(a+b) , b/(a+b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29916\n",
            "16028\n",
            "4626\n",
            "2561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqkRCGPetz38",
        "colab_type": "code",
        "outputId": "b084aae3-4647-4417-db29-886cdb95e523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doc_id', 'label', 'sentence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLEWLUG0Fmi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import tokenization\n",
        "from bert import run_classifier\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/uncased_L-12_H-768_A-12', 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "outputId": "1d497b06-6d77-4094-a7da-509af2d8d9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_InputExamples_fixed = test_fixed.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_features_fixed = run_classifier.convert_examples_to_features(test_InputExamples_fixed, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# ## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1202 16:26:31.578330 140711381813120 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOesPBlKTv94",
        "colab_type": "code",
        "outputId": "060fb772-b4dc-4d12-f31e-e744e2b0a961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "import pickle\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "# try:\n",
        "#   train_features = pickle.load(open('train_features_large.dms','rb'))\n",
        "# except Exception as e: \n",
        "#     print('can not load train features, creating train features: %s'%str(e))\n",
        "# #     train_features = pickle.load(tf.gfile.GFile('gs://bert_example/mask_lm/models/V2_augment/doc_leve/last_2/smallBERT-docLevel-seq512/data/train_features_large.dms', \"rb\"))\n",
        "\n",
        "\n",
        "  \n",
        "#     train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#     with open('train_features_large','wb') as f:\n",
        "#       pickle.dump(train_features,f)\n",
        "try:\n",
        "\n",
        "  dev_features = pickle.load(open('dev_features_large.dsm','rb'))\n",
        "except Exception as e:\n",
        "\n",
        "   \n",
        "    print('can not load from GC, creating dev features')\n",
        "    dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    with open('dev_features_large','wb') as f:\n",
        "      pickle.dump(dev_features,f)\n",
        "print(len(train_features))\n",
        "print(len(dev_features))\n",
        "# test_features = run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "# test_fixed_features = run_classifier.convert_examples_to_features(test_InputExamples_fixed, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1202 16:31:35.666312 140711381813120 run_classifier.py:774] Writing example 0 of 7187\n",
            "I1202 16:31:35.684370 140711381813120 run_classifier.py:461] *** Example ***\n",
            "I1202 16:31:35.685642 140711381813120 run_classifier.py:462] guid: None\n",
            "I1202 16:31:35.688972 140711381813120 run_classifier.py:464] tokens: [CLS] tgt wife of 10 years said tgt is dumping [ mask ] after revelations of alleged sexual allegations with numerous women including some of the industry ' s most well - known actresses variety reported . meanwhile t ##m ##z reported that tgt was seen boarding a private jet on tuesday night bound for a rehabilitation center in europe for sex addiction . t ##mx said harvey decided to take the advice of the people around him and leave immediately citing sources and that he will enter a live - in facility that deals both with sex and other behavioral issues . \" he has his moments where there are bursts but for the most part he ' s pretty calm \" t ##m ##z quoted a source . tgt was considered one of the most powerful figures in hollywood but was fired from the company tgt co - founded when the new york times revealed tgt allegedly sexually harassed actresses ashley judd and rose mc ##go ##wan among others . the new yorker magazine followed on tuesday with its own story slamming wei ##nstein with fresh accusations from asia argent ##o mira so ##r ##vino and rosa ##nna ar ##quette . tgt and tgt wei ##nstein company helped usher in a long string of award - winning movies that were also box office sensations like \" sex lies and video ##ta ##pe \" \" the english patient \" \" pulp fiction \" \" the crying game \" \" shakespeare in love \" and \" the king ##a ##¢ ##aa ##s speech \" stated the new yorker . the magazine said that tgt was also a major fundraiser for numerous democratic party candidates including the presidential campaigns of president barack obama and former presidential candidate hillary clinton . tgt had told the new york post earlier that tgt wife was \" 100 percent behind tgt \" but acknowledged the breakup in a statement tuesday . \" a ##¢ ##aa ##i support her decision i am in counseling and perhaps when i am better we can rebuild tgt said in the statement to the post . \" over the last week there has been a lot of pain for my family that i take responsibility for . i sat down with my wife georgina who i love more than anything and we discussed what was best for our family . a ##¢ ##aa [SEP]\n",
            "I1202 16:31:35.691761 140711381813120 run_classifier.py:465] input_ids: 101 1 2564 1997 2184 2086 2056 1 2003 23642 1031 7308 1033 2044 22191 1997 6884 4424 9989 2007 3365 2308 2164 2070 1997 1996 3068 1005 1055 2087 2092 1011 2124 19910 3528 2988 1012 5564 1056 2213 2480 2988 2008 1 2001 2464 9405 1037 2797 6892 2006 9857 2305 5391 2005 1037 11252 2415 1999 2885 2005 3348 13449 1012 1056 22984 2056 7702 2787 2000 2202 1996 6040 1997 1996 2111 2105 2032 1998 2681 3202 8951 4216 1998 2008 2002 2097 4607 1037 2444 1011 1999 4322 2008 9144 2119 2007 3348 1998 2060 14260 3314 1012 1000 2002 2038 2010 5312 2073 2045 2024 19239 2021 2005 1996 2087 2112 2002 1005 1055 3492 5475 1000 1056 2213 2480 9339 1037 3120 1012 1 2001 2641 2028 1997 1996 2087 3928 4481 1999 5365 2021 2001 5045 2013 1996 2194 1 2522 1011 2631 2043 1996 2047 2259 2335 3936 1 9382 12581 28186 19910 9321 20128 1998 3123 11338 3995 7447 2426 2500 1012 1996 2047 19095 2932 2628 2006 9857 2007 2049 2219 2466 15209 11417 15493 2007 4840 13519 2013 4021 23157 2080 18062 2061 2099 26531 1998 9508 9516 12098 29416 1012 1 1998 1 11417 15493 2194 3271 20774 1999 1037 2146 5164 1997 2400 1011 3045 5691 2008 2020 2036 3482 2436 21378 2066 1000 3348 3658 1998 2678 2696 5051 1000 1000 1996 2394 5776 1000 1000 16016 4349 1000 1000 1996 6933 2208 1000 1000 8101 1999 2293 1000 1998 1000 1996 2332 2050 29645 11057 2015 4613 1000 3090 1996 2047 19095 1012 1996 2932 2056 2008 1 2001 2036 1037 2350 28536 2005 3365 3537 2283 5347 2164 1996 4883 8008 1997 2343 13857 8112 1998 2280 4883 4018 18520 7207 1012 1 2018 2409 1996 2047 2259 2695 3041 2008 1 2564 2001 1000 2531 3867 2369 1 1000 2021 8969 1996 19010 1999 1037 4861 9857 1012 1000 1037 29645 11057 2072 2490 2014 3247 1045 2572 1999 17041 1998 3383 2043 1045 2572 2488 2057 2064 14591 1 2056 1999 1996 4861 2000 1996 2695 1012 1000 2058 1996 2197 2733 2045 2038 2042 1037 2843 1997 3255 2005 2026 2155 2008 1045 2202 5368 2005 1012 1045 2938 2091 2007 2026 2564 27358 2040 1045 2293 2062 2084 2505 1998 2057 6936 2054 2001 2190 2005 2256 2155 1012 1037 29645 11057 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.694497 140711381813120 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.697223 140711381813120 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.698724 140711381813120 run_classifier.py:468] label: True (id = 1)\n",
            "I1202 16:31:35.712085 140711381813120 run_classifier.py:461] *** Example ***\n",
            "I1202 16:31:35.713212 140711381813120 run_classifier.py:462] guid: None\n",
            "I1202 16:31:35.716054 140711381813120 run_classifier.py:464] tokens: [CLS] a ##¢ ##aa ##the boots represent the soldiers that are not here with us no more the missing soldiers a ##¢ ##aa tgt said . [ mask ] stood at the front of the group that day with both hands to tgt sides as tgt stared into the distance . the other men lined up horizontally behind him sal ##uting . tgt a 40 - year - old native of mexico told the washington post that tgt served in the army for seven years and spent a few months in iraq . the pentagon has not verified o ##cona ##¢ ##aa ##s service to the post but tgt provided a picture of tgt during a re ##en ##list ##ment ceremony in 2001 . tgt in 2001 . ( courtesy of ivan o ##con ) he said he was deported a year and a half ago losing his legal status after he was charged with a crime for not reporting to authorities that his brother had kidnapped someone . court records show that tgt was indicted on several charges in 2006 including conspiracy to kidnap and aiding and abe ##tting . he pleaded guilty and spent nearly 10 years in federal prison . in 2016 shortly after serving his prison sentence tgt was sent back to mexico a country tgt left when tgt was 7 . tgt has been living in juarez since in an apartment close to the border and can see the united states from tgt window . tgt said tgt considers america his home a ##¢ ##aa specifically las cr ##uce ##s n . m . where he grew up went to high school and left behind a daughter whoa ##¢ ##aa ##s now 14 . for francisco lopez who was standing right behind o ##con in the picture home is wichita falls tex . where his children and grandchildren live . a ##¢ ##aar ##ight here you ##a ##¢ ##aar ##e basically isolated ; you ##a ##¢ ##aar ##e alone a ##¢ ##aa tgt said . a ##¢ ##aa ##all your friends and family you grew up with are all in the states . everybody you know from your childhood are all there . that one mistake is who you are for the rest of your life . you can never prove you ##a ##¢ ##aar ##e a good person . a ##¢ ##aa but lopez who turns 73 this month has been living in juarez for far longer than o ##con . he was deported in 2003 after serving nine years in federal prison for drug charges . his service record could not be immediately verified but lopez said he served for two years in the military in the late 1960s including a year in vietnam . [SEP]\n",
            "I1202 16:31:35.717567 140711381813120 run_classifier.py:465] input_ids: 101 1037 29645 11057 10760 6879 5050 1996 3548 2008 2024 2025 2182 2007 2149 2053 2062 1996 4394 3548 1037 29645 11057 1 2056 1012 1031 7308 1033 2768 2012 1996 2392 1997 1996 2177 2008 2154 2007 2119 2398 2000 1 3903 2004 1 3592 2046 1996 3292 1012 1996 2060 2273 7732 2039 23190 2369 2032 16183 20807 1012 1 1037 2871 1011 2095 1011 2214 3128 1997 3290 2409 1996 2899 2695 2008 1 2366 1999 1996 2390 2005 2698 2086 1998 2985 1037 2261 2706 1999 5712 1012 1996 20864 2038 2025 20119 1051 24366 29645 11057 2015 2326 2000 1996 2695 2021 1 3024 1037 3861 1997 1 2076 1037 2128 2368 9863 3672 5103 1999 2541 1012 1 1999 2541 1012 1006 14571 1997 7332 1051 8663 1007 2002 2056 2002 2001 17929 1037 2095 1998 1037 2431 3283 3974 2010 3423 3570 2044 2002 2001 5338 2007 1037 4126 2005 2025 7316 2000 4614 2008 2010 2567 2018 11364 2619 1012 2457 2636 2265 2008 1 2001 21801 2006 2195 5571 1999 2294 2164 9714 2000 22590 1998 24791 1998 14863 13027 1012 2002 12254 5905 1998 2985 3053 2184 2086 1999 2976 3827 1012 1999 2355 3859 2044 3529 2010 3827 6251 1 2001 2741 2067 2000 3290 1037 2406 1 2187 2043 1 2001 1021 1012 1 2038 2042 2542 1999 25398 2144 1999 2019 4545 2485 2000 1996 3675 1998 2064 2156 1996 2142 2163 2013 1 3332 1012 1 2056 1 10592 2637 2010 2188 1037 29645 11057 4919 5869 13675 18796 2015 1050 1012 1049 1012 2073 2002 3473 2039 2253 2000 2152 2082 1998 2187 2369 1037 2684 23281 29645 11057 2015 2085 2403 1012 2005 3799 8685 2040 2001 3061 2157 2369 1051 8663 1999 1996 3861 2188 2003 18614 4212 16060 1012 2073 2010 2336 1998 13628 2444 1012 1037 29645 26526 18743 2182 2017 2050 29645 26526 2063 10468 7275 1025 2017 2050 29645 26526 2063 2894 1037 29645 11057 1 2056 1012 1037 29645 11057 8095 2115 2814 1998 2155 2017 3473 2039 2007 2024 2035 1999 1996 2163 1012 7955 2017 2113 2013 2115 5593 2024 2035 2045 1012 2008 2028 6707 2003 2040 2017 2024 2005 1996 2717 1997 2115 2166 1012 2017 2064 2196 6011 2017 2050 29645 26526 2063 1037 2204 2711 1012 1037 29645 11057 2021 8685 2040 4332 6421 2023 3204 2038 2042 2542 1999 25398 2005 2521 2936 2084 1051 8663 1012 2002 2001 17929 1999 2494 2044 3529 3157 2086 1999 2976 3827 2005 4319 5571 1012 2010 2326 2501 2071 2025 2022 3202 20119 2021 8685 2056 2002 2366 2005 2048 2086 1999 1996 2510 1999 1996 2397 4120 2164 1037 2095 1999 5148 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.719204 140711381813120 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.720412 140711381813120 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.721379 140711381813120 run_classifier.py:468] label: True (id = 1)\n",
            "I1202 16:31:35.742203 140711381813120 run_classifier.py:461] *** Example ***\n",
            "I1202 16:31:35.743354 140711381813120 run_classifier.py:462] guid: None\n",
            "I1202 16:31:35.745285 140711381813120 run_classifier.py:464] tokens: [CLS] the idea that the knicks would somehow turn tgt ##int ##o blake griffin or perhaps ky ##rie irving this summer was never more than a crazy dream by those who either didn ##at notice or refused to acknowledge that anthony was nowhere near the all - star player who finished third in the 2012 - 13 mvp voting . anthony is still a devastating scorer but physically he is no longer the leading man to carry a team . as if four straight non - playoff years in new york isn ##at enough evidence look at what the knicks received from oklahoma city on saturday in return for their best player : en ##es kant ##er doug mc ##dermott and a second - round pick . knicks sending tgt ##to ok ##c thunder for 2 players pick the knicks had just three options once carmel ##o expanded his list to three teams 10 days ago . it was be going to be cleveland houston or oklahoma city . that ##as it . without much to work with the mills - perry team had to do something to avoid training camp being hi ##jack ##ed by anthony ##as uncertain status . tgt could never best buddy le ##bron but now heads west to challenge the mighty warriors . ( brad penn ##er / usa today sports ) once tgt agreed in june that it was time to leave the knicks they knew they couldn ##at proceed with him . they were close to a deal with houston before and immediately after jackson was fired but ultimately mills decided to pump the brakes while going through the process of hiring a gm . for nearly two months the knicks and anthony were playing a game of chicken until anthony blinked first and said he would wai ##ve his no - trade for the ca ##vs and thunder as well . 8 defining moments of carmel ##o anthony ' s career with the knicks both sides were wise to wait and in the end this works out beautifully for anthony . he gets two superstar teammates in russell [ mask ] and paul george and joins an organization that is regarded as first class in every way . even in the mighty western conference the thunder will be a playoff team but good luck trying to up ##end the golden state warriors . carmel ##o anthony can still score but isn ' t the kind of player that can carry a team . ( l . e . miller / new york daily news ) for example ok ##c was scouting anthony as far back as january . thunder assistant gm troy weaver recruited anthony to syracuse and the two have remained close . pre ##sti thought that relationship could help . he was right . that ##as called due dil ##igen ##ce . assuming [ mask ] re - signs anthony won ##at have the pressure of needing to score 25 points a night to give his team a chance to win [SEP]\n",
            "I1202 16:31:35.746650 140711381813120 run_classifier.py:465] input_ids: 101 1996 2801 2008 1996 27817 2052 5064 2735 1 18447 2080 6511 9258 2030 3383 18712 7373 12415 2023 2621 2001 2196 2062 2084 1037 4689 3959 2011 2216 2040 2593 2134 4017 5060 2030 4188 2000 13399 2008 4938 2001 7880 2379 1996 2035 1011 2732 2447 2040 2736 2353 1999 1996 2262 1011 2410 12041 6830 1012 4938 2003 2145 1037 14886 10835 2021 8186 2002 2003 2053 2936 1996 2877 2158 2000 4287 1037 2136 1012 2004 2065 2176 3442 2512 1011 7808 2086 1999 2047 2259 3475 4017 2438 3350 2298 2012 2054 1996 27817 2363 2013 5858 2103 2006 5095 1999 2709 2005 2037 2190 2447 1024 4372 2229 26044 2121 8788 11338 29370 1998 1037 2117 1011 2461 4060 1012 27817 6016 1 3406 7929 2278 8505 2005 1016 2867 4060 1996 27817 2018 2074 2093 7047 2320 19443 2080 4423 2010 2862 2000 2093 2780 2184 2420 3283 1012 2009 2001 2022 2183 2000 2022 6044 5395 2030 5858 2103 1012 2008 3022 2009 1012 2302 2172 2000 2147 2007 1996 6341 1011 6890 2136 2018 2000 2079 2242 2000 4468 2731 3409 2108 7632 17364 2098 2011 4938 3022 9662 3570 1012 1 2071 2196 2190 8937 3393 21337 2021 2085 4641 2225 2000 4119 1996 10478 6424 1012 1006 8226 9502 2121 1013 3915 2651 2998 1007 2320 1 3530 1999 2238 2008 2009 2001 2051 2000 2681 1996 27817 2027 2354 2027 2481 4017 10838 2007 2032 1012 2027 2020 2485 2000 1037 3066 2007 5395 2077 1998 3202 2044 4027 2001 5045 2021 4821 6341 2787 2000 10216 1996 13627 2096 2183 2083 1996 2832 1997 14763 1037 13938 1012 2005 3053 2048 2706 1996 27817 1998 4938 2020 2652 1037 2208 1997 7975 2127 4938 7948 2034 1998 2056 2002 2052 23701 3726 2010 2053 1011 3119 2005 1996 6187 15088 1998 8505 2004 2092 1012 1022 12854 5312 1997 19443 2080 4938 1005 1055 2476 2007 1996 27817 2119 3903 2020 7968 2000 3524 1998 1999 1996 2203 2023 2573 2041 17950 2005 4938 1012 2002 4152 2048 18795 13220 1999 5735 1031 7308 1033 1998 2703 2577 1998 9794 2019 3029 2008 2003 5240 2004 2034 2465 1999 2296 2126 1012 2130 1999 1996 10478 2530 3034 1996 8505 2097 2022 1037 7808 2136 2021 2204 6735 2667 2000 2039 10497 1996 3585 2110 6424 1012 19443 2080 4938 2064 2145 3556 2021 3475 1005 1056 1996 2785 1997 2447 2008 2064 4287 1037 2136 1012 1006 1048 1012 1041 1012 4679 1013 2047 2259 3679 2739 1007 2005 2742 7929 2278 2001 14299 4938 2004 2521 2067 2004 2254 1012 8505 3353 13938 9553 14077 8733 4938 2000 11736 1998 1996 2048 2031 2815 2485 1012 3653 16643 2245 2008 3276 2071 2393 1012 2002 2001 2157 1012 2008 3022 2170 2349 29454 29206 3401 1012 10262 1031 7308 1033 2128 1011 5751 4938 2180 4017 2031 1996 3778 1997 11303 2000 3556 2423 2685 1037 2305 2000 2507 2010 2136 1037 3382 2000 2663 102\n",
            "I1202 16:31:35.747945 140711381813120 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I1202 16:31:35.749075 140711381813120 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.750164 140711381813120 run_classifier.py:468] label: False (id = 0)\n",
            "I1202 16:31:35.765137 140711381813120 run_classifier.py:461] *** Example ***\n",
            "I1202 16:31:35.766652 140711381813120 run_classifier.py:462] guid: None\n",
            "I1202 16:31:35.770649 140711381813120 run_classifier.py:464] tokens: [CLS] close sports ##pu ##ls ##e : drama . tgt . comeback ##s . try ##sta k ##rick details the key moments in the patriots ' run to super bowl li ##i . usa today sports patriots qb tgt ( photo : the associated press ) tgt abruptly ended tgt weekly appearance on a boston radio station monday morning after one of the station ' s hosts made a di ##spar ##aging comment last week about tgt ' s 5 - year - old daughter . tgt told the hosts of the morning show [ mask ] & callahan that he was disappointed to hear the comments made by another host last week and is rec ##ons ##ider ##ing whether he will appear on the station wee ##i 93 . 7 fm in the future . last week host alex rei ##mer called brady ' s daughter vivian an \" annoying little piss ##ant \" while discussing the quarterback ' s documentary series tom vs . time . \" i ' ve tried to come on this show for many years and showed you guys a lot of respect tgt said monday morning according to the station . \" i ' ve always tried to come on and do a good job for you guys . it ' s very disappointing when you hear that certainly . my daughter or any child certainly doesn ' t deserve that . \" tom brady on @ alex ##re ##ime ##r ##1 ' s comments : \" it was very disappointing to hear that my daughter or any child certainly does not deserve that . \" tom then concluded the interview . a ##¢ ##aa [ mask ] & callahan ( @ [ mask ] and ##cal ##lah ##an ) january 29 2018 tgt and coach bill bel ##ichi ##ck have made weekly appearances on wee ##i this season and the station announced wednesday that it had agreed to a \" multi - year partnership extension \" with the patriots to ensure the continuation of those appearances . tgt and the patriots will face the philadelphia eagles in the super bowl on sunday . more : tgt bill bel ##ichi ##ck facing last hu ##rra ##h in super bowl vs . philadelphia ? photos : tgt through the years [SEP]\n",
            "I1202 16:31:35.772414 140711381813120 run_classifier.py:465] input_ids: 101 2485 2998 14289 4877 2063 1024 3689 1012 1 1012 12845 2015 1012 3046 9153 1047 11285 4751 1996 3145 5312 1999 1996 11579 1005 2448 2000 3565 4605 5622 2072 1012 3915 2651 2998 11579 26171 1 1006 6302 1024 1996 3378 2811 1007 1 9225 3092 1 4882 3311 2006 1037 3731 2557 2276 6928 2851 2044 2028 1997 1996 2276 1005 1055 6184 2081 1037 4487 27694 16594 7615 2197 2733 2055 1 1005 1055 1019 1011 2095 1011 2214 2684 1012 1 2409 1996 6184 1997 1996 2851 2265 1031 7308 1033 1004 25668 2008 2002 2001 9364 2000 2963 1996 7928 2081 2011 2178 3677 2197 2733 1998 2003 28667 5644 18688 2075 3251 2002 2097 3711 2006 1996 2276 16776 2072 6109 1012 1021 4718 1999 1996 2925 1012 2197 2733 3677 4074 24964 5017 2170 10184 1005 1055 2684 13801 2019 1000 15703 2210 18138 4630 1000 2096 10537 1996 9074 1005 1055 4516 2186 3419 5443 1012 2051 1012 1000 1045 1005 2310 2699 2000 2272 2006 2023 2265 2005 2116 2086 1998 3662 2017 4364 1037 2843 1997 4847 1 2056 6928 2851 2429 2000 1996 2276 1012 1000 1045 1005 2310 2467 2699 2000 2272 2006 1998 2079 1037 2204 3105 2005 2017 4364 1012 2009 1005 1055 2200 15640 2043 2017 2963 2008 5121 1012 2026 2684 2030 2151 2775 5121 2987 1005 1056 10107 2008 1012 1000 3419 10184 2006 1030 4074 2890 14428 2099 2487 1005 1055 7928 1024 1000 2009 2001 2200 15640 2000 2963 2008 2026 2684 2030 2151 2775 5121 2515 2025 10107 2008 1012 1000 3419 2059 5531 1996 4357 1012 1037 29645 11057 1031 7308 1033 1004 25668 1006 1030 1031 7308 1033 1998 9289 14431 2319 1007 2254 2756 2760 1 1998 2873 3021 19337 11319 3600 2031 2081 4882 3922 2006 16776 2072 2023 2161 1998 1996 2276 2623 9317 2008 2009 2018 3530 2000 1037 1000 4800 1011 2095 5386 5331 1000 2007 1996 11579 2000 5676 1996 13633 1997 2216 3922 1012 1 1998 1996 11579 2097 2227 1996 4407 8125 1999 1996 3565 4605 2006 4465 1012 2062 1024 1 3021 19337 11319 3600 5307 2197 15876 11335 2232 1999 3565 4605 5443 1012 4407 1029 7760 1024 1 2083 1996 2086 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.773844 140711381813120 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.774998 140711381813120 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.776223 140711381813120 run_classifier.py:468] label: False (id = 0)\n",
            "I1202 16:31:35.793934 140711381813120 run_classifier.py:461] *** Example ***\n",
            "I1202 16:31:35.795725 140711381813120 run_classifier.py:462] guid: None\n",
            "I1202 16:31:35.797997 140711381813120 run_classifier.py:464] tokens: [CLS] throw ##down ' s 7 ' 1 \" center tgt is in my cross ##hair ##s . tgt has extensive competitive 2 ##k gaming experience dating back to tgt team ' s thrilling run in the 2 ##k all - star tournament . i ' ve watched a number of tgt team ' s 2 ##k ##18 games and focused a great deal on what appears to be tgt strengths and weaknesses . here is a reminder : just because he ' s the first big i ' ve spotlight ##ed doesn ' t mean he ' s the best big - man prospect i ' ve seen . at any rate here is the assessment . big men have a primary responsibility to protect the paint and few i ' ve seen thus far do it as well as mo ##ot ##y ##y . he takes pride in rejecting opponents and trying to limit them to one shot . he has a high 2 ##k pro - am and hoop ##s iq which serves as a major asset to him and his team . i heard him say during one of his teammate ' s streams \" you guys know i don ' t care about stats . i want to win . \" some people say that and still jack up ill - advised shots on one or two contest ##ing defenders . from what i saw that ' s not mo ##ot ##y ##y . he ' s a willing pass ##er but also doesn ' t hesitate to take the shots he should when the ball swings his way . his big - time corner three - point shot in the all - star tournament is proof tgt can and has stepped up with the pressure on and the controller in [ mask ] hands . like many nba 2 ##k league hopeful ##s tgt can be a little sharp with tgt tongue . we don ' t know how the nba 2 ##k league broadcasts will be set up and how much of the player ' s audio will be accessible but tgt and others may have to clean up their language when things go live . the great thing about tgt is that tgt understands tgt appears capable of flipping the professional switch . tgt does a pretty good job of using tgt twitter and other social ##s as a platform to further market tgt . tgt would be a great pick for a team with proven ball handler ##s shot creators and / or knock - down shooters . tgt ability to anchor a defense could set a team up for success on that end of the floor . on offense tgt willingness to sacrifice shots for the better ##ment of the team is only augmented by tgt ability to step up and make them when tgt number is called . [SEP]\n",
            "I1202 16:31:35.799829 140711381813120 run_classifier.py:465] input_ids: 101 5466 7698 1005 1055 1021 1005 1015 1000 2415 1 2003 1999 2026 2892 26227 2015 1012 1 2038 4866 6975 1016 2243 10355 3325 5306 2067 2000 1 2136 1005 1055 26162 2448 1999 1996 1016 2243 2035 1011 2732 2977 1012 1045 1005 2310 3427 1037 2193 1997 1 2136 1005 1055 1016 2243 15136 2399 1998 4208 1037 2307 3066 2006 2054 3544 2000 2022 1 20828 1998 21775 1012 2182 2003 1037 14764 1024 2074 2138 2002 1005 1055 1996 2034 2502 1045 1005 2310 17763 2098 2987 1005 1056 2812 2002 1005 1055 1996 2190 2502 1011 2158 9824 1045 1005 2310 2464 1012 2012 2151 3446 2182 2003 1996 7667 1012 2502 2273 2031 1037 3078 5368 2000 4047 1996 6773 1998 2261 1045 1005 2310 2464 2947 2521 2079 2009 2004 2092 2004 9587 4140 2100 2100 1012 2002 3138 6620 1999 21936 7892 1998 2667 2000 5787 2068 2000 2028 2915 1012 2002 2038 1037 2152 1016 2243 4013 1011 2572 1998 27669 2015 26264 2029 4240 2004 1037 2350 11412 2000 2032 1998 2010 2136 1012 1045 2657 2032 2360 2076 2028 1997 2010 10809 1005 1055 9199 1000 2017 4364 2113 1045 2123 1005 1056 2729 2055 26319 1012 1045 2215 2000 2663 1012 1000 2070 2111 2360 2008 1998 2145 2990 2039 5665 1011 9449 7171 2006 2028 2030 2048 5049 2075 12534 1012 2013 2054 1045 2387 2008 1005 1055 2025 9587 4140 2100 2100 1012 2002 1005 1055 1037 5627 3413 2121 2021 2036 2987 1005 1056 16390 2000 2202 1996 7171 2002 2323 2043 1996 3608 18755 2010 2126 1012 2010 2502 1011 2051 3420 2093 1011 2391 2915 1999 1996 2035 1011 2732 2977 2003 6947 1 2064 1998 2038 3706 2039 2007 1996 3778 2006 1998 1996 11486 1999 1031 7308 1033 2398 1012 2066 2116 6452 1016 2243 2223 17772 2015 1 2064 2022 1037 2210 4629 2007 1 4416 1012 2057 2123 1005 1056 2113 2129 1996 6452 1016 2243 2223 8960 2097 2022 2275 2039 1998 2129 2172 1997 1996 2447 1005 1055 5746 2097 2022 7801 2021 1 1998 2500 2089 2031 2000 4550 2039 2037 2653 2043 2477 2175 2444 1012 1996 2307 2518 2055 1 2003 2008 1 19821 1 3544 5214 1997 18497 1996 2658 6942 1012 1 2515 1037 3492 2204 3105 1997 2478 1 10474 1998 2060 2591 2015 2004 1037 4132 2000 2582 3006 1 1012 1 2052 2022 1037 2307 4060 2005 1037 2136 2007 10003 3608 28213 2015 2915 17277 1998 1013 2030 7324 1011 2091 28310 1012 1 3754 2000 8133 1037 3639 2071 2275 1037 2136 2039 2005 3112 2006 2008 2203 1997 1996 2723 1012 2006 10048 1 19732 2000 8688 7171 2005 1996 2488 3672 1997 1996 2136 2003 2069 19335 2011 1 3754 2000 3357 2039 1998 2191 2068 2043 1 2193 2003 2170 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.801350 140711381813120 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.802975 140711381813120 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1202 16:31:35.804203 140711381813120 run_classifier.py:468] label: True (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can not load from GC, creating dev features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1202 16:32:54.122934 140711381813120 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c26d636c06d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev_features_large'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# test_features = run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnRhr2sAzrYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = train['label']\n",
        "test_label = test['label']\n",
        "test_fixed_label = test_fixed['label']\n",
        "dev_label = dev['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6TrW6rjlZZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.head(500).to_csv(open('train_500.csv','w'),encoding='latin-1')\n",
        "data_train = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2x0zox7vWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_selective -f '\\bdata_test_fixed\\b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz3_JA2X6eAc",
        "colab_type": "code",
        "outputId": "ddf77932-9825-433d-debc-74d5dd72658e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# reset_selective -f data_train\n",
        "# who_ls\n",
        "import sys\n",
        "ipython_vars =[]# ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
        "\n",
        "\n",
        "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('train', 753257168),\n",
              " ('test_fixed', 146768632),\n",
              " ('data_test', 117534962),\n",
              " ('test', 117469922),\n",
              " ('data_dev', 70042071),\n",
              " ('dev', 69984575),\n",
              " ('train_InputExamples', 1837864),\n",
              " ('test_InputExamples_fixed', 504264),\n",
              " ('train_features', 406504),\n",
              " ('test_InputExamples', 325304),\n",
              " ('dev_InputExamples', 287584),\n",
              " ('lines', 253640),\n",
              " ('test_features_fixed', 111008),\n",
              " ('test_fixed_features', 111008),\n",
              " ('test_features', 69168),\n",
              " ('dev_features', 61440),\n",
              " ('train_label', 46048),\n",
              " ('test_fixed_label', 12708),\n",
              " ('test_label', 8234),\n",
              " ('dev_label', 7291),\n",
              " ('auth_info', 1048),\n",
              " ('AdamWeightDecayOptimizer', 904),\n",
              " ('Out', 280),\n",
              " ('In', 200),\n",
              " ('f', 144),\n",
              " ('data_pref', 132),\n",
              " ('create_optimizer', 120),\n",
              " ('load_dataset', 120),\n",
              " ('load_directory_data', 120),\n",
              " ('CONFIG_FILE', 109),\n",
              " ('INIT_CHECKPOINT', 108),\n",
              " ('OUTPUT_DIR', 97),\n",
              " ('VOCAB_FILE', 97),\n",
              " ('path', 97),\n",
              " ('e', 96),\n",
              " ('BERT_MODEL_HUB', 92),\n",
              " ('BERT_PRETRAINED_DIR', 92),\n",
              " ('label_list', 88),\n",
              " ('get_ipython', 80),\n",
              " ('absolute_import', 72),\n",
              " ('division', 72),\n",
              " ('ipython_vars', 72),\n",
              " ('print_function', 72),\n",
              " ('exit', 64),\n",
              " ('f_in', 64),\n",
              " ('f_out', 64),\n",
              " ('quit', 64),\n",
              " ('session', 64),\n",
              " ('tf', 64),\n",
              " ('tokenizer', 64),\n",
              " ('tpu_cluster_resolver', 64),\n",
              " ('BERT_MODEL', 60),\n",
              " ('TPU_ADDRESS', 60),\n",
              " ('auth', 56),\n",
              " ('hub', 56),\n",
              " ('modeling', 56),\n",
              " ('pd', 56),\n",
              " ('run_classifier', 56),\n",
              " ('run_classifier_with_tfhub', 56),\n",
              " ('tokenization', 56),\n",
              " ('BUCKET', 49),\n",
              " ('DATA_COLUMN', 45),\n",
              " ('line', 43),\n",
              " ('LABEL_COLUMN', 42),\n",
              " ('data_train', 37),\n",
              " ('DO_LOWER_CASE', 24),\n",
              " ('EVAL_BATCH_SIZE', 24),\n",
              " ('ITERATIONS_PER_LOOP', 24),\n",
              " ('LEARNING_RATE', 24),\n",
              " ('MAX_SEQ_LENGTH', 24),\n",
              " ('NUM_TPU_CORES', 24),\n",
              " ('NUM_TRAIN_EPOCHS', 24),\n",
              " ('PREDICT_BATCH_SIZE', 24),\n",
              " ('SAVE_CHECKPOINTS_STEPS', 24),\n",
              " ('SAVE_SUMMARY_STEPS', 24),\n",
              " ('TRAIN_BATCH_SIZE', 24),\n",
              " ('WARMUP_PROPORTION', 24),\n",
              " ('num_train_steps', 24),\n",
              " ('num_warmup_steps', 24),\n",
              " ('use_tpu', 24)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Sj7di-oaUB",
        "colab_type": "code",
        "outputId": "1bd67947-2900-44b6-8bb8-d0b09cbef002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEcWYt6jjaf",
        "colab_type": "code",
        "outputId": "7224dba4-bec3-46e0-e9fb-b8136519884e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_w = 1./len(dev[dev['label']==0])\n",
        "pos_w = 1./len(dev[dev['label']==1])\n",
        "\n",
        "class_weights_arr = [neg_w/(neg_w+pos_w),pos_w/(neg_w+pos_w)]\n",
        "# class_weights_arr = [0.6,0.4]\n",
        "trainable = True\n",
        "print(class_weights_arr)\n",
        "# class_weights_arr = [0.5,0.5]\n",
        "def create_model(bert_config,is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels,use_one_hot_embeddings):#, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "    \n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB, tags=tags, trainable=trainable)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "###### TO DO add weights\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "    print(one_hot_labels.get_shape())\n",
        "        \n",
        "    one_hot_labels = one_hot_labels * class_weights_arr\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "       \n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels,init_checkpoint, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu,use_one_hot_embeddings):# bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) =create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,use_one_hot_embeddings)\n",
        "\n",
        "    \n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "     \n",
        "      train_op = create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,scaffold_fn=scaffold_fn)\n",
        "#       output_spec = tf.Print(output_spec,[tf_loss,total_loss],message='tfloss, total loss')\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predictions,weights=is_real_example)\n",
        "        loss = tf.metrics.mean(per_example_loss,weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics,scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities},scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6436621678029776, 0.35633783219702236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "outputId": "8d874c0c-5af2-4aff-8b90-32187a1e064e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "## These two lines should be activated if ** is not activated\n",
        "num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 100 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "### Activate it if ** part is not activated \n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True\n",
        "#   bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=get_run_config(OUTPUT_DIR),\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE, \n",
        "# )\n",
        "# #####################################################################\n",
        "## No Error\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    keep_checkpoint_max=15,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "# model_fn = run_classifier.model_fn_builder(\n",
        "#     bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#     num_labels=len(label_list),\n",
        "#     init_checkpoint=INIT_CHECKPOINT,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     num_train_steps=num_train_steps,\n",
        "#     num_warmup_steps=num_warmup_steps,\n",
        "#     use_tpu=use_tpu,\n",
        "#     use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "\n",
        "# estimator_from_tfhub._export_to_tpu = False\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1202 16:36:07.142973 140711381813120 estimator.py:1994] Estimator's model_fn (<function model_fn at 0x7ff97ef66de8>) includes params argument, but params are not passed to Estimator.\n",
            "I1202 16:36:07.145716 140711381813120 estimator.py:212] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.42.39.162:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 15, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff97ef76090>, '_model_dir': 'gs://bert_example/aug_19_models/mask_lm/weighted/last3layers', '_protocol': None, '_save_checkpoints_steps': 50, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=2871, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7ff981b4ce50>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.42.39.162:8470', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': u'grpc://10.42.39.162:8470'}\n",
            "I1202 16:36:07.149149 140711381813120 tpu_context.py:220] _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "931f62c6-0b6a-4503-9869-3815c3985020"
      },
      "source": [
        "# Train the model\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "def model_train(estimator,train_features=train_features):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_features)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  print('start running estimator')\n",
        "#   estimator._export_to_tpu = False\n",
        "  md = estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "  return md\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a816a99c2a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# We'll set sequences to be at most 128 tokens long.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'***** Started training at {} *****'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Num examples = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator,eval_examples=None,eval_features=dev_features):\n",
        "  # Eval the model.\n",
        "#   eval_examples = dev_InputExamples#processor.get_dev_examples(TASK_DATA_DIR)\n",
        "#   eval_features = run_classifier.convert_examples_to_features(\n",
        "#       eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "#   with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "  print(\"***** Eval results *****\")\n",
        "  for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "#       writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "      \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# labels = [1,0]\n",
        "# def model_predict(estimator,prediction_examples):\n",
        "#   # Make predictions on a subset of eval examples\n",
        "# #   prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "#   input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#   predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "#   predictions = estimator.predict(predict_input_fn)\n",
        "#   return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n",
        "def model_predict(estimator,input_features,input_examples,checkpoint_path=None):\n",
        "  # Make predictions on a subset of eval examples\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmU7E5v1-k0",
        "colab_type": "code",
        "outputId": "02e449cd-7563-460f-86f2-6057f55aa1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "model_train(estimator,train_features=train_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started training at 2019-11-20 00:45:54.601040 *****\n",
            "  Num examples = 45944\n",
            "  Batch size = 16\n",
            "start running estimator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 00:47:06.452250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.455482 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.457176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.458959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.467869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.479541 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.488974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.496045 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.499067 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.523791 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.527118 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.534768 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.538186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.545877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.549019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.568387 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.571590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.581841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.585083 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.595128 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.599123 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.608743 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.611717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.623051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.626230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.639640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.643160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.650221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.654248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.661731 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.664555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.682265 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.685348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.695935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.699109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.708868 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.712785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.725483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.728724 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.742732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.746524 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.759268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.762723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.771559 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.774894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.783488 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.786988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.805985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.810400 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.820926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.824990 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.835321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.839857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.850804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.854145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.865312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.869234 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.881721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.885682 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.893836 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.898396 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.905759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.910543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.929325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.934499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.944822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.952294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.964709 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.971427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.984261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.987329 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:06.997327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.000361 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.014003 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.019433 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.027420 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.030415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.039308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.044294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.066303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.072509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.084403 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.088108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.100001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.106352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.115931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.118985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.129012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.132602 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.144649 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.150039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.157325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.160701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.168260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.171353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.190819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.194103 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.204610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.208528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.218489 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.222845 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.234654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.237935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.248831 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.252840 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.265762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.270019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.279839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.284789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.292675 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.297959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.316926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.322321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.332916 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.338063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.347532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.352493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.362513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.365628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.376537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.379569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.391937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.396004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.403213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.406794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.415011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.420222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.437892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.443226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.454775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.458633 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.468672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.473350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.483948 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.489160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.500077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.504893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.515845 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.520277 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.527190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.530498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.538841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.541732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.560194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.566298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.575567 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.581056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.590034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.594640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.604532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.609858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.620807 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.623712 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.637423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.642066 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.649070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.651973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.659409 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.662235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.680149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.685887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.695431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.698368 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.709012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.712786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.723077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.726491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.737307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.741360 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.753245 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.756434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.763151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.766386 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.773128 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.776937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.799168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.802826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.814016 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.818070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.828474 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.832029 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.841564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.847275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.857438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.861495 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.874052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.877383 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.885274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.888611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.897595 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.902851 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.922281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.926961 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.935964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.940334 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.948817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.952838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.964879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.968794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.980025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:07.983618 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.013928 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.018141 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.030590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.034264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.043570 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.047849 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 00:47:08.057929 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2, 2)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Finished training at 2019-11-20 01:16:37.241792 *****\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7f45841ac750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYlSED2fBRC",
        "colab_type": "code",
        "outputId": "3e327972-aaa3-454d-9b2b-4bfe3bd2a783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(2871,28711,2871):\n",
        "  print('evaluating epoch: %d'%(i/2871))\n",
        "  pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%(i))\n",
        "  true_label = list(dev['label'])\n",
        "  labels_val = []\n",
        "  for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:47:28.787249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.790396 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.792452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.794035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.802660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.817955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.828022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.834934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.838350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.859389 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.863677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.870687 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.873877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.880351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.883647 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.898747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.902481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.907816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.911048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.923171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.928765 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.937701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.943145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.950933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.956157 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.969923 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.978048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.985508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.989025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:28.997312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.004142 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.019869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.024197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.032008 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.036660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.046578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.051168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.062581 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.066585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.072592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.076169 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.090017 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.093298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.100430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.105372 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.111139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.114346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.128572 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.132019 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.138322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.141443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.150531 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.157283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.166249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.170824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.177256 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.180814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.193212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.196654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.204540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.208347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.215353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.224827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.242418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.245723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.253703 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.257051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.268870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.273817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.284069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.287444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.294183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.297977 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.310322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.316067 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.324261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.327534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.335838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.340183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.355703 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.361763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.368443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.372056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.383213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.389069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.399044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.402662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.409616 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.412920 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.426013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.429620 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.437378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.440814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.448359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.451776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.468242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.475833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.483829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.487832 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.502295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.507153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.518322 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.522069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.529012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.533859 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.546773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.550590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.559021 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.562767 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.570251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.574206 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.594854 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.598810 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.606642 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.610394 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.620537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.624944 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.635377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.638952 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.645781 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.650337 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.661539 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.664653 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.673044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.678447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.685918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.689810 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.712374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.716037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.724725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.728424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.740662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.745049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.754981 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.758418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.765229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.768821 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.780751 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.784037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.792352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.796909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.805011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.810626 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.826512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.831137 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.838324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.844048 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.854213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.859766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.870229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.874914 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.880867 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.887382 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.900549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.905014 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.911880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.917714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.923114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.926564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.941056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.944214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.950956 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.954425 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.965111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.969530 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.979716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.983645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.990230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:29.994235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.006059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.011517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.018224 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.023145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.029627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.034590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.052274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.055679 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.062586 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.066078 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.075015 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.079499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.089519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.092791 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.099193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.104516 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.116210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.120918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.129050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.133644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.140139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.143578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.160268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.164052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.172250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.177248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.186944 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.191446 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.200984 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.204220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.209817 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.215373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.245521 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.248842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.262046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.266490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.275964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.279378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:47:30.290585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[1845  716]\n",
            " [ 198 4428]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.90      0.72      0.80      2561\n",
            "        True       0.86      0.96      0.91      4626\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      7187\n",
            "   macro avg       0.88      0.84      0.85      7187\n",
            "weighted avg       0.88      0.87      0.87      7187\n",
            "\n",
            "evaluating epoch: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:48:19.019186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.021737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.023825 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.026325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.035711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.049695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.059983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.067075 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.070759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.090147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.093648 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.101763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.105839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.112973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.116389 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.133280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.136547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.143063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.146508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.154819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.158998 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.169044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.172378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.178148 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.181437 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.192035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.195255 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.201947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.205233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.210951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.214437 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.234663 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.239279 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.245198 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.250842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.259402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.264053 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.274190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.277753 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.287298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.293478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.306188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.309705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.319638 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.328664 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.336915 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.344824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.360824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.364444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.373541 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.377573 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.387904 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.394697 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.406285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.410387 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.417963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.422152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.435156 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.439065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.447762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.452063 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.458621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.463650 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.479578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.483344 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.489901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.493424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.503652 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.508755 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.518580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.523046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.527992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.532133 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.543536 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.547434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.555197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.558837 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.566355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.570667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.585453 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.589138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.599081 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.604537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.615053 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.621597 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.631779 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.638858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.645999 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.650038 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.663073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.667880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.675574 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.679724 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.686757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.690813 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.707283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.710967 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.716655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.721307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.733099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.739129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.749313 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.755878 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.762757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.768285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.779367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.784713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.793138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.797138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.803785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.806988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.822051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.825109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.830507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.834974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.843622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.848306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.857130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.860238 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.866050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.869611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.881197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.884351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.891777 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.896505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.902630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.905801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.921888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.925062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.930486 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.934881 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.944094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.948190 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.958085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.961297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.967756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.973305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.984427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.988593 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:19.995927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.001976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.009650 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.012808 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.028059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.031997 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.037534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.040971 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.051068 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.054933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.064385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.068676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.075407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.078504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.089684 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.092811 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.100311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.104012 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.110090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.113193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.128366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.132162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.138659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.142000 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.151134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.154987 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.166858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.171168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.176630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.180172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.194931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.199290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.206485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.210180 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.216193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.219429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.235737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.239090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.244478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.249212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.259183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.262407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.273452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.277082 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.283431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.286508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.298690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.303507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.310795 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.318521 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.324934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.328974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.344392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.347785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.354927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.358776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.369050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.372654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.383884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.387111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.392720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.397152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.433366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.439994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.453010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.456310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.466039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.469822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:48:20.479497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452ee5d960> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1619  942]\n",
            " [ 192 4434]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.63      0.74      2561\n",
            "        True       0.82      0.96      0.89      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.86      0.80      0.81      7187\n",
            "weighted avg       0.85      0.84      0.83      7187\n",
            "\n",
            "evaluating epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:49:10.288819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.291101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.293690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.295897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.305232 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.317280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.327111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.335154 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.338538 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.359611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.363595 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.371049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.374728 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.381145 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.384030 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.397344 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.400250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.405236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.408226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.416923 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.420490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.429230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.432130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.437271 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.440218 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.450609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.453484 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.459960 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.462877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.469172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.471961 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.485008 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.487906 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.494828 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.498310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.506942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.510481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.519994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.523001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.528630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.531614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.543152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.546050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.552428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.555193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.563950 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.568547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.582684 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.586478 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.592421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.595189 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.604609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.610555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.620152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.623434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.628375 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.632694 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.643954 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.646781 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.654098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.658333 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.665580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.668515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.683809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.687170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.693672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.698785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.708551 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.713167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.723421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.728051 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.735033 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.737942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.749864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.754869 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.762689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.766381 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.772824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.775599 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.789901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.794784 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.800327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.803375 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.812917 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.816370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.825207 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.827970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.834414 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.837152 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.848747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.851624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.859226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.862114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.869082 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.872716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.887080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.891129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.896895 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.901405 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.911592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.916026 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.926103 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.929708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.936023 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.940370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.952945 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.956562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.964184 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.967765 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.974666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.978761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.993254 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:10.996491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.004419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.007504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.017303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.022105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.036252 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.041697 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.048439 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.051803 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.064306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.067287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.076292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.079144 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.086350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.090295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.105643 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.110217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.116611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.119390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.130094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.135270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.148336 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.151623 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.160240 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.163429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.177789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.182334 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.190639 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.194880 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.202603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.207355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.226963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.233158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.241102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.245090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.254965 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.258933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.270185 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.273964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.281660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.285907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.300740 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.306523 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.315099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.319195 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.326491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.329711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.346421 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.350222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.358479 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.363100 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.379878 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.385600 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.402430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.408004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.415761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.419074 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.432256 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.436794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.445007 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.448662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.456299 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.459202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.480307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.484108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.490310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.494016 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.507842 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.511594 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.523654 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.526576 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.531976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.536294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.549062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.552968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.561696 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.565031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.572158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.575789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.590436 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.593566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.600035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.602996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.613980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.620434 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.632200 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.636723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.643409 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.647084 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.679289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.682908 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.697369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.701884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.713614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.717010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:49:11.726483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f45240b2050> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1780  781]\n",
            " [ 341 4285]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.84      0.70      0.76      2561\n",
            "        True       0.85      0.93      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.84      0.81      0.82      7187\n",
            "weighted avg       0.84      0.84      0.84      7187\n",
            "\n",
            "evaluating epoch: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:50:04.071634 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.073693 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.079722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.081476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.090557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.103039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.112876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.120563 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.125227 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.144861 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.148040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.155658 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.158662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.166938 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.170159 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.186652 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.191235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.197262 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.200197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.210074 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.213879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.224395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.227689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.233896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.237493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.249689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.254393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.261898 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.265741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.275010 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.278862 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.295241 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.298330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.304287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.307432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.317178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.321820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.331443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.334397 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.340826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.343837 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.359877 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.364274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.372205 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.375513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.383279 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.386173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.402271 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.405432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.412014 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.415498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.425714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.430263 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.444659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.450686 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.458220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.462613 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.475615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.478902 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.486670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.489959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.497395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.500762 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.518076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.522243 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.529545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.535866 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.547017 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.552035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.562448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.565892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.574139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.577363 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.588422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.594589 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.604772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.608577 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.614749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.618297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.633310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.636121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.642447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.646543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.656308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.660938 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.670384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.673151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.679150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.682771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.696526 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.701874 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.708183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.711009 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.717318 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.720978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.735296 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.738900 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.744414 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.748285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.758233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.761996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.776230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.780996 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.787163 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.792357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.806324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.809815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.817034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.820760 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.828972 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.833123 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.848031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.851197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.858335 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.862107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.872060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.876497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.888897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.892235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.897300 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.902657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.915354 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.918554 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.925734 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.928858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.935359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.940870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.956042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.959491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.966274 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.969870 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.980530 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.985557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.995630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:04.999104 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.007179 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.011423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.025283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.028352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.036412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.041073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.048763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.053138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.070640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.074590 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.080229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.084291 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.094424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.098304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.109292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.112306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.119393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.122551 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.135013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.138431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.146024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.149561 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.158838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.162303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.177560 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.180557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.187210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.190629 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.201090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.204941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.215795 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.218924 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.224073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.228007 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.241039 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.247241 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.255173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.258220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.265826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.269031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.286953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.292197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.298758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.303934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.314858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.320812 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.331764 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.337147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.347232 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.351171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.365998 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.370656 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.379607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.384149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.393143 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.396502 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.413512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.418350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.425575 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.429790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.440041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.445575 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.456552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.463741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.471496 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.475115 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.514086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.517411 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.530797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.535216 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.544512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.549422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:05.559757 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f4529924190> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1865  696]\n",
            " [ 518 4108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.78      0.73      0.75      2561\n",
            "        True       0.86      0.89      0.87      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.82      0.81      0.81      7187\n",
            "weighted avg       0.83      0.83      0.83      7187\n",
            "\n",
            "evaluating epoch: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:50:51.857301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.860691 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.862374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.864186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.873210 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.887892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.898049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.904660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.907741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.927335 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.930273 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.937400 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.940355 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.946971 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.949959 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.963972 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.969202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.974725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.977721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.986057 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.989914 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:51.999443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.002501 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.007827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.011518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.024049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.027642 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.035092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.038059 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.043771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.046585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.060324 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.065545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.070771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.073658 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.082108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.085659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.096323 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.100706 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.106159 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.110304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.120275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.123173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.129722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.134675 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.139929 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.143480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.158721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.161715 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.167526 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.171787 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.183896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.188147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.197561 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.200793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.206685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.210424 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.220793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.223700 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.231050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.233846 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.240353 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.244102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.258857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.262490 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.268004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.272979 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.284024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.287659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.296401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.299412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.304986 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.308758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.319761 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.325006 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.333879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.339705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.345942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.350203 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.364720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.370534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.376511 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.380995 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.390062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.394937 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.404139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.408282 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.416305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.420342 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.432766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.436705 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.445172 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.448517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.454143 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.457941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.474035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.477243 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.483165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.488311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.499268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.503385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.512790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.515783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.521042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.525369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.536813 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.541167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.547183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.549983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.556114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.559035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.574130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.577853 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.583373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.586406 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.600147 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.604295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.613851 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.616771 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.622430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.625392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.637794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.640930 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.647964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.652060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.657994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.662031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.678046 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.683777 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.692384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.695518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.705509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.709072 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.718559 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.722379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.727691 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.730557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.741935 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.744755 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.752072 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.755433 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.762222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.765165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.780519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.783504 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.788894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.791713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.800884 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.804440 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.812966 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.817357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.822338 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.826673 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.837614 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.841026 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.847651 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.850852 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.856611 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.860430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.876746 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.880681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.886055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.888931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.898719 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.903296 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.912499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.915359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.920977 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.925945 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.937011 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.940474 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.947305 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.950109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.956069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.959050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.973557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.976450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.981982 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.985013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.994402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:52.998156 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.007373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.010220 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.013925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.016947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.032582 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.035547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.042418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.045260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.051528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.054341 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.074110 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.079302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.084796 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.090653 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.102257 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.107665 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.117986 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.122392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.128834 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.133280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.166167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.170264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.183767 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.187108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.196702 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.201117 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:50:53.209532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452ab7e9b0> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1730  831]\n",
            " [ 436 4190]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.80      0.68      0.73      2561\n",
            "        True       0.83      0.91      0.87      4626\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      7187\n",
            "   macro avg       0.82      0.79      0.80      7187\n",
            "weighted avg       0.82      0.82      0.82      7187\n",
            "\n",
            "evaluating epoch: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:51:48.618505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.624773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.626554 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.628412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.637659 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.646815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.655992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.662627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.665936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.685934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.689476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.696855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.700267 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.707086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.710392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.723676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.728085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.734549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.738714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.747786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.752655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.763163 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.766907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.774162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.777749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.790023 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.793648 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.801670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.805609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.812443 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.816040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.838596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.845196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.853307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.858859 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.871226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.877258 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.890557 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.895662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.904041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.908809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.920448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.924717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.931545 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.935255 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.941178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.945272 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.959109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.962251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.969214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.972901 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.982136 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.986936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.996088 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:48.999183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.007107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.011893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.023792 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.027398 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.035731 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.039911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.048830 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.053378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.069483 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.075089 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.080888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.084515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.094158 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.099756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.109833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.113616 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.119685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.125402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.137876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.142251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.150373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.154450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.163270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.176431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.195888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.201626 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.207515 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.212713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.222832 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.228827 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.239316 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.244574 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.251069 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.256660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.271194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.276917 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.283797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.287681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.293476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.298968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.315236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.318427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.325535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.328775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.340452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.346132 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.360196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.364144 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.370806 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.374293 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.386262 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.389609 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.396080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.400331 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.406374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.410824 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.429828 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.432790 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.439050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.444052 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.455820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.462244 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.472310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.476747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.481021 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.486532 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.497622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.502192 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.508991 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.512881 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.519166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.522073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.539992 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.544121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.550514 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.554594 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.564266 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.569472 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.578537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.582484 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.588535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.592823 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.603384 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.607569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.615197 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.619359 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.626365 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.631174 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.643785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.648657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.654212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.657829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.667711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.671936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.681020 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.684432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.690371 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.693875 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.705873 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.709242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.720206 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.723630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.729976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.733328 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.749090 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.752758 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.759783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.763395 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.774552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.779171 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.791167 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.796428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.804580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.808710 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.821600 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.825187 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.833259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.837013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.845004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.848543 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.865314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.873310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.880740 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.884497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.895196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.899416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.909776 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.913431 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.919230 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.925211 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.937480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.942822 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.950278 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.955873 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.963092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.969780 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.985438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.991089 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:49.997920 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.003134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.013701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.019325 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.029819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.034392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.041049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.045763 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.079298 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.083734 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.097304 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.102674 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.111438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.116194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:51:50.125428 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f452732d190> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1673  888]\n",
            " [ 333 4293]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.83      0.65      0.73      2561\n",
            "        True       0.83      0.93      0.88      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.83      0.79      0.80      7187\n",
            "weighted avg       0.83      0.83      0.82      7187\n",
            "\n",
            "evaluating epoch: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:52:40.265716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.267649 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.269099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.270385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.278759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.293327 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.303056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.309864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.312892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.332667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.336772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.344342 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.347754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.355297 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.358426 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.372209 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.376018 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.383105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.386146 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.395289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.399599 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.408899 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.412050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.418725 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.421974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.433178 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.436203 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.443783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.446685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.452970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.455707 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.471095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.476095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.482858 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.485953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.495934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.500107 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.509390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.513370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.519752 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.522830 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.534440 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.538055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.546165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.549427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.556603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.560379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.578783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.581746 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.588855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.591742 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.601150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.604829 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.614501 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.617302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.623553 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.626235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.637528 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.640275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.648166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.650943 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.658480 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.661838 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.677719 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.680670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.686234 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.689708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.699804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.704034 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.714394 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.717264 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.723102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.726418 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.740045 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.742963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.750415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.753621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.759964 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.763509 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.777441 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.781301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.787446 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.791096 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.800717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.805624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.814062 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.817436 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.823585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.826800 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.839170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.843194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.851401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.854562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.864377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.867801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.884251 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.887846 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.894192 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.897049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.907111 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.910888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.921962 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.924931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.933723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.939268 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.951549 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.955183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.962723 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.965748 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.972717 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.980741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.995419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:40.998641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.005155 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.009727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.025619 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.030826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.043893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.048413 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.054105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.058326 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.071116 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.074259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.081361 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.085242 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.092535 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.095385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.111695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.114732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.119925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.123491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.134571 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.138216 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.147378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.150149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.156495 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.159219 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.170809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.173722 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.180370 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.183712 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.189853 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.192583 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.210630 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.213821 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.219346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.223617 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.233429 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.237087 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.246798 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.249876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.256002 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.259668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.271770 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.276079 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.284049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.287492 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.294351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.298168 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.312505 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.316518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.322926 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.326958 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.336330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.340620 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.351783 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.355098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.362399 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.365727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.378931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.382721 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.390932 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.394140 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.400093 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.403060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.416786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.419713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.426177 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.429416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.438669 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.442423 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.451540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.454289 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.460768 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.463610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.475022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.478660 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.485435 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.489366 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.495980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.500124 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.514300 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.517677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.524974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.528975 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.538882 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.543567 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.553188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.557540 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.564627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.568701 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.597951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.602978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.615381 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.620497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.628942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.633281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:52:41.642055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451e73ca00> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1886  675]\n",
            " [ 571 4055]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.77      0.74      0.75      2561\n",
            "        True       0.86      0.88      0.87      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.81      0.81      0.81      7187\n",
            "weighted avg       0.83      0.83      0.83      7187\n",
            "\n",
            "evaluating epoch: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:53:27.240442 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.243529 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.247287 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.249231 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.260756 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.274105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.285186 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.292542 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.295955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.317713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.321244 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.328744 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.332235 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.339138 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.344301 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.357508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.362529 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.368685 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.371773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.380228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.384035 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.393233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.396332 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.401598 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.405497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.416644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.420087 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.427747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.431385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.437427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.440922 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.458425 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.465049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.472049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.477802 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.487715 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.494055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.503479 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.506833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.514506 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.517759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.531934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.537631 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.544481 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.548213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.554965 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.559149 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.574815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.578083 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.585537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.589627 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.604373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.608911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.620024 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.625610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.631618 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.636786 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.648602 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.654438 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.662097 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.665512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.672909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.675954 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.691348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.694407 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.699615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.702879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.713624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.717968 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.727427 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.730390 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.736588 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.741507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.752963 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.757550 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.765193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.768603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.776055 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.779217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.795726 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.798985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.805115 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.808578 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.818773 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.823596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.833307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.836369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.842422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.845793 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.859184 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.862385 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.869759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.873001 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.879702 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.882610 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.897022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.901146 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.906580 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.910099 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.920667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.924491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.934351 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.938214 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.943888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.947139 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.958892 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.962032 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.969317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.972373 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.980094 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.983607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:27.998897 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.002306 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.008711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.013187 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.023302 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.030889 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.041596 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.046303 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.051645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.054974 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.070290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.074655 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.081662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.086102 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.092809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.095833 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.112260 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.115350 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.120348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.124969 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.135236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.139321 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.150362 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.153547 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.158638 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.162261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.174215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.177246 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.184202 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.188514 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.195357 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.198352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.216077 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.222641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.229804 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.235280 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.244657 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.250248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.259779 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.265240 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.272155 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.277664 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.290193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.296847 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.303311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.307801 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.314013 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.322839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.341890 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.347493 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.354247 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.357345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.368259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.373095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.383347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.387799 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.395924 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.399672 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.416894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.420628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.429065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.432662 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.439879 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.443330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.463037 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.467816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.474978 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.479444 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.489401 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.496710 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.509727 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.513233 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.518886 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.522983 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.534488 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.537508 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.544893 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.548863 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.555135 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.561307 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.576173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.579898 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.587142 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.590967 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.604101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.609049 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.619076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.623666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.630029 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.633857 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.664513 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.668865 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.685376 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.688988 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.698379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.701883 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:53:28.711785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451ae54320> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1795  766]\n",
            " [ 414 4212]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.81      0.70      0.75      2561\n",
            "        True       0.85      0.91      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.83      0.81      0.81      7187\n",
            "weighted avg       0.83      0.84      0.83      7187\n",
            "\n",
            "evaluating epoch: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:54:20.631896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.634183 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.635942 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.638317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.646512 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.660676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.670666 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.677151 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.680270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.699136 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.702785 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.709415 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.713391 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.719109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.722668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.736784 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.740044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.745690 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.748714 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.756797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.760668 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.769737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.772711 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.778176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.781213 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.792236 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.795416 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.802624 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.806025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.812031 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.815004 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.828583 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.831435 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.838222 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.841377 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.850975 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.856754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.865314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.868114 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.874644 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.877839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.891766 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.894934 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.905056 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.908706 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.915843 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.919943 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.934562 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.938622 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.946363 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.950485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.962754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.968592 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.980176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.983951 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.989645 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:20.992876 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.004736 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.009953 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.017261 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.022120 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.028229 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.034991 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.053122 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.057888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.063193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.067095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.077392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.082221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.090615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.095101 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.100640 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.105042 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.117588 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.122137 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.130194 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.133933 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.140686 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.144864 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.159336 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.163445 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.170713 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.174258 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.184294 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.188628 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.202228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.207670 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.215125 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.219700 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.231292 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.234848 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.241868 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.245095 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.251497 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.254856 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.272085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.279092 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.285887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.289346 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.302887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.307949 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.318088 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.321417 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.328196 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.331976 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.343797 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.347537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.354558 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.357641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.364450 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.367907 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.387862 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.391894 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.397286 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.400188 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.408698 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.412498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.422449 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.425939 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.431839 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.436516 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.447560 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.450606 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.457911 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.461030 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.466552 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.469494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.484086 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.487162 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.493217 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.496315 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.505290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.509259 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.517741 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.521809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.527358 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.531430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.545408 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.550689 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.557677 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.563448 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.571120 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.574973 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.592633 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.597121 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.603308 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.607161 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.616852 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.621041 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.630070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.633181 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.638636 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.641452 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.652896 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.656491 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.663347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.666100 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.672849 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.675612 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.691749 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.696367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.702080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.705936 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.715487 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.720585 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.730170 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.734246 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.739912 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.743607 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.755215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.759511 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.766994 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.770663 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.778131 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.782330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.797708 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.801867 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.808507 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.812369 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.822621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.827733 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.837347 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.841769 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.849273 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.852955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.867615 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.871176 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.880419 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.884248 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.891129 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.896747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.911070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.914737 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.920794 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.926275 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.936098 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.940676 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.949947 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.953720 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.960572 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.964485 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:21.996165 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.000281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.014442 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.018073 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.028736 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.032519 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:54:22.041603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f451d07ecd0> ignored\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1630  931]\n",
            " [ 282 4344]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.85      0.64      0.73      2561\n",
            "        True       0.82      0.94      0.88      4626\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      7187\n",
            "   macro avg       0.84      0.79      0.80      7187\n",
            "weighted avg       0.83      0.83      0.82      7187\n",
            "\n",
            "evaluating epoch: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1120 01:55:13.477834 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.479759 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.481285 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.482909 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.491228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.505517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.516432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.523371 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.526566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.545430 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.548402 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.554681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.557816 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.564826 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.567754 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.583065 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.586215 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.591299 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.594506 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.603667 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.607311 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.616362 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.619277 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.624445 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.627918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.641195 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.645199 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.652498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.655476 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.662281 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.665250 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.681931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.687179 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.693422 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.696568 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.707257 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.711314 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.721312 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.725239 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.731153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.737531 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.754182 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.757498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.765841 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.770411 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.777970 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.781249 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.798732 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.802290 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.809520 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.822345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.836865 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.841887 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.852044 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.855855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.862494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.866548 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.878447 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.882392 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.889918 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.893022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.901326 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.905913 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.926084 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.930227 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.938517 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.943286 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.955132 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.960527 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.972109 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.976475 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.983378 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.987374 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:13.999603 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.003941 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.011518 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.015064 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.025665 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.029661 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.045475 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.049040 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.057166 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.060566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.072221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.077193 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.088330 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.092173 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.099076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.103348 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.115406 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.123080 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.134345 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.139343 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.146888 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.150108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.166775 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.171788 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.179903 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.185028 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.195671 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.200113 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.214555 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.220221 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.225127 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.230494 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.240952 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.243792 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.251050 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.253855 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.260310 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.263226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.278076 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.281153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.287060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.291397 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.300738 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.305413 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.314681 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.318823 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.327925 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.330770 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.342796 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.345747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.352814 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.355927 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.362379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.365226 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.379295 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.385085 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.391108 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.394270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.404412 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.408548 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.418283 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.422022 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.428787 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.432018 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.443955 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.447112 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.455205 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.458534 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.466449 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.469553 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.485738 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.489820 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.497174 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.500315 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.509931 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.513809 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.523498 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.526979 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.533751 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.537379 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.551985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.555569 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.564538 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.568118 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.576201 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.580130 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.596432 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.600150 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.607134 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.610831 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.620956 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.625441 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.635716 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.639319 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.646343 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.650563 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.663815 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.668566 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.675621 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.678985 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.686789 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.690980 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.706228 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.709160 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.715537 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.718499 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.728070 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.731587 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.741182 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.744060 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.748025 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.753747 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.768403 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.774772 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.784641 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.788367 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.795212 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.799352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.816113 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.821352 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.827899 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.831317 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.842695 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.847393 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.857613 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.860564 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.867119 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.871105 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.904002 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.907902 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.922360 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.925679 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.935153 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.940270 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1120 01:55:14.949819 139937064892288 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "[[1814  747]\n",
            " [ 397 4229]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.82      0.71      0.76      2561\n",
            "        True       0.85      0.91      0.88      4626\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      7187\n",
            "   macro avg       0.84      0.81      0.82      7187\n",
            "weighted avg       0.84      0.84      0.84      7187\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUkPORA68JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd = model_predict(estimator,test_features_fixed,test_InputExamples_fixed)\n",
        "true_label = list(test_fixed['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "    labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQReX-Hf0WiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(1,13):\n",
        "  \n",
        "#   print('processing epoch: %d'%i)\n",
        "#   pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=OUTPUT_DIR+'/saved_models/model.ckpt-%d'%(i*3000))\n",
        "#   true_label = list(dev['label'])\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#       labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpz2FhG3_sS",
        "colab_type": "code",
        "outputId": "8a496250-e34e-4336-a89d-4881ae1d656f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas\n",
        "pd = model_predict(estimator,dev_features,dev_InputExamples,checkpoint_path=\"gs://bert_example/aug19_models/mask_lm/weighted/last3layers/best_model/model.ckpt-2871\")\n",
        "true_label = list(dev['label'])\n",
        "labels_val = []\n",
        "for item in pd:\n",
        "      labels_val.append(np.argmax(item[1]))\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "print(labels_val)\n",
        "print(dev)\n",
        "dev['predicted'] = pandas.Series(labels_val)\n",
        "dev.to_csv('/content/dev_predicted.csv', encoding='utf8')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1202 16:38:30.290132 140711381813120 estimator.py:1148] Calling model_fn.\n",
            "I1202 16:38:40.225941 140711381813120 <ipython-input-12-2b4e9740ca24>:69] *** Features ***\n",
            "I1202 16:38:40.228056 140711381813120 <ipython-input-12-2b4e9740ca24>:71]   name = input_ids, shape = (1, 512)\n",
            "I1202 16:38:40.229468 140711381813120 <ipython-input-12-2b4e9740ca24>:71]   name = input_mask, shape = (1, 512)\n",
            "I1202 16:38:40.233813 140711381813120 <ipython-input-12-2b4e9740ca24>:71]   name = label_ids, shape = (1,)\n",
            "I1202 16:38:40.235363 140711381813120 <ipython-input-12-2b4e9740ca24>:71]   name = segment_ids, shape = (1, 512)\n",
            "E1202 16:38:45.441310 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.443294 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.444889 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.447694 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.457315 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.472230 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.484105 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.492336 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.496223 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.518151 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.521698 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.530038 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.534288 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.541749 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.545685 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.562135 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.565618 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.573982 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.579432 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.588403 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.594830 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.605897 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.611601 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.617494 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.622565 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.634253 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.640005 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.649632 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.655249 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.661530 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.664808 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.680066 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.683568 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.691823 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.697524 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.709034 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.715230 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.725450 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.731667 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.738266 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.742844 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.753904 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.757071 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.765321 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.770457 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.776432 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.781330 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.797013 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.800173 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.810993 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.816121 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.827384 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.839510 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.853085 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.858210 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.864274 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.867747 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.879667 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.882958 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.889893 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.894049 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.901371 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.905437 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.923108 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.927288 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.935105 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.941035 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.951170 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.955260 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.966154 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.973407 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.979738 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.982841 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.993339 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:45.996391 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.004441 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.007740 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.013978 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.017904 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.032890 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.035818 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.043029 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.048353 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.060816 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.068129 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.077887 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.081918 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.089834 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.093698 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.107161 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.110686 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.120027 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.124490 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.133745 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.137443 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.154175 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.157675 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.165255 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.169342 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.180370 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.186172 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.195704 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.199915 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.207009 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.211489 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.228430 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.237870 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.245805 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.251418 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.258479 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.264955 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.279962 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.286163 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.295125 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.300148 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.312452 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.318885 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.331136 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.334673 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.343826 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.348727 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.362205 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.366905 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.376766 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.380759 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.388756 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.393532 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.416409 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.421519 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.429200 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.432266 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.446305 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.451592 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.461903 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.466384 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.473645 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.477313 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.490921 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.494453 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.502403 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.507492 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.514311 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.518124 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.539675 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.545219 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.551558 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.555089 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.566306 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.570502 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.580748 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.584182 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.590008 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.594012 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.607119 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.610836 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.624070 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.627985 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.637449 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.641513 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.657726 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.660773 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.668373 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.671716 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.682034 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.686319 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.696666 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.703855 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.710731 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.716052 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.726811 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.730062 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.738971 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.744618 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.751027 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.756449 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.777275 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.780580 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.785888 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.789292 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.799530 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.803956 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.819004 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.824819 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.832482 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.836294 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.854024 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.859729 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.866127 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.873872 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.880435 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.886177 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.900566 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.905142 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.912695 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.916594 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.929147 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.933908 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.943890 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.947942 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.953867 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.956940 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:46.996933 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.000801 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.014372 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.017612 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.027189 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.030384 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1202 16:38:47.040164 140711381813120 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "I1202 16:38:47.118437 140711381813120 saver.py:1503] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1202 16:38:47.738950 140711381813120 <ipython-input-12-2b4e9740ca24>:100] **** Trainable Variables ****\n",
            "I1202 16:38:47.743808 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/embeddings/word_embeddings:0, shape = (30522, 768)\n",
            "I1202 16:38:47.749263 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I1202 16:38:47.755975 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I1202 16:38:47.760407 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.763369 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.766947 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.769181 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.770838 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.772746 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.774427 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.776319 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.777856 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.779818 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.781285 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.783025 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.784810 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.785773 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.786902 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.787800 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.788717 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.789623 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.790710 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.791707 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.792584 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.793661 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.794533 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.795588 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.796452 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.797585 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.798448 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.799444 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.800399 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.801399 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.802290 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.803283 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.804198 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.805161 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.806037 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.807045 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.809678 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.810688 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.811660 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.812735 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.813653 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.814613 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.815505 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.816307 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.817305 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.818171 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.819174 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.820043 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.820914 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.821886 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.822768 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.823910 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.824893 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.825809 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.826828 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.827723 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.829025 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.830401 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.831299 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.832417 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.833306 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.834287 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.835170 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.836252 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.837150 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.838100 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.838989 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.840078 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.841065 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.842087 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.843012 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.844122 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.845037 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.846070 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.847003 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.848018 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.848998 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.850178 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.851346 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.852365 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.853519 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.854577 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.855703 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.856826 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.857827 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.859031 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.860178 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.861167 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.862307 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.863481 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.864449 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.865587 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.866565 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.867708 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.868846 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.869890 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.871030 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.872200 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.873188 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.874321 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.875319 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.876406 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.877329 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.878309 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.879242 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.880441 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.881398 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.882396 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.883527 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.884531 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.885926 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.887008 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.888147 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.889800 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.890736 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.892183 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.894392 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.895617 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.896755 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.897757 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.898948 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.900300 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.901318 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.902478 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.903656 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.904696 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.905828 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.906991 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.907982 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.909157 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.910757 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.912014 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.913001 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.914180 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.915261 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.916302 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.917445 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.918459 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.919609 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.920761 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.921756 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.922918 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.924069 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.925057 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.926186 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.927190 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.928729 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.929980 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.931622 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.932796 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.933806 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.934959 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.936083 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.937129 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.938268 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.939239 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.940372 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.941343 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.942693 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.943850 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.944991 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.945971 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.947153 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.948261 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.949301 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.950476 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.951435 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.952606 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.953788 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.954812 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.955952 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.957149 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.958307 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.959284 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.960515 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.961680 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.962660 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.963861 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.964988 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.966011 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I1202 16:38:47.967262 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.968389 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I1202 16:38:47.969402 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.970668 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I1202 16:38:47.971795 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.972816 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.974004 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.975203 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.976201 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I1202 16:38:47.977814 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I1202 16:38:47.979137 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I1202 16:38:47.981030 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.982826 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.984014 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.985488 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.990014 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/bert/pooler/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.991364 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I1202 16:38:47.992528 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I1202 16:38:47.993926 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I1202 16:38:47.994738 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I1202 16:38:47.996068 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = module/cls/predictions/output_bias:0, shape = (30522,)\n",
            "I1202 16:38:47.996895 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = output_weights:0, shape = (2, 768)\n",
            "I1202 16:38:47.998153 140711381813120 <ipython-input-12-2b4e9740ca24>:106]   name = output_bias:0, shape = (2,)\n",
            "I1202 16:38:48.917650 140711381813120 estimator.py:1150] Done calling model_fn.\n",
            "I1202 16:38:48.926949 140711381813120 tpu_estimator.py:506] TPU job name worker\n",
            "I1202 16:38:49.420531 140711381813120 monitored_session.py:240] Graph was finalized.\n",
            "I1202 16:38:49.472172 140711381813120 saver.py:1284] Restoring parameters from gs://bert_example/aug19_models/mask_lm/weighted/last3layers/best_model/model.ckpt-2871\n",
            "I1202 16:39:04.774432 140711381813120 session_manager.py:500] Running local_init_op.\n",
            "I1202 16:39:04.961173 140711381813120 session_manager.py:502] Done running local_init_op.\n",
            "W1202 16:39:05.217267 140711381813120 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:818: load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I1202 16:39:05.415333 140711381813120 tpu_estimator.py:567] Init TPU system\n",
            "I1202 16:39:12.813209 140711381813120 tpu_estimator.py:576] Initialized TPU in 7 seconds\n",
            "I1202 16:39:12.816000 140709457921792 tpu_estimator.py:521] Starting infeed thread controller.\n",
            "I1202 16:39:12.817920 140709449529088 tpu_estimator.py:540] Starting outfeed thread controller.\n",
            "I1202 16:39:14.259958 140711381813120 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I1202 16:39:14.502230 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:14.504054 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:14.507564 140709449529088 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\n",
            "I1202 16:39:18.760032 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.761512 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.770312 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.771454 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.777694 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.779139 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.785451 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.787019 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.796019 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.797802 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.804743 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.806130 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.812037 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.813674 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.819847 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.821037 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.827455 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.828927 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.835306 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.836498 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.847953 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.853250 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.862643 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.864289 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.873569 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.875819 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.892950 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.894479 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.905695 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.907130 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.918474 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.919706 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.936929 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.938502 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.956876 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.958620 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.970983 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.973172 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.983746 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:18.985131 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:18.999628 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.000992 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.013463 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.015625 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.031445 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.032654 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.053735 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.055720 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.069192 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.070858 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.085329 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.086703 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.097814 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.098959 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.110909 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.112086 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.128817 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.130615 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.145344 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.146651 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.161799 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.163103 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.175695 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.176929 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.190273 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.191739 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.207856 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.209891 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.222245 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.224030 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.244621 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.245935 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.264878 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.266319 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.281897 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.283267 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.302984 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.304490 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.318017 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.319374 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.337836 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.339378 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.353890 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.355374 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.367292 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.368705 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.382314 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.383649 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.396683 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.398386 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.413507 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.415076 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.427814 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.429451 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.449171 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.450536 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.463139 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.464452 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.479852 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.481259 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.502350 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.503793 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.517607 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.519674 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.535339 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.536952 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.550710 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.552994 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.564898 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.566186 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.576164 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.577136 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.588002 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.589081 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.603691 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.605041 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.617455 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.618511 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.631872 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.632812 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.642177 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.643265 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.654726 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.655656 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.665812 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.667021 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.680835 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.682096 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.694622 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.695786 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.711805 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.713335 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.729118 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.730238 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.743201 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.744607 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.761588 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.762736 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.773155 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.774230 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.788616 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.789726 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.800759 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.801815 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.813273 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.814476 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.825140 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.826143 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.839291 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.840301 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.849919 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.850863 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.861205 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.862113 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.872723 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.873641 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.885248 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.886529 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.901664 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.902771 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.914902 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.916019 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.926775 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.927922 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.939727 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.940897 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.951894 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.953011 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.964407 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.965459 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.976007 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.977016 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.986148 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.987035 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:19.995534 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:19.996517 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.010173 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.011529 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.034869 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.036124 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.049654 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.051273 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.063462 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.064805 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.080141 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.081614 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.095072 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.096844 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.111110 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.112373 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.132814 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.134466 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.147073 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.148415 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.159255 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.160644 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.171824 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.173151 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.185930 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.187889 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.198781 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.200611 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.218295 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.219892 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.233069 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.234611 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.254132 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.255628 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.277398 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.279057 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.290927 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.294192 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.306927 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.308267 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.321727 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.323062 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.336868 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.338131 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.361251 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.362775 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.376379 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.378026 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.394129 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.395658 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.410068 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.411272 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.422578 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.424252 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.438633 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.439779 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.454751 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.456341 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.468970 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.470791 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.486584 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.487878 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.501528 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.503029 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.514286 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.515474 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.526377 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.527725 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.542748 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.544044 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.567228 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.568831 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.579180 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.580683 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.593780 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.595160 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.608582 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.609952 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.626627 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.628213 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.642205 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.643910 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.655353 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.656929 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.671732 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.672892 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.684856 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.686203 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.698837 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.700217 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.713893 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.715158 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.729188 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.730882 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.747864 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.749043 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.763808 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.764938 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.781652 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.782900 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.797441 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.798851 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.817949 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.819240 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.829796 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.831041 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.843422 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.844811 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.865459 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.866796 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.876969 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.878195 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.892210 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.893505 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.903162 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.904680 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.916615 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.917814 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.930928 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.932759 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.944257 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.945492 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.958723 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.960119 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.971590 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.972848 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.984460 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.986304 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:20.998203 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:20.999315 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.011132 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.012406 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.024462 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.025729 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.037185 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.038297 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.051983 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.054105 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.071407 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.072750 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.093081 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.094930 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.108225 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.109386 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.121117 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.122273 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.136233 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.137415 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.148188 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.149328 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.164588 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.165826 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.176845 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.178155 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.194278 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.195611 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.209825 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.211215 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.221662 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.222919 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.236712 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.238043 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.250230 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.251497 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.269870 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.271254 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.284784 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.286128 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.298036 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.299448 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.317341 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.318947 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.332355 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.334073 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.345845 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.347143 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.362346 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.363810 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.374696 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.376308 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.388505 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.390060 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.404723 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.406141 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.414887 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.415980 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.427366 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.428684 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.437716 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.438818 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.450721 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.452697 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.463932 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.465290 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.483897 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.486789 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.498323 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.499871 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.509856 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.511239 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.520292 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.521359 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.530618 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.531877 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.542444 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.543636 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.553178 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.554610 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.565445 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.566864 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.582081 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.583285 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.593779 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.595017 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.604942 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.607337 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.617721 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.618906 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.634182 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.635334 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.645361 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.646460 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.654258 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.655273 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.663775 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.665728 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.674845 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.676029 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.685590 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.687238 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.695981 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.697066 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.706790 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.707933 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.718069 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.719206 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.730469 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.732841 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.745112 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.746932 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.762207 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.763772 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.777283 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.778835 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.791974 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.796142 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.809504 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.811125 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.824917 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.826591 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.843426 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.844852 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.855745 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.857014 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.863894 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.865081 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.872127 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.873394 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.881565 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.882730 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.890523 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.891714 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.897962 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.899188 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.905746 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.907278 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.915071 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.917493 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.933940 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.935340 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.949637 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.951056 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.963677 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.965117 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.980551 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.981761 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:21.995563 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:21.997134 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.012212 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.013494 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.026205 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.027650 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.043721 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.046657 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.058309 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.059573 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.074739 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.076113 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.091809 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.093240 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.108151 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.110423 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.131252 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.132497 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.147707 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.149362 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.161432 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.162978 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.172472 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.173422 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.180491 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.181493 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.188635 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.189934 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.197751 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.198563 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.206228 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.207178 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.212145 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.213289 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.218049 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.219393 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.225924 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.228693 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.234848 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.235805 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.242387 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.243355 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.249034 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.249929 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.256381 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.257392 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.265080 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.266197 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.272839 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.273942 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.280774 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.281786 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.289665 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.290637 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.300040 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.301229 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.306704 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.309412 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.316035 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.318533 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.325107 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.327825 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.335083 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.336606 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.342786 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.344814 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.351718 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.353573 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.359833 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.362485 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.369441 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.371087 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.378236 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.379334 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.390340 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.392112 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.400338 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.401485 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.410451 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.411684 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.420784 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.422250 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.432957 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.434472 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.446774 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.448245 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.461321 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.462688 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.475258 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.476694 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.493187 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.494460 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.507710 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.509115 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.527211 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.529678 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.541395 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.543510 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.558895 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.560498 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.574249 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.575462 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.587038 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.588181 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.597769 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.598854 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.611139 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.612299 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.625504 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.626737 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.638936 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.640105 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.655689 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.656879 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.671475 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.672750 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.684880 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.692163 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.707565 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.708892 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.727294 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.729095 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.741921 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.743294 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.761513 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.762932 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.775294 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.777686 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.790596 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.791834 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.804038 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.805330 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.817495 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.819169 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.833853 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.835156 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.846251 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.847897 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.865194 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.866482 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.876913 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.878093 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.896419 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.898714 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.911361 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.912640 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.924433 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.925970 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.942883 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.944107 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.955321 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.956507 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.967335 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.968923 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.982204 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:22.983699 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:22.998675 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.000053 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.021125 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.022332 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.033236 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.034478 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.049978 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.051155 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.062849 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.064222 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.076906 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.078551 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.089854 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.091053 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.100992 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.102092 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.115092 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.116163 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.126981 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.127921 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.134130 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.135582 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.141632 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.142689 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.149688 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.150712 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.156224 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.157907 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.162816 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.164014 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.168605 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.170030 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.175682 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.177083 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.183248 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.184935 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.191173 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.192145 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.198705 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.199680 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.207062 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.208043 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.212992 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.214332 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.220496 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.222412 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.227654 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.228530 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.235286 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.236149 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.242712 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.243685 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.250020 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.251585 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.257283 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.258182 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.263641 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.264616 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.270174 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.272203 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.278425 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.280807 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.292845 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.294950 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.300065 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.301515 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.306389 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.308155 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.313389 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.316277 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.321167 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.322459 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.328628 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.329890 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.336009 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.336940 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.342418 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.343518 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.348393 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.349606 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.355264 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.356679 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.363370 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.364253 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.371325 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.372626 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.379374 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.380692 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.387039 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.388094 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.394145 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.395379 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.401521 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.402657 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.408615 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.409785 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.415266 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.416865 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.423446 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.425955 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.434946 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.439717 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.445143 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.446831 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.452390 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.457360 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.462726 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.467092 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.476804 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.477958 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.485038 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.486377 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.492726 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.494657 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.504250 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.505439 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.513794 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.515703 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.522938 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.523974 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.532351 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.533282 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.543562 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.545130 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.552993 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.553838 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.560486 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.561295 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.568737 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.569859 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.578207 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.579281 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.588342 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.590655 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.598793 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.599958 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.609319 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.610712 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.619498 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.620611 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.631210 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.632452 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.642590 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.643852 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.652811 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.654103 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.662861 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.664684 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.671703 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.672771 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.683440 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.685748 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.696413 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.698105 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.706635 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.707803 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.716212 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.717294 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.725804 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.726772 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.734457 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.735687 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.744589 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.746025 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.760000 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.761209 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.776595 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.777923 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.794012 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.795176 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.804691 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.805623 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.814735 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.815800 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.824609 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.825486 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.833317 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.834224 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.841691 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.842575 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.850753 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.851912 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.860275 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.861121 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.870810 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.871927 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.880105 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.881081 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.888395 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.889385 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.896509 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.897485 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.905376 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.906362 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.913829 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.914891 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.920398 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.921747 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.927620 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.928510 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.933237 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.934919 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.940609 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.941598 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.946652 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.947890 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.953761 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.954874 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.960613 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.962652 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.968321 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.969361 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.976202 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.977263 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.983007 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.984474 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:23.990797 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:23.991739 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.000430 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.001440 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.008527 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.009497 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.017328 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.018335 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.025738 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.027144 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.035970 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.037053 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.043560 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.044440 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.049665 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.051554 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.058453 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.059643 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.067514 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.068562 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.074738 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.075676 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.082848 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.083981 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.091798 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.092777 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.101617 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.102854 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.109512 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.111422 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.117989 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.119573 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.124370 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.126808 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.132657 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.133831 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.139637 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.141433 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.147947 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.150403 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.156003 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.158301 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.163578 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.165014 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.171766 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.172693 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.177268 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.179632 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.184868 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.186520 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.192749 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.193739 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.200022 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.201585 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.208293 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.209250 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.219171 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.220200 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.228745 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.229832 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.239897 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.241266 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.252675 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.253945 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.263381 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.264695 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.271420 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.273199 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.281153 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.282224 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.289087 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.290138 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.296866 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.297879 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.304881 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.305903 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.312179 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.314862 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.322707 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.324242 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.329865 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.331176 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.338090 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.340713 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.347979 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.350678 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.358308 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.361049 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.368817 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.369863 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.376849 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.378560 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.384936 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.385766 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.392364 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.394212 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.401793 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.403680 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.411119 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.412105 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.417622 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.422422 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.433339 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.434386 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.440989 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.442749 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.447937 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.449819 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.456013 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.457654 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.463505 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.465500 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.472707 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.473769 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.479749 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.480870 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.487628 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.489824 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.497773 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.499387 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.507121 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.508243 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.513643 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.515558 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.522243 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.523505 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.530570 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.532701 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.539377 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.540576 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.547964 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.549160 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.557449 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.558864 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.565450 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.567814 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.573693 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.574610 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.580383 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.581321 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.587680 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.588689 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.595907 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.598061 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.604954 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.606266 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.615788 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.617234 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.622818 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.626054 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.632272 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.634108 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.641561 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.643281 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.649884 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.650971 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.656830 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.657651 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.663664 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.664658 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.670572 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.671530 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.677752 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.678750 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.689810 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.694509 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.701646 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.702987 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.711096 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.712212 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.719278 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.720148 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.725871 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.726728 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.732076 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.732919 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.739595 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.740780 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.747822 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.749890 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.756484 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.758044 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.764149 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.766366 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.775693 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.777034 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.783864 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.785526 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.791791 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.792963 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.800267 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.801703 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.806752 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.809514 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.815200 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.818006 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.825225 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.826761 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.832329 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.834424 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.842788 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.844572 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.852835 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.853828 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.859839 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.861088 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.867475 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.868398 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.874512 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.875416 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.881974 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.883017 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.889396 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.891088 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.900466 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.901931 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.907207 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.909382 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.917510 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.921158 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.926806 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.927783 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.932403 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.933357 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.941699 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.942919 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.949469 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.950877 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.956523 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.958616 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.965336 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.966401 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.973314 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.974689 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.980787 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.982532 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.990223 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:24.991213 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:24.999402 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.000951 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.007082 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.008068 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.014522 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.015749 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.022077 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.023164 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.028477 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.029876 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.035455 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.036798 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.042139 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.043874 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.049698 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.050704 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.058514 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.059436 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.069756 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.070929 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.077718 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.078583 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.085460 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.086416 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.092741 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.093594 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.100174 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.101280 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.107206 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.108493 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.116044 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.117330 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.123862 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.125009 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.132152 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.134224 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.141257 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.144347 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.150609 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.151578 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.157341 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.159637 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.167300 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.168133 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.172758 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.175396 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.181269 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.182245 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.187444 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.189053 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.194420 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.195971 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.201528 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.203017 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.210063 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.210998 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.217179 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.218135 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.223686 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.224920 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.229396 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.230884 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.237287 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.238332 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.243289 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.244640 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.250514 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.251332 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.256113 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.257345 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.261949 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.263144 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.273854 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.275233 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.282629 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.283601 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.289776 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.290663 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.296000 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.297496 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.303972 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.304889 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.311189 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.312115 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.317322 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.318708 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.324446 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.325656 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.331403 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.332783 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.339226 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.340234 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.346781 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.347822 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.355048 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.356064 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.362457 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.363401 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.370712 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.372059 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.379378 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.382097 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.387732 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.390388 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.397403 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.398485 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.403923 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.406198 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.412143 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.413153 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.420432 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.421186 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.428432 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.429199 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.436320 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.437530 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.444230 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.445307 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.453360 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.454399 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.460474 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.462399 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.467869 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.469755 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.476182 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.477488 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.485409 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.486759 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.496206 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.498040 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.505378 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.506306 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.512478 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.514322 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.522044 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.523055 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.530246 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.531210 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.539748 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.540971 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.548835 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.550317 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.557529 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.558485 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.565258 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.566263 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.574250 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.575367 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.584225 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.585356 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.593358 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.594213 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.600267 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.601579 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.607584 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.608419 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.615246 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.616147 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.622844 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.623723 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.631716 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.632698 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.640336 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.641752 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.647803 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.649749 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.658344 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.659727 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.667680 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.669248 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.674587 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.676532 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.682816 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.683598 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.688436 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.690078 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.694911 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.696403 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.702708 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.703455 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.710362 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.711561 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.719429 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.720535 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.727130 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.728308 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.735066 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.736222 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.742786 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.744324 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.751358 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.752996 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.760262 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.761445 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.768660 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.769705 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.775966 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.777924 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.783529 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.784601 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.795366 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.796962 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.806478 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.811778 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.820271 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.821580 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.831455 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.835997 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.842521 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.845278 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.852710 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.853737 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.861092 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.862576 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.868665 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.869749 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.876650 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.877713 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.882765 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.884610 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.890126 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.891366 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.897361 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.898468 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.904994 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.906084 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.912087 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.915728 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.921422 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.922518 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.927710 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.929290 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.934242 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.936676 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.941947 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.943327 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.950134 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.952338 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.958106 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.959389 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.965728 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.967061 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.972136 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.973442 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.978971 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.980233 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.985102 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.986371 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.991635 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:25.992603 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:25.999011 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.000297 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.006699 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.008472 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.013781 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.015402 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.021737 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.024822 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.032119 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.033528 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.041150 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.043203 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.050467 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.051661 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.059813 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.061759 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.071691 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.073677 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.082840 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.084291 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.094594 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.095808 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.106590 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.107692 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.115787 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.117753 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.127872 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.129959 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.136578 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.137934 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.143897 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.145397 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.151253 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.153064 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.159112 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.160856 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.167659 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.168834 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.177122 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.179730 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.187238 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.188590 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.196449 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.197684 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.206079 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.207262 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.215826 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.217340 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.226116 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.227263 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.235430 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.238327 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.245367 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.247570 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.260608 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.263020 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.269753 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.271308 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.278345 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.279604 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.287187 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.289133 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.296730 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.298655 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.306447 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.309685 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.318492 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.320491 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.330291 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.331516 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.339895 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.341053 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.355936 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.361177 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.372334 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.373630 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.384027 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.385442 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.396778 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.398495 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.409054 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.410306 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.420840 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.422200 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.431819 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.433299 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.455135 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.459047 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.472150 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.473586 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.490667 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.491847 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.506769 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.508218 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.519665 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.521379 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.532759 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.534133 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.544096 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.545392 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.558434 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.559700 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.571214 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.572669 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.584512 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.586008 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.597613 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.599081 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.606980 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.608088 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.615308 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.616306 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.622185 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.623517 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.629789 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.631293 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.639008 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.640233 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.646513 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.648283 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.655476 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.656790 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.665039 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.666349 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.675903 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.677195 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.686657 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.688127 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.697746 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.699177 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.711529 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.713066 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.722110 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.725136 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.735110 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.736265 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.745806 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.747071 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.756266 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.757594 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.767066 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.768450 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.777573 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.780183 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.790271 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.791534 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.801043 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.802448 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.810966 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.812354 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.821597 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.822643 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.831897 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.833451 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.841730 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.842798 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.851190 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.852339 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.861707 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.862991 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.872337 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.873486 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.882685 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.884066 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.893297 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.896487 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.907964 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.909248 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.921224 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.923429 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.932709 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.933939 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.945277 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.948185 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.957515 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.959037 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.969523 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.970957 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.979736 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.980779 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:26.989969 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:26.991370 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.000256 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.001597 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.010709 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.011919 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.020276 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.021509 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.030174 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.031277 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.039278 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.043766 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.053843 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.054991 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.068614 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.070246 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.082365 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.083925 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.096270 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.097732 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.106681 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.108017 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.116229 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.117461 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.126094 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.127474 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.136343 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.137507 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.146306 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.147373 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.156147 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.157299 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.166322 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.167692 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.178985 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.180337 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.189754 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.192071 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.202070 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.203623 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.213392 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.214751 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.223784 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.224883 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.233061 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.234255 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.242717 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.244281 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.253493 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.255590 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.265136 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.266421 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.275788 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.277249 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.287283 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.288885 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.298779 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.300710 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.311429 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.312613 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.320594 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.321681 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.330801 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.332019 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.344208 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.345465 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.356730 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.358705 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.368391 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.369600 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.378451 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.381531 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.392807 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.394248 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.403110 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.404515 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.414135 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.415347 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.425388 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.426412 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.435969 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.437133 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.445789 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.447221 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.456517 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.457777 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.467410 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.469046 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.480638 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.482136 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.493583 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.494668 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.503990 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.505112 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.515733 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.516843 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.526556 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.527686 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.536015 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.537043 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.545350 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.546277 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.554373 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.555397 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.563832 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.564874 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.574064 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.575272 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.586184 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.587837 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.597238 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.598381 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.607064 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.608124 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.616661 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.617662 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.627063 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.628237 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.637032 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.638391 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.646835 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.648694 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.658974 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.660378 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.670053 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.671696 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.682502 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.684324 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.694827 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.696285 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.710313 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.711647 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.720585 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.721841 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.730339 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.731462 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.740516 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.741852 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.751607 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.752896 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.761754 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.764631 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.778659 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.779690 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.787384 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.788856 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.797933 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.799781 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.810827 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.812305 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.821294 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.823309 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.848517 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.850301 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.861366 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.862478 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.871023 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.872308 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.881908 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.883799 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.893151 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.895663 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.904988 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.906291 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.916448 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.917964 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.928011 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.929416 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.937825 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.939429 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.947623 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.948900 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.958580 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.959940 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.970149 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.971457 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.980922 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.982017 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:27.990978 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:27.992121 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.001816 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.002995 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.011651 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.013500 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.027791 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.029205 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.043437 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.044936 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.058331 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.059839 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.069454 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.070717 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.082288 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.084290 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.093925 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.095340 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.104379 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.105997 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.116019 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.117491 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.128653 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.130335 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.140388 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.141561 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.149852 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.151211 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.163415 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.164556 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.172859 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.174021 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.186006 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.187114 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.195729 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.196836 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.205929 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.207026 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.214772 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.216196 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.228118 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.229624 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.239521 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.240770 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.250658 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.252156 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.261049 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.262422 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.271564 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.272876 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.281508 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.282946 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.291789 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.293289 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.302369 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.303704 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.311702 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.312809 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.321208 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.322714 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.331641 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.332798 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.341336 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.342969 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.352123 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.353435 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.362792 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.363984 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.372980 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.374519 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.383210 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.384198 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.393172 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.394150 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.402911 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.403899 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.412589 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.413420 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.421144 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.421957 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.430515 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.432027 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.440372 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.441385 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.449671 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.450743 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.458630 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.460796 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.472623 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.475883 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.483972 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.485521 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.494342 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.495589 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.503684 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.505065 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.514087 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.516161 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.525913 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.527005 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.535664 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.537967 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.547530 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.548829 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.558799 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.559897 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.569268 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.570513 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.578879 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.579902 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.589626 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.590703 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.599998 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.601285 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.611413 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.612960 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.621524 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.622986 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.632437 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.633743 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.642874 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.644012 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.653384 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.654511 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.666398 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.667629 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.677382 140711381813120 tpu_estimator.py:600] Enqueue next (1) batch(es) of data to infeed.\n",
            "I1202 16:39:28.678630 140711381813120 tpu_estimator.py:604] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I1202 16:39:28.826879 140711381813120 error_handling.py:101] prediction_loop marked as finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1845  716]\n",
            " [ 198 4428]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.90      0.72      0.80      2561\n",
            "        True       0.86      0.96      0.91      4626\n",
            "\n",
            "   micro avg       0.87      0.87      0.87      7187\n",
            "   macro avg       0.88      0.84      0.85      7187\n",
            "weighted avg       0.88      0.87      0.87      7187\n",
            "\n",
            "[1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
            "      doc_id  label                                           sentence\n",
            "0       3922   True  tgt wife of 10 years  said   tgt   is dumping ...\n",
            "1       3733   True  Ã¢ÂÂThe boots represent the soldiers that ar...\n",
            "2       3547  False  The idea that the Knicks would somehow turn  t...\n",
            "3       3900  False  CLOSE SportsPulse: Drama.   tgt  . Comebacks. ...\n",
            "4       3678   True  Throwdown's 7'1\" center    tgt is in my crossh...\n",
            "5       3390  False  Volunteers In NYC Show Support For Immigrants ...\n",
            "6       3507   True  Washington (CNN) White House press secretary S...\n",
            "7       3384   True  NAIROBI   Kenya ( AP ) â Kenya ' s attorney ...\n",
            "8       3558   True  \"There are expensive Uber rides (thanks  surge...\n",
            "9       3513   True  Lurid allegations against tgtspread like wildf...\n",
            "10      3900   True  CLOSE SportsPulse: Drama.   [MASK]  . Comeback...\n",
            "11      3801  False  Since The New York Times published a bombshell...\n",
            "12      3439   True  A prosecutor said tgt admitted shooting Long a...\n",
            "13      3513   True  Lurid allegations against tgtspread like wildf...\n",
            "14      3412   True           Dear [MASK] , . \\nYou inspire me , tgt .\n",
            "15      3651   True  Not too long ago  if   tgt had gone to a docto...\n",
            "16      3617  False  More than decade ago  Shah and a friend  tgt  ...\n",
            "17      3398   True  [MASK]got the surprise of a lifetime when  tgt...\n",
            "18      3846  False  Then  we have to keep in mind that Democrats u...\n",
            "19      3390   True  Volunteers In NYC Show Support For Immigrants ...\n",
            "20      3920   True  tgt discussed the details of the incident Mond...\n",
            "21      3390  False  Volunteers In NYC Show Support For Immigrants ...\n",
            "22      3513   True  Lurid allegations against tgtspread like wildf...\n",
            "23      3777   True  An unnamed actress has filed  a lawsuit agains...\n",
            "24      3652   True  tgt  . (Photo: David Butler II  USA TODAY Spor...\n",
            "25      3490   True  The  tgtstory continues to reverberate  as new...\n",
            "26      3677   True  President-elect Donald Trump on Wednesday tapp...\n",
            "27      3487   True  tgt[MASK]  PBS Series And Working On The 'Toda...\n",
            "28      3649   True  Jefferson County Attorney Mike O'Connell's off...\n",
            "29      3619   True  \"I could have  though  which is one reason it ...\n",
            "...      ...    ...                                                ...\n",
            "7157    3800   True  One of the numerous women who shared a story r...\n",
            "7158    3722   True  THE constitution of the Chinese Communist Part...\n",
            "7159    3855   True  Ever since   tgt and  his  Patriots won a come...\n",
            "7160    3828   True   [MASK] delivered   tgt   own message minutes ...\n",
            "7161    3557  False  As officials counted ballots in the leadership...\n",
            "7162    3631  False  Forty years ago this month   tgtbecame the fir...\n",
            "7163    3582  False  Last month  a red carpet interview with Uma Th...\n",
            "7164    3463   True  \"It's absolutely our ambition to have this ful...\n",
            "7165    3462   True  Having just wrapped up a visit to China on the...\n",
            "7166    3445   True  tgt failed to convert two second set match poi...\n",
            "7167    3799  False  WASHINGTON (AP) Ã¢ÂÂ   tgt announced Thursda...\n",
            "7168    3713  False  'Promise Me':  Joe  Biden On Loss  Grief And R...\n",
            "7169    3649   True  Jefferson County Attorney Mike O'Connell's off...\n",
            "7170    3826   True  PORT ST. LUCIE Ã¢ÂÂ It sounds as if   [MASK]...\n",
            "7171    3459   True  But Monday  tgt held tgt nose long enough to m...\n",
            "7172    3717   True  (Reuters) - Walt Disney Co executive John Lass...\n",
            "7173    3542   True  CLOSE Former President Barack Obama told  Prin...\n",
            "7174    3857  False   tgt previously suggested   tgt   might call f...\n",
            "7175    3922   True  tgt wife of 10 years  said   tgt   is dumping ...\n",
            "7176    3391  False  Disgruntled wide receiver Martavis Bryant foll...\n",
            "7177    3532   True  Gutierrez  who came forward in 2015 to accuse ...\n",
            "7178    3691   True  After tweeting that snow in Atlanta might keep...\n",
            "7179    3717   True  (Reuters) - Walt Disney Co executive John Lass...\n",
            "7180    3710  False  tgt center  with [MASK]  left  the chairman of...\n",
            "7181    3373   True  If they fail  [MASK] can exercise tgt right to...\n",
            "7182    3465   True  The Washington Post's Margaret Sullivan discus...\n",
            "7183    3724   True  [MASK] was convicted in   tgt   third trial in...\n",
            "7184    3906  False  Meet The Russian TV Personality Running For Pr...\n",
            "7185    3494   True  As tgt are praising the bravery of the women w...\n",
            "7186    3640   True  Bitcoin medals (Photo: Karen Bleier  AFP/Getty...\n",
            "\n",
            "[7187 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GMYpmM-Q_B",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascfbXXbzEpw",
        "colab_type": "code",
        "outputId": "3b443645-20f0-4a63-9812-e3219a444502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# ## Run it if you want to train for a range of epochs and see the validation error and save the prediction on than\n",
        "# ## ** Part Name **\n",
        "# # mds = []\n",
        "# # evs = []\n",
        "# pds_dev = []\n",
        "# pds_tr = []\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL) \n",
        "\n",
        "# for i in range(1,11):\n",
        "#   print('----------------------- Starting Epoch %d-----------------------'%i)\n",
        "\n",
        "#   NUM_TRAIN_EPOCHS = i\n",
        "  \n",
        "#   num_train_steps = int(len(train_InputExamples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "#   num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# #   model_fn = model_fn_builder(\n",
        "# #   num_labels=len(label_list),\n",
        "# #   learning_rate=LEARNING_RATE,\n",
        "# #   num_train_steps=num_train_steps,\n",
        "# #   num_warmup_steps=num_warmup_steps,\n",
        "# #   use_tpu=True,\n",
        "# #   bert_hub_module_handle=BERT_MODEL_HUB)\n",
        "\n",
        "#   model_fn = model_fn_builder(\n",
        "#   bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "#   num_labels=len(label_list),\n",
        "#   init_checkpoint=INIT_CHECKPOINT,\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps,\n",
        "#   use_tpu=True,\n",
        "#   use_one_hot_embeddings=True)\n",
        "  \n",
        "  \n",
        "#   estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "#   use_tpu=True,\n",
        "#   model_fn=model_fn,\n",
        "#   config=run_config,\n",
        "#   train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#   eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#   predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "  \n",
        "#   model_train(estimator_from_tfhub)\n",
        "# #   ev = model_eval(estimator_from_tfhub)\n",
        "\n",
        "# #   print(' -------------------- Train Prediction --------------------')\n",
        "# #   pd = model_predict(estimator_from_tfhub,train_features,train_InputExamples)\n",
        "# #   true_label = list(train['label'])\n",
        "# #   pds_tr.append(pd)\n",
        "# #   labels_val = []\n",
        "# #   for item in pd:\n",
        "# #     labels_val.append(np.argmax(item[1]))\n",
        "# #   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# #   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "#   print(' -------------------- Dev Prediction --------------------')\n",
        "#   pd = model_predict(estimator_from_tfhub,dev_features[:3100],dev_InputExamples[:3100])\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   pds_dev.append(pd)\n",
        "#   labels_val = []\n",
        "#   for item in pd:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for tr,dv in zip(pds_tr,pds_dev):\n",
        "#   labels_val = []\n",
        "#   true_label = list(train['label'])\n",
        "#   for item in tr:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "#   labels_val = []\n",
        "#   true_label = list(dev['label'][:3100])\n",
        "#   for item in dv:\n",
        "#     labels_val.append(np.argmax(item[1]))\n",
        "#   print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "#   print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------- Starting Epoch 1-----------------------\n",
            "***** Started training at 2019-05-15 17:57:14.080193 *****\n",
            "  Num examples = 122150\n",
            "  Batch size = 8\n",
            "start running estimator\n",
            "(1, 2)\n",
            "excluded trainable variables: >> [<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>, <tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32>, <tf.Variable 'output_bias:0' shape=(2,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_dvH1MmpAr3",
        "colab_type": "code",
        "outputId": "ae794af2-b7a8-4188-b4ef-8dc97e42c452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2520
        }
      },
      "source": [
        "# prd = model_predict(estimator,train_features,train_InputExamples)\n",
        "# prd = [item for item in predictions]\n",
        "len(prd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9efd420be1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_InputExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# prd = [item for item in predictions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-3ccd6d9b41c7>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(estimator, input_features, input_examples)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mpredict_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2498\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.pyc\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m   2492\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m           yield_single_examples=yield_single_examples):\n\u001b[0m\u001b[1;32m   2495\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mscaffold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 config=self._session_config),\n\u001b[0;32m--> 627\u001b[0;31m             hooks=all_hooks) as mon_sess:\n\u001b[0m\u001b[1;32m    628\u001b[0m           \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpreds_evaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    932\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    933\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \"\"\"\n\u001b[1;32m   1121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.pyc\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1312\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n\nCaused by op u'save_2/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-9efd420be1df>\", line 1, in <module>\n    prd = model_predict(estimator,train_features,train_InputExamples)\n  File \"<ipython-input-45-3ccd6d9b41c7>\", line 18, in model_predict\n    return [(sentence, prediction['probabilities']) for sentence, prediction in zip(input_examples, predictions)]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2494, in predict\n    yield_single_examples=yield_single_examples):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 627, in predict\n    hooks=all_hooks) as mon_sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 934, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1122, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1127, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 805, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 562, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 3033, in _finalize\n    wrapped_finalize()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 217, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 604, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 507, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 385, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512/model.ckpt-979: Permission denied: Error executing an HTTP request: HTTP response code 403 with body '{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"forbidden\",\n    \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n   }\n  ],\n  \"code\": 403,\n  \"message\": \"service-495559152420@cloud-tpu.iam.gserviceaccount.com does not have storage.objects.list access to bert_example.\"\n }\n}\n'\n\t when reading gs://bert_example/mask_lm/models/V2/doc_leve/nontrainable/weighted/smallBERT-docLevel-seq512\n\t [[node save_2/RestoreV2 (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py:627) ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khuNEnMw2wiv",
        "colab_type": "code",
        "outputId": "83f8b377-31f6-4976-8cf4-d26796869d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "len(a)\n",
        "# import numpy as np\n",
        "# from sklearn import metrics\n",
        "\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(dev['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(np.argmax(item[1]))\n",
        "# true_label = list(train['label'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  48]\n",
            " [  0 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        48\n",
            "        True       0.76      1.00      0.86       152\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       200\n",
            "   macro avg       0.38      0.50      0.43       200\n",
            "weighted avg       0.58      0.76      0.66       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "outputId": "f4559586-e7a1-4674-c430-a3b426c101f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed['sentiment'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-985837170ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_from_tfhub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_InputExamples_fixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabels_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fixed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_InputExamples_fixed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzzg7MRSk5E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5SpMUg7b9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.logging.set_verbosity(tf.logging.DEBUG) #DEBUG,ERROR,FATAL,INFO,WARN\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test['sentiment'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqh11HPVAsvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####seq 128, Small BERT, Batchsize 32 for train, 8 for dev and test\n",
        "# 1)I0423 16:57:01.507206 140076901431168 basic_session_run_hooks.py:249] loss = 0.40919852, step = 46\n",
        "# 2)I0423 18:08:31.116359 140500355729280 basic_session_run_hooks.py:249] loss = 0.7756149, step = 92\n",
        "# 4)Loss for final step: 0.6891643.\n",
        "# 5)Loss for final step: 0.9730371\n",
        "# 6)Loss for final step: 0.4775733\n",
        "# 7)Loss for final step: 1.2802429.\n",
        "# 8)Loss for final step: 0.509207\n",
        "# 9)loss = 0.29262337, step = 414\n",
        "# 10)Loss for final step: 0.47267017\n",
        "\n",
        "\n",
        "# 2)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 92\n",
        "#   loss = 0.79767215\n",
        "# 3)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.8630275\n",
        "#   global_step = 138\n",
        "#   loss = 0.79767215\n",
        "# 4)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.86550355\n",
        "#   global_step = 184\n",
        "#   loss = 0.83540887\n",
        "# 5)***** Eval results *****\n",
        "#   eval_accuracy = 0.5964286\n",
        "#   eval_loss = 0.88053304\n",
        "#   global_step = 230\n",
        "#   loss = 0.91362196\n",
        "# 6)***** Eval results *****\n",
        "#   eval_accuracy = 0.5857143\n",
        "#   eval_loss = 0.90498805\n",
        "#   global_step = 276\n",
        "#   loss = 1.0806552\n",
        "# 7)***** Eval results *****\n",
        "#   eval_accuracy = 0.56785715\n",
        "#   eval_loss = 0.92742974\n",
        "#   global_step = 322\n",
        "#   loss = 1.0180835\n",
        "# 8)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9436406\n",
        "#   global_step = 368\n",
        "#   loss = 0.9721719\n",
        "# 9)***** Eval results *****\n",
        "#   eval_accuracy = 0.5607143\n",
        "#   eval_loss = 0.9714315\n",
        "#   global_step = 414\n",
        "#   loss = 0.5798999\n",
        "# 10)***** Eval results *****\n",
        "#   eval_accuracy = 0.54285717\n",
        "#   eval_loss = 1.0072392\n",
        "#   global_step = 460\n",
        "#   loss = 1.053762\n",
        "  \n",
        "#   DEV Info:\n",
        "# 1)[[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "# 2)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 3)[[  0   9  14]\n",
        "#  [  0  25  72]\n",
        "#  [  0  21 143]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.45      0.26      0.33        97\n",
        "#     Positive       0.62      0.87      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.36      0.38      0.35       284\n",
        "# weighted avg       0.52      0.59      0.53       284\n",
        "# 4)[[  0   8  42]\n",
        "#  [  0  16 100]\n",
        "#  [  0  17 157]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        50\n",
        "#      Neutral       0.39      0.14      0.20       116\n",
        "#     Positive       0.53      0.90      0.66       174\n",
        "\n",
        "#    micro avg       0.51      0.51      0.51       340\n",
        "#    macro avg       0.31      0.35      0.29       340\n",
        "# weighted avg       0.40      0.51      0.41       340\n",
        "# 5)[[  0  17   6]\n",
        "#  [  0  40  57]\n",
        "#  [  0  35 129]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.41      0.42        97\n",
        "#     Positive       0.67      0.79      0.72       164\n",
        "\n",
        "#    micro avg       0.60      0.60      0.60       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.60      0.56       284\n",
        "# 6)[[  0  19   4]\n",
        "#  [  0  44  53]\n",
        "#  [  0  42 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.45      0.44        97\n",
        "#     Positive       0.68      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.37      0.40      0.38       284\n",
        "# weighted avg       0.54      0.58      0.56       284\n",
        "# 7)[[  0  18   5]\n",
        "#  [  0  46  51]\n",
        "#  [  0  47 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.41      0.47      0.44        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.36      0.40      0.38       284\n",
        "# weighted avg       0.53      0.57      0.55       284\n",
        "# 8)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  53 111]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.68      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "# 9)[[  0  18   5]\n",
        "#  [  0  48  49]\n",
        "#  [  0  54 110]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.49      0.44        97\n",
        "#     Positive       0.67      0.67      0.67       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.39      0.37       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "# 10)[[  0  18   5]\n",
        "#  [  0  51  46]\n",
        "#  [  0  62 102]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.53      0.45        97\n",
        "#     Positive       0.67      0.62      0.64       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.54      0.52       284\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0h7wMprmvyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Seq:256, small BERT, batch size 32 for train, 8 for test and dev\n",
        "# 3) {'loss': 0.79650426, 'eval_accuracy': 0.58214283, 'eval_loss': 0.86125755, 'global_step': 138}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# 4) {'loss': 0.73700047, 'eval_accuracy': 0.5928571, 'eval_loss': 0.8223035, 'global_step': 184}\n",
        "# [[  0  11  12]\n",
        "#  [  0  20  77]\n",
        "#  [  0  17 147]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.21      0.28        97\n",
        "#     Positive       0.62      0.90      0.73       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.35      0.37      0.34       284\n",
        "# weighted avg       0.50      0.59      0.52       284\n",
        "\n",
        "# 5) {'loss': 0.71594626, 'eval_accuracy': 0.5928571, 'eval_loss': 0.82239425, 'global_step': 230}\n",
        "# [[  0  19   4]\n",
        "#  [  0  51  46]\n",
        "#  [  0  48 116]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.43      0.53      0.47        97\n",
        "#     Positive       0.70      0.71      0.70       164\n",
        "\n",
        "#    micro avg       0.59      0.59      0.59       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.55      0.59      0.57       284\n",
        "\n",
        "# 6) {'loss': 0.7227276, 'eval_accuracy': 0.5857143, 'eval_loss': 0.8479867, 'global_step': 276}\n",
        "# [[  0  21   2]\n",
        "#  [  2  51  44]\n",
        "#  [  1  49 114]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.42      0.53      0.47        97\n",
        "#     Positive       0.71      0.70      0.70       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.38      0.41      0.39       284\n",
        "# weighted avg       0.56      0.58      0.57       284\n",
        "\n",
        "# 7){'loss': 0.83304703, 'eval_accuracy': 0.5642857, 'eval_loss': 0.90589315, 'global_step': 322}\n",
        "# [[  0  19   4]\n",
        "#  [  4  40  53]\n",
        "#  [  3  42 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.40      0.41      0.40        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.36      0.38      0.37       284\n",
        "# weighted avg       0.53      0.56      0.54       284\n",
        "\n",
        "# 8){'loss': 0.84049994, 'eval_accuracy': 0.575, 'eval_loss': 0.93640614, 'global_step': 368}\n",
        "# [[  2  18   3]\n",
        "#  [  5  42  50]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.22      0.09      0.12        23\n",
        "#      Neutral       0.40      0.43      0.42        97\n",
        "#     Positive       0.69      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.44      0.41      0.42       284\n",
        "# weighted avg       0.55      0.57      0.56       284\n",
        "\n",
        "# 9){'loss': 0.8601472, 'eval_accuracy': 0.5642857, 'eval_loss': 0.95250976, 'global_step': 414}\n",
        "# [[  0  18   5]\n",
        "#  [  4  37  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.39      0.38      0.38        97\n",
        "#     Positive       0.67      0.74      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.35      0.38      0.36       284\n",
        "# weighted avg       0.52      0.56      0.54       284\n",
        "\n",
        "\n",
        "# 10){'loss': 0.9091368, 'eval_accuracy': 0.5535714, 'eval_loss': 0.9715489, 'global_step': 460}\n",
        "# [[  0  19   4]\n",
        "#  [  4  37  56]\n",
        "#  [  1  44 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.37      0.38      0.38        97\n",
        "#     Positive       0.66      0.73      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.34      0.37      0.36       284\n",
        "# weighted avg       0.51      0.55      0.53       284\n",
        "\n",
        "# 11){'loss': 0.9756337, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0129306, 'global_step': 506}\n",
        "# [[  3  16   4]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.39      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 12){'loss': 0.9551747, 'eval_accuracy': 0.56785715, 'eval_loss': 1.0222387, 'global_step': 552}\n",
        "# [[  3  17   3]\n",
        "#  [  5  38  54]\n",
        "#  [  2  43 119]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.30      0.13      0.18        23\n",
        "#      Neutral       0.39      0.39      0.39        97\n",
        "#     Positive       0.68      0.73      0.70       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.45      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 13){'loss': 1.0276382, 'eval_accuracy': 0.5714286, 'eval_loss': 1.0547612, 'global_step': 598}\n",
        "# [[  3  17   3]\n",
        "#  [  5  36  56]\n",
        "#  [  1  41 122]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.37      0.38        97\n",
        "#     Positive       0.67      0.74      0.71       164\n",
        "\n",
        "#    micro avg       0.57      0.57      0.57       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.57      0.55       284\n",
        "\n",
        "# 14){'loss': 1.0036505, 'eval_accuracy': 0.55714285, 'eval_loss': 1.0697843, 'global_step': 644}\n",
        "# [[  3  17   3]\n",
        "#  [  7  36  54]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.37      0.37      0.37        97\n",
        "#     Positive       0.67      0.72      0.70       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.43      0.41      0.41       284\n",
        "# weighted avg       0.54      0.55      0.54       284\n",
        "\n",
        "# 15){'loss': 1.0295725, 'eval_accuracy': 0.5642857, 'eval_loss': 1.0809377, 'global_step': 690}\n",
        "# [[  3  17   3]\n",
        "#  [  5  39  53]\n",
        "#  [  1  46 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.33      0.13      0.19        23\n",
        "#      Neutral       0.38      0.40      0.39        97\n",
        "#     Positive       0.68      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.56      0.56      0.56       284\n",
        "#    macro avg       0.46      0.42      0.42       284\n",
        "# weighted avg       0.55      0.56      0.55       284\n",
        "\n",
        "# 16){'loss': 1.0889318, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1056138, 'global_step': 736}\n",
        "# [[  3  17   3]\n",
        "#  [  8  33  56]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.42      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 17){'loss': 1.1312778, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1440269, 'global_step': 782}\n",
        "# [[  3  17   3]\n",
        "#  [  9  33  55]\n",
        "#  [  2  44 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.35      0.34      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.52      0.54      0.53       284\n",
        "\n",
        "# 18){'loss': 1.1436831, 'eval_accuracy': 0.55, 'eval_loss': 1.1613237, 'global_step': 828}\n",
        "# [[  3  16   4]\n",
        "#  [  8  35  54]\n",
        "#  [  2  45 117]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.23      0.13      0.17        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.71      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.54       284\n",
        "\n",
        "# 19){'loss': 1.1817628, 'eval_accuracy': 0.55, 'eval_loss': 1.1834545, 'global_step': 874}\n",
        "# [[  3  16   4]\n",
        "#  [  8  34  55]\n",
        "#  [  1  45 118]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.25      0.13      0.17        23\n",
        "#      Neutral       0.36      0.35      0.35        97\n",
        "#     Positive       0.67      0.72      0.69       164\n",
        "\n",
        "#    micro avg       0.55      0.55      0.55       284\n",
        "#    macro avg       0.42      0.40      0.41       284\n",
        "# weighted avg       0.53      0.55      0.53       284\n",
        "\n",
        "# 20){'loss': 1.1921012, 'eval_accuracy': 0.54642856, 'eval_loss': 1.1976833, 'global_step': 920}\n",
        "# [[  3  16   4]\n",
        "#  [  9  35  53]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.13      0.16        23\n",
        "#      Neutral       0.36      0.36      0.36        97\n",
        "#     Positive       0.67      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.54      0.54      0.54       284\n",
        "#    macro avg       0.41      0.40      0.40       284\n",
        "# weighted avg       0.53      0.54      0.53       284\n",
        "\n",
        "# 21){'loss': 1.2514663, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2244278, 'global_step': 966}\n",
        "# [[  3  16   4]\n",
        "#  [ 11  32  54]\n",
        "#  [  2  47 115]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.19      0.13      0.15        23\n",
        "#      Neutral       0.34      0.33      0.33        97\n",
        "#     Positive       0.66      0.70      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.40      0.39      0.39       284\n",
        "# weighted avg       0.51      0.53      0.52       284\n",
        "\n",
        "# 22){'loss': 1.2630298, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2527977, 'global_step': 1012}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284\n",
        "\n",
        "# 23){'loss': 1.3330169, 'eval_accuracy': 0.53571427, 'eval_loss': 1.2815608, 'global_step': 1058}\n",
        "# [[  6  13   4]\n",
        "#  [ 14  31  52]\n",
        "#  [  9  42 113]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.21      0.26      0.23        23\n",
        "#      Neutral       0.36      0.32      0.34        97\n",
        "#     Positive       0.67      0.69      0.68       164\n",
        "\n",
        "#    micro avg       0.53      0.53      0.53       284\n",
        "#    macro avg       0.41      0.42      0.42       284\n",
        "# weighted avg       0.53      0.53      0.53       284\n",
        "\n",
        "# 24){'loss': 1.3111317, 'eval_accuracy': 0.5321429, 'eval_loss': 1.2898273, 'global_step': 1104}\n",
        "# [[  5  14   4]\n",
        "#  [ 14  32  51]\n",
        "#  [  9  43 112]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.18      0.22      0.20        23\n",
        "#      Neutral       0.36      0.33      0.34        97\n",
        "#     Positive       0.67      0.68      0.68       164\n",
        "\n",
        "#    micro avg       0.52      0.52      0.52       284\n",
        "#    macro avg       0.40      0.41      0.41       284\n",
        "# weighted avg       0.52      0.52      0.52       284"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqSe326lwHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Larg BERT seq:256, batchsize 8 for train, dev , test\n",
        "# {'loss': 1.0679293, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8965841, 'global_step': 93}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0362501, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88540924, 'global_step': 186}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0290864, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8812368, 'global_step': 279}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.0271251, 'eval_accuracy': 0.5955882, 'eval_loss': 0.88323754, 'global_step': 372}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# {'loss': 1.02654, 'eval_accuracy': 0.5955882, 'eval_loss': 0.8804489, 'global_step': 465}\n",
        "# [[  0   0  23]\n",
        "#  [  0   0  97]\n",
        "#  [  0   0 164]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     Negative       0.00      0.00      0.00        23\n",
        "#      Neutral       0.00      0.00      0.00        97\n",
        "#     Positive       0.58      1.00      0.73       164\n",
        "\n",
        "#    micro avg       0.58      0.58      0.58       284\n",
        "#    macro avg       0.19      0.33      0.24       284\n",
        "# weighted avg       0.33      0.58      0.42       284\n",
        "\n",
        "# SAME FOR 10 EPOCHS!!!!!"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}