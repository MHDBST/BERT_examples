{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyBert_paragraph_TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/MyBert_paragraph_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "outputId": "dbfda781-3056-4c19-eb84-a8e188a21091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python2.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "('TPU address is', 'grpc://10.4.54.194:8470')\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 5559332680567153430),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10445438877619348536),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11447852806683110462),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3560235416749192272),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2304354183769936225),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10919000243704167055),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6670514842439782462),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13017166376088599804),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5391196356824532543),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 759290107800634268),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5145262404904090999)]\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "outputId": "39363d9b-8f15-4e25-a705-0bda52c271b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "# import python modules defined by BERT\n",
        "from bert import modeling\n",
        "# import optimization\n",
        "# import run_classifier\n",
        "from bert import run_classifier_with_tfhub\n",
        "# import tokenization\n",
        "\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0520 19:37:19.440768 140680220788608 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "d7b4e9e0-f4c0-47be-fb50-a077c0a31fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/smallBERT_ParWithLabel_seq128'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/bert-tfhub/models/smallBERT_ParWithLabel_seq128 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\" ##Small Bert\n",
        "# # BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\" ##Big Bert\n",
        "# def create_tokenizer_from_hub_module():\n",
        "#   \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "#   with tf.Graph().as_default():\n",
        "#     bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "#     tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "#     with tf.Session() as sess:\n",
        "#       vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "#                                             tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "#   return bert.tokenization.FullTokenizer(\n",
        "#       vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/uncased_L-12_H-768_A-12', 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "PREDICT_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 200\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "outputId": "e2531b11-ac03-46ae-f5fd-d0333dbf6eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# directory = '/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv'\n",
        "# data_train = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv', encoding='latin-1')\n",
        "# data_dev = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_fixed_test_v3.csv', encoding='latin-1')\n",
        "\n",
        "# data_train = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_train_v3.csv', encoding='latin-1')\n",
        "# data_dev = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_fixed_test_v3.csv', encoding='latin-1')\n",
        "\n",
        "def fix_doc_tgt(doc):\n",
        "\n",
        "  new_doc = doc.replace('.[TGT]',' .\\n[TGT] ')\n",
        "  new_doc = new_doc.replace('\\[TGT\\]','tgt')\n",
        "  return new_doc\n",
        "\n",
        "\n",
        "def load_paragraphs(df):\n",
        "    paragraph_labels = []\n",
        "    paragraph_texts = []\n",
        "    paragraph_ids = []\n",
        "    doc_label = []\n",
        "    num_doc = 0\n",
        "    index = -1\n",
        "    print(len(df['MASKED_DOCUMENT']))\n",
        "    for doc in df['MASKED_DOCUMENT']:\n",
        "      index += 1\n",
        "      docs = doc.split('\\n')\n",
        "      doc_length = len(docs)\n",
        "\n",
        "      if pd.isnull(df['Paragraph0'].iloc[index]):\n",
        "#         Comment the following lines to skip documents with no paragraph labels, I can change it to distant supervision but I don't like :D\n",
        "        paragraph_labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "        paragraph_texts.append(doc.replace('\\[TGT\\]','tgt'))\n",
        "        paragraph_ids.append(index)\n",
        "        doc_label.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "        num_doc +=1\n",
        "        continue\n",
        "      try:\n",
        "        if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "          doc = fix_doc_tgt(doc)\n",
        "          docs = doc.split('\\n')\n",
        "          doc_length = len(docs)\n",
        "          if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "            print('error on document %d'% df['DOCUMENT_INDEX'].iloc[index])\n",
        "            print('document length is %s'%str(doc_length))\n",
        "            continue\n",
        "      except Exception as e:\n",
        "        print('error occured: %s'%str(e))\n",
        "        print('this document has %d paragraphs %d' %(doc_length,df['DOCUMENT_INDEX'].iloc[index]))\n",
        "        \n",
        "      has_null_label = False\n",
        "      for i in range(doc_length):  \n",
        "        if pd.isnull(df['Paragraph%s'%str(i)].iloc[index]):\n",
        "            print('document %d has null paragraph labels in index: %d'%(index,i))\n",
        "            has_null_label = True\n",
        "            break\n",
        "            \n",
        "      if has_null_label:\n",
        "        continue\n",
        "\n",
        "      for i in range(doc_length):\n",
        "        \n",
        "        \n",
        "        paragraph_texts.append(docs[i])\n",
        "        label_i = df['Paragraph%d'%i].iloc[index]\n",
        "        paragraph_labels.append(label_i)\n",
        "        paragraph_ids.append(index)\n",
        "        doc_label.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "    print('number of 1 paragraph docs: %d'%num_doc)\n",
        "    return(paragraph_texts,paragraph_labels,paragraph_ids,doc_label)\n",
        "  \n",
        "\n",
        "# train_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv')\n",
        "# train_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_train_v3.csv')\n",
        "\n",
        "train_par_file = open('/content/train_v5.csv')\n",
        "train_par_df = pd.read_csv(train_par_file)\n",
        "\n",
        "# dev_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_dev_v3.csv')\n",
        "# dev_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_dev_v3.csv')\n",
        "\n",
        "dev_par_file = open('/content/dev_v5.csv')\n",
        "dev_par_df = pd.read_csv(dev_par_file)\n",
        "\n",
        "# test_random_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_random_test_v3.csv')\n",
        "# test_random_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_random_test_v3.csv')\n",
        "\n",
        "test_random_par_file = open('/content/random_test_v5.csv')\n",
        "test_random_par_df = pd.read_csv(test_random_par_file)\n",
        "\n",
        "# test_fixed_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_fixed_test_v3.csv')\n",
        "# test_fixed_par_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_fixed_test_v3.csv')\n",
        "\n",
        "test_fixed_par_file =  open('/content/fixed_test_v5.csv')\n",
        "test_fixed_par_df = pd.read_csv(test_fixed_par_file)\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_paragraphs_file(df):\n",
        "  data = {}\n",
        "  (paragraphs,labels,paragraph_ids,doc_label ) = load_paragraphs(df)\n",
        "  \n",
        "  data[\"sentence\"] = paragraphs\n",
        "  data[\"sentiment\"] =labels\n",
        "  data[\"doc_id\"] = paragraph_ids\n",
        "  data['doc_label'] = doc_label\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset_par(df,index = None):\n",
        "  df_new = load_paragraphs_file(df)\n",
        "  pos_df = df_new[df_new['sentiment'] == 'Positive']\n",
        "  neg_df = df_new[df_new['sentiment'] == 'Negative']\n",
        "  neu_df = df_new[df_new['sentiment'] == 'Neutral']\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = -1\n",
        "  neu_df[\"polarity\"] = 0\n",
        "  \n",
        "  shuffled_out = pd.concat([pos_df, neg_df,neu_df]).sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  return (shuffled_out,df_new)\n",
        "\n",
        "print('-------processing train set ---------')\n",
        "\n",
        "train_par,train_ori = load_dataset_par(train_par_df)\n",
        "print('-------processing dev set ---------')\n",
        "\n",
        "dev_par,dev_ori = load_dataset_par(dev_par_df)\n",
        "print('-------processing random test set ---------')\n",
        "\n",
        "test_par,test_ori = load_dataset_par(test_random_par_df)\n",
        "print('-------processing fixed test set ---------')\n",
        "\n",
        "test_fixed_par,test_fixed_ori = load_dataset_par(test_fixed_par_df)\n",
        "print('Number of train paragraphs %d, Number of dev paragraphs %d,  Number of fixed test paragraphs %d,  Number of random test paragraphs %d, '\n",
        "      %(len(train_par),len(dev_par),len(test_par),len(test_fixed_par)))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------processing train set ---------\n",
            "1501\n",
            "number of 1 paragraph docs: 252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------processing dev set ---------\n",
            "284\n",
            "number of 1 paragraph docs: 45\n",
            "-------processing random test set ---------\n",
            "285\n",
            "document 267 has null paragraph labels in index: 9\n",
            "document 267 has null paragraph labels in index: 10\n",
            "number of 1 paragraph docs: 52\n",
            "-------processing fixed test set ---------\n",
            "340\n",
            "error on document 2171\n",
            "document length is 6\n",
            "number of 1 paragraph docs: 114\n",
            "Number of train paragraphs 9715, Number of dev paragraphs 1926,  Number of fixed test paragraphs 1876,  Number of random test paragraphs 1925, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otr12QcMgW50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c7846ef-4fa0-4611-8cdd-ef1f9047e2e9"
      },
      "source": [
        "print(len(set(test_ori['doc_id'])))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv5GmZbPz8Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_polarity(row):\n",
        "  if row['sentiment'] == 'Positive':\n",
        "    return 1\n",
        "  if row['sentiment'] == 'Negative':\n",
        "    return -1\n",
        "  if row['sentiment'] == 'Neutral':\n",
        "    return 0\n",
        "  \n",
        "train_ori['polarity'] = train_ori.apply(lambda row: set_polarity(row),axis=1)\n",
        "dev_ori['polarity'] = dev_ori.apply(lambda row: set_polarity(row),axis=1)\n",
        "test_ori['polarity'] = test_ori.apply(lambda row: set_polarity(row),axis=1)\n",
        "test_fixed_ori['polarity'] = test_fixed_ori.apply(lambda row: set_polarity(row),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMNSsUHdsbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "label_list = [-1, 0, 1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "train_InputExamples_par = train_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "dev_InputExamples_par = dev_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "dev_features = run_classifier.convert_examples_to_features(dev_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_InputExamples_par = test_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_InputExamples_fixed_par = test_fixed_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_features_fixed = run_classifier.convert_examples_to_features(test_InputExamples_fixed_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "train_ori_InputExample = train_ori.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "train_ori_features = run_classifier.convert_examples_to_features(train_ori_InputExample, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "dev_ori_InputExample = dev_ori.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "dev_ori_features = run_classifier.convert_examples_to_features(dev_ori_InputExample, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "test_ori_InputExample = test_ori.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_ori_features = run_classifier.convert_examples_to_features(test_ori_InputExample, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "test_fixed_ori_InputExample =test_fixed_ori.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_fixed_ori_features = run_classifier.convert_examples_to_features(test_fixed_ori_InputExample, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "# train_par_features = bert.run_classifier.convert_examples_to_features(train_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "# dev_par_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "# test_par_features = bert.run_classifier.convert_examples_to_features(test_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "# test_par_features_fixed = bert.run_classifier.convert_examples_to_features(test_InputExamples_fixed_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "num_train_steps = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 1000 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEcWYt6jjaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu, bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,\n",
        "        bert_hub_module_handle)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
        "        loss = tf.metrics.mean(per_example_loss)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities})\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "def model_train(estimator):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      train_InputExamples_par, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_InputExamples_par)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOlSCl1sj18V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "# model_train(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "  eval_examples = dev_InputExamples_par#processor.get_dev_examples(TASK_DATA_DIR)\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmyd1fRsUjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_eval(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#   eval_accuracy = 0.5473958\n",
        "#   eval_loss = 1.1722633\n",
        "#   global_step = 909\n",
        "#   loss = 1.2637016\n",
        "  \n",
        "  \n",
        "#   ***** Eval results *****\n",
        "#   eval_accuracy = 0.5375\n",
        "#   eval_loss = 1.2887536\n",
        "#   global_step = 1212\n",
        "#   loss = 1.3013718\n",
        "def model_predict(estimator,prediction_examples,input_features,checkpoint_path=None):\n",
        "  # Make predictions on a subset of eval examples\n",
        "#   prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if not checkpoint_path == None: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFdjUfn2Iq-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1411
        },
        "outputId": "e0f0152f-366f-4b2c-f192-f17696e8242b"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "##### Document Level Predictions:\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "def predict_doc_class_by_paragraph_pred(predictions, dev_pred_df,method='avg'):\n",
        "\n",
        "    dev_pred_df['predictions'] = pd.Series([item[1] for item in predictions])\n",
        "    doc_ids = set(dev_pred_df['doc_id'])\n",
        "    document_predicted_class = []\n",
        "    document_true_class = []\n",
        "    for doc_id in doc_ids:\n",
        "#       print('processing document: ', doc_id)\n",
        "      neg = []\n",
        "      neu = []\n",
        "      pos = []\n",
        "      labels = []\n",
        "      true_label = list(dev_pred_df[dev_pred_df['doc_id']==doc_id]['doc_label'])[0]\n",
        "      \n",
        "      probabilities = list(dev_pred_df[dev_pred_df['doc_id']==doc_id]['predictions'])\n",
        "      if pd.isnull(probabilities).any():\n",
        "          print('null paragraph found %d'%doc_id)\n",
        "          continue\n",
        "      for pr in probabilities:\n",
        "\n",
        "        neg.append(pr[0])\n",
        "        neu.append(pr[1])\n",
        "        pos.append(pr[2])\n",
        "        labels.append(np.argmax(pr))\n",
        "      all_class_pres = [np.average(neg),np.average(neu),np.average(pos)]\n",
        "      if method == 'avg' or method == 'both':\n",
        "        predicted_class= np.argmax(all_class_pres)\n",
        "      if method == 'majority' or method == 'both':\n",
        "        predicted_class = max(labels, key=labels.count)\n",
        "      document_true_class.append(true_label)\n",
        "      if predicted_class == 0:\n",
        "        document_predicted_class.append('Negative')\n",
        "      elif predicted_class == 1:\n",
        "        document_predicted_class.append('Neutral')\n",
        "      elif predicted_class == 2:\n",
        "        document_predicted_class.append('Positive')    \n",
        "      else:\n",
        "        document_predicted_class.append(None)\n",
        "    print(metrics.confusion_matrix(y_pred=document_predicted_class,y_true=document_true_class))\n",
        "    print(metrics.classification_report(y_pred=document_predicted_class,y_true = document_true_class))\n",
        "\n",
        "ckpt = 4030\n",
        "####### Previous:\n",
        "### 1212: avg: 46 majority: 44\n",
        "### 4030: avg: 49 majority: 46\n",
        "### 4545: avg: 49 majority: 47 -> fixed: avg: 37 maj: 37 , random:  avg: 32 maj: 31\n",
        "### 4210: avg: 43 majority: 43\n",
        "### 7375: avg: 44 majority: 45 -> majo: fixed: 29, random: 32\n",
        " \n",
        "####### NEW ( all paragraphs)\n",
        "### 1212: avg: 49 majority: 47 \n",
        "### 4030: avg: 54 majority: 52 -> fixed: avg: 43, majority: 44 , random: avg: 46, majority: 45 -> reported in paper\n",
        "### 4545: avg: 54 majority: 52 -> fixed: avg: 42, majority: 41 , random: avg: 48, majority: 47\n",
        "### 4210: avg: 42 majority: 43\n",
        "### 7375: avg: 44 majority: 45 -> \n",
        "\n",
        "####### NEW ( labeled paragraphs)\n",
        "### 1212: avg: 46 majority: 44 \n",
        "### 4030: avg: 52 majority: 49  \n",
        "### 4545: avg: 52 majority: 50 -> \n",
        "### 4210: avg: 41 majority: 42 \n",
        "### 7375: avg: 43 majority: 43 -> \n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,dev_ori_InputExample,dev_ori_features,checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "predict_doc_class_by_paragraph_pred(predictions,dev_ori,method='avg')\n",
        "predict_doc_class_by_paragraph_pred(predictions,dev_pred_df = dev_ori,method='majority')\n",
        "\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_ori_InputExample,test_ori_features,checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "predict_doc_class_by_paragraph_pred(predictions,test_ori,method='avg')\n",
        "predict_doc_class_by_paragraph_pred(predictions,test_ori,method='majority')\n",
        "\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_fixed_ori_InputExample,test_fixed_ori_features,checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "predict_doc_class_by_paragraph_pred(predictions,test_fixed_ori,method='avg')\n",
        "predict_doc_class_by_paragraph_pred(predictions,test_fixed_ori,method='majority')\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "null paragraph found 283\n",
            "[[  7   9   7]\n",
            " [  3  51  43]\n",
            " [  1  44 118]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.30      0.41        23\n",
            "     Neutral       0.49      0.53      0.51        97\n",
            "    Positive       0.70      0.72      0.71       163\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       283\n",
            "   macro avg       0.61      0.52      0.54       283\n",
            "weighted avg       0.62      0.62      0.62       283\n",
            "\n",
            "null paragraph found 283\n",
            "[[  7   8   8]\n",
            " [  4  49  44]\n",
            " [  1  51 111]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.58      0.30      0.40        23\n",
            "     Neutral       0.45      0.51      0.48        97\n",
            "    Positive       0.68      0.68      0.68       163\n",
            "\n",
            "   micro avg       0.59      0.59      0.59       283\n",
            "   macro avg       0.57      0.50      0.52       283\n",
            "weighted avg       0.60      0.59      0.59       283\n",
            "\n",
            "[[  7  10  11]\n",
            " [  3  42  42]\n",
            " [  4  61 104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.25      0.33        28\n",
            "     Neutral       0.37      0.48      0.42        87\n",
            "    Positive       0.66      0.62      0.64       169\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.51      0.45      0.46       284\n",
            "weighted avg       0.56      0.54      0.54       284\n",
            "\n",
            "[[ 7 11 10]\n",
            " [ 3 41 43]\n",
            " [ 6 64 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.44      0.25      0.32        28\n",
            "     Neutral       0.35      0.47      0.40        87\n",
            "    Positive       0.65      0.59      0.62       169\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       284\n",
            "   macro avg       0.48      0.44      0.45       284\n",
            "weighted avg       0.54      0.52      0.52       284\n",
            "\n",
            "null paragraph found 339\n",
            "[[  9  22  19]\n",
            " [  6  47  63]\n",
            " [  4  51 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.47      0.18      0.26        50\n",
            "     Neutral       0.39      0.41      0.40       116\n",
            "    Positive       0.59      0.68      0.63       172\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       338\n",
            "   macro avg       0.48      0.42      0.43       338\n",
            "weighted avg       0.50      0.51      0.50       338\n",
            "\n",
            "null paragraph found 339\n",
            "[[  9  22  19]\n",
            " [  6  51  59]\n",
            " [  5  53 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.45      0.18      0.26        50\n",
            "     Neutral       0.40      0.44      0.42       116\n",
            "    Positive       0.59      0.66      0.63       172\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       338\n",
            "   macro avg       0.48      0.43      0.44       338\n",
            "weighted avg       0.51      0.51      0.50       338\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khuNEnMw2wiv",
        "colab_type": "code",
        "outputId": "e3c296c4-dedc-423c-e855-ad91a2ddcb16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "predictions = model_predict(estimator_from_tfhub,dev_InputExamples_par[:-2],dev_features[:-2],checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_par['sentiment'])[:-2]\n",
        "\n",
        "print('---------------Paragraph-level Dev results-------------')\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_par,test_features,checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_par['sentiment'])\n",
        "print('---------------Paragraph-level Random Test results-------------')\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par[:-1],test_features_fixed[:-1],checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-%d'%ckpt)\n",
        "\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_par['sentiment'])[:-1]\n",
        "print('---------------Paragraph-level Fixed Test results-------------')\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('---------------Document-level results-------------')\n",
        "# doc_labels = []\n",
        "# pred_labels = []\n",
        "# for doc_id in dev_par['doc_id'].unique():\n",
        "#   doc_labels.append(list(dev_par[dev_par['doc_id']==doc_id]['doc_label'])[0])\n",
        "#   ids = dev_par.index[dev_par['doc_id']==doc_id].tolist()\n",
        "#   par_labels = []\n",
        "#   for idd in ids :\n",
        "#     par_labels.append(labels_val[idd])\n",
        "#   mod_label = max(set(par_labels), key=par_labels.count)\n",
        "#   pred_labels.append(mod_label)\n",
        "# dev_par\n",
        "\n",
        "# print(metrics.confusion_matrix(y_pred=pred_labels,y_true=doc_labels))\n",
        "# print(metrics.classification_report(y_pred=pred_labels,y_true = doc_labels))\n",
        "\n",
        "\n",
        "# print(len(doc_labels))\n",
        "# print(len(pred_labels))\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Paragraph-level Dev results-------------\n",
            "[[ 33  69  51]\n",
            " [ 58 360 354]\n",
            " [ 41 348 610]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.22      0.23       153\n",
            "     Neutral       0.46      0.47      0.46       772\n",
            "    Positive       0.60      0.61      0.61       999\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1924\n",
            "   macro avg       0.44      0.43      0.43      1924\n",
            "weighted avg       0.52      0.52      0.52      1924\n",
            "\n",
            "---------------Paragraph-level Random Test results-------------\n",
            "[[ 33  81  65]\n",
            " [ 65 325 320]\n",
            " [ 50 365 572]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.22      0.18      0.20       179\n",
            "     Neutral       0.42      0.46      0.44       710\n",
            "    Positive       0.60      0.58      0.59       987\n",
            "\n",
            "   micro avg       0.50      0.50      0.50      1876\n",
            "   macro avg       0.41      0.41      0.41      1876\n",
            "weighted avg       0.50      0.50      0.49      1876\n",
            "\n",
            "---------------Paragraph-level Fixed Test results-------------\n",
            "[[ 39 100 125]\n",
            " [ 67 311 400]\n",
            " [ 64 317 501]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.15      0.18       264\n",
            "     Neutral       0.43      0.40      0.41       778\n",
            "    Positive       0.49      0.57      0.53       882\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1924\n",
            "   macro avg       0.38      0.37      0.37      1924\n",
            "weighted avg       0.43      0.44      0.43      1924\n",
            "\n",
            "---------------Document-level results-------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par[:1724],checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-4030')\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_par['sentiment'])[:1724]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzzg7MRSk5E9",
        "colab_type": "code",
        "outputId": "66b47480-f5fc-4975-c27e-1b9afe9561ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 31  89 117]\n",
            " [ 57 270 353]\n",
            " [ 56 290 461]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.22      0.13      0.16       237\n",
            "     Neutral       0.42      0.40      0.41       680\n",
            "    Positive       0.50      0.57      0.53       807\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1724\n",
            "   macro avg       0.38      0.37      0.37      1724\n",
            "weighted avg       0.43      0.44      0.43      1724\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5SpMUg7b9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_par[:1780],checkpoint_path = 'gs://bert_example/bert-tfhub/models/smallBERT-15It_allParagraphs/model.ckpt-4030')\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_par['sentiment'])[:1780]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIwoEwmrk6ld",
        "colab_type": "code",
        "outputId": "d11d0b11-d9a0-434d-eb70-6ef07d8644bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 25  77  57]\n",
            " [ 62 307 306]\n",
            " [ 46 352 548]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.19      0.16      0.17       159\n",
            "     Neutral       0.42      0.45      0.44       675\n",
            "    Positive       0.60      0.58      0.59       946\n",
            "\n",
            "   micro avg       0.49      0.49      0.49      1780\n",
            "   macro avg       0.40      0.40      0.40      1780\n",
            "weighted avg       0.49      0.49      0.49      1780\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHBbus84h69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Prediction on Files\n",
        "predictions = model_predict(estimator_from_tfhub,train_ori_InputExample)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "train_ori['prediction'] = pd.Series(labels_val)\n",
        "train_ori.to_csv(open('train_prediction.csv','w'))\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,dev_ori_InputExample)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "dev_ori['prediction'] = pd.Series(labels_val)\n",
        "dev_ori.to_csv(open('dev_prediction.csv','w'))\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_ori_InputExample)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "test_ori['prediction'] = pd.Series(labels_val)\n",
        "test_ori.to_csv(open('test_prediction.csv','w'))\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_fixed_ori_InputExample)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "test_fixed_ori['prediction'] = pd.Series(labels_val)\n",
        "test_fixed_ori.to_csv(open('test_fixed_prediction.csv','w'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PQQUwVE3IEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}