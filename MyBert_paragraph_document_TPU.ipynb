{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyBert_paragraph_document_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/MyBert_paragraph_document_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "outputId": "ed6c111e-2f40-4e80-8ae1-d29a6da0a300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python2.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "('TPU address is', 'grpc://10.100.81.210:8470')\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 101861328127962686),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3074579463015287770),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 6355886190978737158),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7741006640621725586),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2059934109840612536),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8452089884994406552),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14612564297289248146),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8294645066605027610),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3897757234578804245),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8480275243141588705),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5667305348840544088)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "# !test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "# if not 'bert_repo' in sys.path:\n",
        "#   sys.path += ['bert_repo']\n",
        "\n",
        "from bert import modeling\n",
        "from bert import run_classifier_with_tfhub\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "25fac842-2bcf-4144-bf98-77832cb97a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "# OUTPUT_DIR = 'gs://{}/bert-tfhub/models/combined/document/smallBERT-doc_512'.format(BUCKET)\n",
        "OUTPUT_DIR = 'gs://{}/aug19_models/models/sentim_class/smallBERT-docLevel-seq512'.format(BUCKET)\n",
        "\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/aug19_models/models/sentim_class/smallBERT-docLevel-seq512 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\" ##Small Bert\n",
        "# # BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\" ##Big Bert\n",
        "# def create_tokenizer_from_hub_module():\n",
        "#   \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "#   with tf.Graph().as_default():\n",
        "#     bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "#     tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "#     with tf.Session() as sess:\n",
        "#       vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "#                                             tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "#   return bert.tokenization.FullTokenizer(\n",
        "#       vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/%s'%str(BERT_MODEL), 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pl = False\n",
        "dl = True\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10.\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 100\n",
        "SAVE_SUMMARY_STEPS = 150\n",
        "keep_checkpoint_max = 15\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "outputId": "416b4535-c1c0-4803-ac2e-0625d9edd0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# directory = '/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv'\n",
        "# data_train = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv', encoding='latin-1')\n",
        "# data_dev = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_fixed_test_v3.csv', encoding='latin-1')\n",
        "def fix_doc_tgt(doc,index,df):\n",
        "  new_doc = doc.replace('.[TGT]',' .\\n[TGT] ')\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "\n",
        "  new_doc = new_doc.replace('?[TGT]',' ?\\n[TGT] ')\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "  new_doc = new_doc.replace( '[TGT][TGT]', '[TGT] \\n [TGT]' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "\n",
        "  new_doc = new_doc.replace( '. USA TODAY', '. USA TODAY Sports \\n' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  if doc_length == 16 or  not pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "    return new_doc\n",
        "  \n",
        "  new_doc = new_doc.replace('![TGT]','!\\n[TGT]' )\n",
        "  doc_length = len(new_doc.split('\\n'))\n",
        "  return new_doc\n",
        "\n",
        "## if pl is true, append  paragraph leve data and label to the output\n",
        "## if dl is true, append document leve data and label to the output\n",
        "#### Both pl and dl can not be false\n",
        "def load_paragraphs_documents(df,pl=True,dl=True,ent=False,column= 'MASKED_DOCUMENT'):\n",
        "    if not dl and not pl:\n",
        "      print('both document level label and paragaph level label is false, choose one of them a True')\n",
        "      return \n",
        "    labels = []\n",
        "    texts = []\n",
        "    doc_ids = []\n",
        "    uniq_ents = []\n",
        "    num_doc = 0\n",
        "    index = -1\n",
        "    for doc in df[column]:\n",
        "      index += 1\n",
        "      docs = doc.split('\\n')\n",
        "      doc_length = len(docs)\n",
        "\n",
        "      if pd.isnull(df['Paragraph0'].iloc[index]):\n",
        "      # add documents with no paragraph labels as one document and its label to the input data dataframe\n",
        "        if dl:\n",
        "          labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "          texts.append(doc)\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "        num_doc +=1\n",
        "        \n",
        "        continue\n",
        "      try:\n",
        "        if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "         \n",
        "          doc = fix_doc_tgt(doc,index,df)\n",
        "          docs = doc.split('\\n')\n",
        "          \n",
        "          doc_length = len(docs)\n",
        "          if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "            # print(doc)\n",
        "            if column == 'summary':\n",
        "              pass\n",
        "            else:\n",
        "              print('error on document %d'% df['DOCUMENT_INDEX'].iloc[index])\n",
        "              print('document length is %s'%str(doc_length))\n",
        "              continue\n",
        "\n",
        "      except Exception as e:\n",
        "        print('err is %s'%str(e))\n",
        "        print('this document has %d paragraphs %d' %(doc_length,df['DOCUMENT_INDEX'].iloc[index]))\n",
        "\n",
        "      if pl:\n",
        "        for i in range(doc_length):\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "          \n",
        "          texts.append(docs[i])\n",
        "          label_i = df['Paragraph%d'%i].iloc[index]\n",
        "          labels.append(label_i)\n",
        "        ### increase the effect of documents by adding the whole document per each paragraph :D\n",
        "          if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "      else:\n",
        "         if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "         \n",
        "    print('number of one-paragraph docs: %d'%num_doc)\n",
        "    return(texts,labels,doc_ids,uniq_ents)\n",
        "  \n",
        "\n",
        "# train_file = open('/content/train_v5.csv')\n",
        "# # train_file = open('/content/train_v4.csv')\n",
        "# train_df = pd.read_csv(train_file)\n",
        "\n",
        "# # dev_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_dev_v3.csv')\n",
        "# dev_file = open('/content/dev_v5.csv')\n",
        "# dev_df = pd.read_csv(dev_file)\n",
        "\n",
        "# # test_random_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_random_test_v3.csv')\n",
        "# test_random_file = open('/content/random_test_v5.csv')\n",
        "# test_random_df = pd.read_csv(test_random_file)\n",
        "\n",
        "# # test_fixed_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_fixed_test_v3.csv')\n",
        "# test_fixed_file = open('/content/fixed_test_v5.csv')\n",
        "# test_fixed_df = pd.read_csv(test_fixed_file)\n",
        "\n",
        "\n",
        "# train_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/train_v5.csv'), encoding='latin-1')\n",
        "# dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/dev_v5.csv'), encoding='latin-1')\n",
        "# test_random_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/random_test_v5.csv'), encoding='latin-1')\n",
        "# test_fixed_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/fixed_test_v5.csv'), encoding='latin-1')\n",
        "\n",
        "# train_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/train_v5.csv'), encoding='latin-1')\n",
        "# dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/dev_entities_v5.csv'), encoding='latin-1')\n",
        "# test_random_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/random_test_entities_v5.csv'), encoding='latin-1')\n",
        "# test_fixed_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/fix_test_entities_v5.csv'), encoding='latin-1')\n",
        "\n",
        "\n",
        "data_pref = 'gs://bert_example/data_aug19/all_data_combined_shuffled_3Dec_7Dec_aug19_%s.csv'\n",
        "train_df = pd.read_csv(tf.gfile.GFile(data_pref % str('train')), encoding='latin-1')\n",
        "# dev_df = pd.read_csv(tf.gfile.GFile(data_pref % str('dev')), encoding='latin-1')\n",
        "dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data_aug19/abstractive_pgn_summary_dev_comma.csv'), encoding='latin-1')\n",
        "test_random_df= pd.read_csv(tf.gfile.GFile(data_pref % str('random_test')), encoding='latin-1')\n",
        "test_fixed_df= pd.read_csv(tf.gfile.GFile(data_pref % str('fixed_test')), encoding='latin-1')\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_file(df,dl=True,pl=True,ent=False,column= 'MASKED_DOCUMENT'):\n",
        "  data = {}\n",
        "  (texts,labels,doc_ids,uniq_ent ) = load_paragraphs_documents(df,dl=dl,pl=pl,column= column)\n",
        "  data[\"sentence\"] = texts\n",
        "  data[\"sentiment\"] =labels\n",
        "  data[\"doc_id\"] = doc_ids\n",
        "  if ent:\n",
        "    data[\"uniq_ent\"] = uniq_ent\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None,dl=True,pl=True,column= 'MASKED_DOCUMENT'):\n",
        "  df_new = load_file(df,dl,pl,column= column)\n",
        "  pos_df = df_new[df_new['sentiment'] == 'Positive']\n",
        "  neg_df = df_new[df_new['sentiment'] == 'Negative']\n",
        "  neu_df = df_new[df_new['sentiment'] == 'Neutral']\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = -1\n",
        "  neu_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df,neu_df]).sample(frac=1).reset_index(drop=True)\n",
        "# train_all,dev_par,dev_doc,test_par,test_doc,test_fixed_par,test_fixed_doc= [],[],[],[],[],[],[]\n",
        "### train should consist both paragraph level and document level labels\n",
        "print('processing train set')\n",
        "# train_all = load_dataset(train_df,dl=True,pl=True)\n",
        "# train_par = load_dataset(train_df,dl=dl,pl=pl)\n",
        "train_doc = load_dataset(train_df,dl=dl,pl=pl)\n",
        "### two dev set, to test the model with paragraph level dev and document level dev\n",
        "print('processing dev set')\n",
        "# dev_par = load_dataset(dev_df,pl=pl,dl=dl)\n",
        "dev_doc = load_dataset(dev_df,pl=pl,dl=dl)\n",
        "# ### two random tests like above\n",
        "dev_summary = load_dataset(dev_df,pl=pl,dl=dl,column= 'summary')#'summarised')\n",
        "\n",
        "print('processing random test set')\n",
        "# test_par = load_dataset(test_random_df,pl=pl,dl=dl)\n",
        "test_doc = load_dataset(test_random_df,pl=pl,dl=dl)\n",
        "# ### two fixed tests like above\n",
        "print('processing fixed test set')\n",
        "# test_fixed_par = load_dataset(test_fixed_df,pl=pl,dl=dl)\n",
        "test_fixed_doc = load_dataset(test_fixed_df,pl=pl,dl=dl)\n",
        "\n",
        "# print('Here the input consists of both paragraph level data and document level data')\n",
        "# print('Number of train inputs: %d\\n Number of paragraph dev inputs: %d,Number of document level dev inputs: %d\\n  \\\\\n",
        "#        Number of paragraph level fixed test inputs: %d, Number of document level fixed test inputs: %d \\n \\\\\n",
        "#        Number of paragraph level random test inputs: %d,  Number of document level random test inputs: %d'\n",
        "#       %(len(train_doc),len(dev_par),len(dev_doc),len(test_par),len(test_doc),len(test_fixed_par),len(test_fixed_doc)))\n",
        "print('Number of train inputs: %d\\n \\\\\n",
        "        Number of document level dev inputs: %d\\n  \\\\\n",
        "        Number of document level summarized dev inputs: %d\\n  \\\\\n",
        "        Number of document level fixed test inputs: %d \\n \\\\\n",
        "       Number of document level random test inputs: %d'\n",
        "      %(len(train_doc),len(dev_doc),len(dev_summary),len(test_fixed_doc),len(test_doc)))\n",
        "\n",
        "\n",
        "# print('Number of train inputs: %d\\n Number of document level dev inputs: %d\\n  \\\\\n",
        "#         Number of document level fixed test inputs: %d \\n \\\\\n",
        "#        Number of document level random test inputs: %d'\n",
        "#       %(len(train_par),len(dev_par),len(test_fixed_par),len(test_par)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing train set\n",
            "number of one-paragraph docs: 252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:168: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:169: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "processing dev set\n",
            "number of one-paragraph docs: 45\n",
            "number of one-paragraph docs: 45\n",
            "processing random test set\n",
            "number of one-paragraph docs: 52\n",
            "processing fixed test set\n",
            "number of one-paragraph docs: 114\n",
            "Number of train inputs: 3355\n",
            " \\        Number of document level dev inputs: 578\n",
            "  \\        Number of document level summarized dev inputs: 578\n",
            "  \\        Number of document level fixed test inputs: 827 \n",
            " \\       Number of document level random test inputs: 579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMNSsUHdsbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "label_list = [-1, 0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "# train_InputExamples_all = train_all.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# train_InputExamples_par = train_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "train_InputExamples_doc = train_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "# dev_InputExamples_par = dev_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_InputExamples_doc = dev_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_summ_InputExamples_doc = dev_summary.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "# test_InputExamples_par = test_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples_doc = test_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# test_InputExamples_fixed_par = test_fixed_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_InputExamples_fixed_doc = test_fixed_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "# num_train_steps = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "# num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEcWYt6jjaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu, bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,\n",
        "        bert_hub_module_handle)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
        "        loss = tf.metrics.mean(per_example_loss)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities})\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "outputId": "b80180d1-f3e7-40b0-e1cc-9b13e440e479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "# num_train_steps = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_train_steps = int(len(train_InputExamples_doc) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 1000 # I don't know what it is doing just decrease it to smaller value\n",
        "# ITERATIONS_PER_LOOP = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples_doc) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    keep_checkpoint_max=keep_checkpoint_max,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1113 15:23:09.979496 140020120303488 estimator.py:1994] Estimator's model_fn (<function model_fn at 0x7f58b54f2758>) includes params argument, but params are not passed to Estimator.\n",
            "I1113 15:23:09.981550 140020120303488 estimator.py:212] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.100.81.210:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 15, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f58b4f42c50>, '_model_dir': 'gs://bert_example/aug19_models/models/sentim_class/smallBERT-docLevel-seq512', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=104, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f58b4f42f50>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.100.81.210:8470', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': u'grpc://10.100.81.210:8470'}\n",
            "I1113 15:23:09.982992 140020120303488 tpu_context.py:220] _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "def model_train(estimator):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  \n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_InputExamples_doc)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOlSCl1sj18V",
        "colab_type": "code",
        "outputId": "42f36e8f-aad3-4e69-bc2e-db9f3a447856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "model_train(estimator_from_tfhub)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started training at 2019-11-13 15:26:37.338513 *****\n",
            "  Num examples = 3355\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-11-13 15:26:38.917681 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "397f3e66-4d91-46fa-bfd4-7503b69c096e"
      },
      "source": [
        "# eval_examples = dev_InputExamples_doc\n",
        "# eval_features = run_classifier.convert_examples_to_features(\n",
        "#       eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "eval_summ_features = run_classifier.convert_examples_to_features(dev_summ_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1113 15:48:18.053637 140020120303488 run_classifier.py:774] Writing example 0 of 578\n",
            "I1113 15:48:18.060069 140020120303488 run_classifier.py:461] *** Example ***\n",
            "I1113 15:48:18.061224 140020120303488 run_classifier.py:462] guid: None\n",
            "I1113 15:48:18.064244 140020120303488 run_classifier.py:464] tokens: [CLS] sen . ki ##rsten gill ##ib ##rand - l ##rb - d - n . y . - rr ##b - who has been a leader on raising awareness about the epidemic of sexual misconduct was the first to call on frank ##en to go . i have been shocked and disappointed to learn over the last few weeks that a colleague i am fond of personally has engaged in behavior towards women that [SEP]\n",
            "I1113 15:48:18.066612 140020120303488 run_classifier.py:465] input_ids: 101 12411 1012 11382 19020 12267 12322 13033 1011 1048 15185 1011 1040 1011 1050 1012 1061 1012 1011 25269 2497 1011 2040 2038 2042 1037 3003 2006 6274 7073 2055 1996 16311 1997 4424 23337 2001 1996 2034 2000 2655 2006 3581 2368 2000 2175 1012 1045 2031 2042 7135 1998 9364 2000 4553 2058 1996 2197 2261 3134 2008 1037 11729 1045 2572 13545 1997 7714 2038 5117 1999 5248 2875 2308 2008 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.068242 140020120303488 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.070606 140020120303488 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.072505 140020120303488 run_classifier.py:468] label: 1 (id = 2)\n",
            "I1113 15:48:18.080074 140020120303488 run_classifier.py:461] *** Example ***\n",
            "I1113 15:48:18.081110 140020120303488 run_classifier.py:462] guid: None\n",
            "I1113 15:48:18.082761 140020120303488 run_classifier.py:464] tokens: [CLS] ' ' most international tickets will be allocated through individual national olympic committees , ` ` seems to me what we ' ve got on offer here is a spectacular proposition that it ' s absolutely our ambition to have this full ' ' london organizing committee chief executive paul dei ##ghton said in a conference call . ' ' most international tickets will put in their applications next autumn with tickets also going on sale in those countries in the spring of 2011 ` ` [SEP]\n",
            "I1113 15:48:18.084652 140020120303488 run_classifier.py:465] input_ids: 101 1005 1005 2087 2248 9735 2097 2022 11095 2083 3265 2120 4386 9528 1010 1036 1036 3849 2000 2033 2054 2057 1005 2310 2288 2006 3749 2182 2003 1037 12656 14848 2008 2009 1005 1055 7078 2256 16290 2000 2031 2023 2440 1005 1005 2414 10863 2837 2708 3237 2703 14866 21763 2056 1999 1037 3034 2655 1012 1005 1005 2087 2248 9735 2097 2404 1999 2037 5097 2279 7114 2007 9735 2036 2183 2006 5096 1999 2216 3032 1999 1996 3500 1997 2249 1036 1036 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.086955 140020120303488 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.088566 140020120303488 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.090559 140020120303488 run_classifier.py:468] label: 1 (id = 2)\n",
            "I1113 15:48:18.095331 140020120303488 run_classifier.py:461] *** Example ***\n",
            "I1113 15:48:18.096988 140020120303488 run_classifier.py:462] guid: None\n",
            "I1113 15:48:18.099066 140020120303488 run_classifier.py:464] tokens: [CLS] harvey wei ##nstein showers and sex enraged him she wrote hay ##ek . fr ##ida ka ##hl ##o bio ##pic fr ##ida ' ' into a nightmare after the actress refused wei ##nstein ' s relentless advances ` ` for years he was my monster ' ' hay ##ek wrote in an op - ed published wednesday by the new york times , ` ` she says . [SEP]\n",
            "I1113 15:48:18.100779 140020120303488 run_classifier.py:465] input_ids: 101 7702 11417 15493 23442 1998 3348 18835 2032 2016 2626 10974 5937 1012 10424 8524 10556 7317 2080 16012 24330 10424 8524 1005 1005 2046 1037 10103 2044 1996 3883 4188 11417 15493 1005 1055 21660 9849 1036 1036 2005 2086 2002 2001 2026 6071 1005 1005 10974 5937 2626 1999 2019 6728 1011 3968 2405 9317 2011 1996 2047 2259 2335 1010 1036 1036 2016 2758 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.102899 140020120303488 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.104505 140020120303488 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.106443 140020120303488 run_classifier.py:468] label: 1 (id = 2)\n",
            "I1113 15:48:18.110924 140020120303488 run_classifier.py:461] *** Example ***\n",
            "I1113 15:48:18.112432 140020120303488 run_classifier.py:462] guid: None\n",
            "I1113 15:48:18.114074 140020120303488 run_classifier.py:464] tokens: [CLS] world daughter of putin ' s former announces presidential run daughter of putin ' s former announces presidential run listen 3 : 11 3 : 11 . her father ana ##to ##ly sob ##cha ##k was mayor of st . petersburg and a mentor to vladimir putin . in the same way putin always wins . [SEP]\n",
            "I1113 15:48:18.116163 140020120303488 run_classifier.py:465] input_ids: 101 2088 2684 1997 22072 1005 1055 2280 17472 4883 2448 2684 1997 22072 1005 1055 2280 17472 4883 2448 4952 1017 1024 2340 1017 1024 2340 1012 2014 2269 9617 3406 2135 17540 7507 2243 2001 3664 1997 2358 1012 8062 1998 1037 10779 2000 8748 22072 1012 1999 1996 2168 2126 22072 2467 5222 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.117741 140020120303488 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.119445 140020120303488 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.122005 140020120303488 run_classifier.py:468] label: 1 (id = 2)\n",
            "I1113 15:48:18.128177 140020120303488 run_classifier.py:461] *** Example ***\n",
            "I1113 15:48:18.130414 140020120303488 run_classifier.py:462] guid: None\n",
            "I1113 15:48:18.132296 140020120303488 run_classifier.py:464] tokens: [CLS] backlash has brought unwanted attention to tai ##mei elementary which dates its history from the late 19th century and whose well - known ivy - covered school building is just blocks away from tokyo ' s most expensive real estate that the uniforms would be designed by arm ##ani in november when wa ##da sent a letter to the school community announcing his decision and justify ##ing the luxury children ##sw ##ear by dee ##ming the school a gin ##za landmark . [SEP]\n",
            "I1113 15:48:18.133934 140020120303488 run_classifier.py:465] input_ids: 101 25748 2038 2716 18162 3086 2000 13843 26432 4732 2029 5246 2049 2381 2013 1996 2397 3708 2301 1998 3005 2092 1011 2124 7768 1011 3139 2082 2311 2003 2074 5991 2185 2013 5522 1005 1055 2087 6450 2613 3776 2008 1996 11408 2052 2022 2881 2011 2849 7088 1999 2281 2043 11333 2850 2741 1037 3661 2000 1996 2082 2451 13856 2010 3247 1998 16114 2075 1996 9542 2336 26760 14644 2011 9266 6562 1996 2082 1037 18353 4143 8637 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.136732 140020120303488 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.138962 140020120303488 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I1113 15:48:18.140618 140020120303488 run_classifier.py:468] label: 0 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmyd1fRsUjr",
        "colab_type": "code",
        "outputId": "c0ff7d78-892e-4dfe-8ab5-cf9254f0471b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_eval(estimator_from_tfhub)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Started evaluation at 2019-11-13 15:27:14.560157 *****\n",
            "  Num examples = 578\n",
            "  Batch size = 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E1113 15:27:20.187311 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.188952 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.192251 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.195204 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.205626 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.214994 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.224004 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.229969 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.232691 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.251209 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.254153 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.260236 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.263376 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.269809 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.272490 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.285149 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.287801 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.292447 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.295392 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.304181 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.308514 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.317105 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.319725 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.324472 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.327429 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.338356 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.341063 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.347012 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.349807 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.355308 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.357862 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.370855 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.373500 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.378130 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.381115 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.393474 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.399079 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.407629 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.410525 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.419162 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.421902 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.433759 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.439052 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.445072 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.448043 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.454899 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.457516 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.471586 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.475320 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.479933 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.482953 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.493200 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.497107 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.506613 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.509663 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.514358 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.518228 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.529900 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.533982 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.540261 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.544033 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.550415 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.552994 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.566591 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.569243 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.575557 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.580718 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.589077 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.592555 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.602340 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.604911 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.611346 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.613991 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.628040 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.631114 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.638668 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.642183 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.649487 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.652566 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.668869 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.671797 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.677521 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.681366 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.690160 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.694449 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.703376 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.707936 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.713155 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.717516 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.728308 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.731657 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.738657 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.742182 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.748999 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.752352 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.765877 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.769927 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.775909 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.779844 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.788085 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.795783 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.807892 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.811820 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.818037 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.821943 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.835270 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.838247 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.845768 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.848705 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.855907 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.858809 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.875901 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.880085 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.886234 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.890631 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:20.899208 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.154278 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.163662 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.166635 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.170999 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.174020 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.186794 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.189562 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.196001 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.198694 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.205029 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.207721 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.220592 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.223375 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.228444 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.232271 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.241401 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.244998 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.253583 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.256515 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.262275 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.265026 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.275022 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.278045 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.284267 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.287077 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.292855 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.295680 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.307898 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.310792 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.315716 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.319895 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.327862 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.331660 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.340775 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.343871 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.350184 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.353797 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.364805 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.368344 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.376873 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.379957 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.388060 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.391853 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.408200 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.411813 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.418236 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.422589 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.432423 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.436343 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.446367 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.449567 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.456543 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.459882 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.470525 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.474176 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.481569 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.484992 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.492366 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.495569 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.508938 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.512985 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.519550 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.522715 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.532263 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.536483 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.546494 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.549993 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.556062 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.559190 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.570633 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.574031 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.580939 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.584211 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.590831 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.594307 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.607153 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.611217 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.617264 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.620321 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.629362 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.633317 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.641963 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.644648 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.651808 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.654536 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.686950 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.690052 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.702845 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.705612 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.713888 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.716624 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:27:21.724601 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Finished evaluation at 2019-11-13 15:28:12.049013 *****\n",
            "***** Eval results *****\n",
            "  eval_accuracy = 0.5555556\n",
            "  eval_loss = 2.8318007\n",
            "  global_step = 1040\n",
            "  loss = 3.1217384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model_predict(estimator,prediction_examples,input_features,checkpoint_path=None):\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascfbXXbzEpw",
        "colab_type": "code",
        "outputId": "f1776cca-418f-42b6-dd57-69f8ab1b30c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features_fixed = run_classifier.convert_examples_to_features(test_InputExamples_fixed_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,eval_examples,eval_summ_features,checkpoint_path=OUTPUT_DIR+'/'+'best_model'+'/model.ckpt-728')\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,test_features,checkpoint_path=OUTPUT_DIR+'/'+'Best_Model_51Dev'+'/model.ckpt-184')\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,test_features_fixed,checkpoint_path=OUTPUT_DIR+'/'+'Best_Model_51Dev'+'/model.ckpt-184')\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_fixed_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E1113 15:49:29.307502 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.311620 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.313576 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.315237 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.322381 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.332634 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.341475 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.348977 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.352421 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.370599 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.373562 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.381098 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.384471 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.391161 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.394673 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.407799 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.412302 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.418844 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.422714 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.431760 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.435899 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.444866 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.448471 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.454524 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.457884 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.468529 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.471546 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.479310 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.483016 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.489964 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.493465 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.508610 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.512108 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.518752 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.523160 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.532483 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.536698 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.548718 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.551930 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.558408 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.562670 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.576333 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.579180 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.586862 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.590188 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.598156 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.601524 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.615295 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.618172 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.624819 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.628020 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.637028 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.640502 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.650459 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.653273 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.659869 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.663108 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.674308 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.677191 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.685414 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.688370 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.696069 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.698992 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.715390 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.720165 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.725683 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.730072 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.740431 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.743715 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.754508 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.759049 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.764913 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.767594 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.779716 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.783185 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.790050 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.793085 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.799905 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.802752 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.815952 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.819356 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.825836 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.830363 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.843924 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.847917 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.861251 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.864866 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.870867 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.875555 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.896974 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.901360 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.908224 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.911165 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.918673 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.921736 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.936660 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.939637 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.946751 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.949629 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.961067 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.965544 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.975622 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.978698 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.985943 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:29.989361 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.000874 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.004642 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.012331 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.015582 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.022644 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.025475 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.039767 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.042695 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.050476 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.053916 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.063698 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.067883 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.079195 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.082029 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.088911 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.092243 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.104763 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.109383 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.115788 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.118518 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.125092 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.127815 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.141558 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.144335 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.150669 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.153487 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.162833 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.166529 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.174830 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.179414 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.185770 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.189085 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.199979 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.203037 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.210752 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.214204 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.221699 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.225157 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.238159 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.241791 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.249000 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.252866 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.267220 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.274753 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.284842 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.289087 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.295414 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.299112 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.310434 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.313394 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.321050 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.324606 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.331640 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.335194 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.352736 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.356389 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.363909 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.368433 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.378614 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.383528 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.395220 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.399075 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.406524 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.410252 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.421603 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.425466 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.434441 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.439163 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.448048 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.452164 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.467864 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.471249 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.478308 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.481861 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.491636 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.496426 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.505805 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.510056 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.516999 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.520323 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.533243 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.537007 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.544564 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.548274 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.555470 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.558988 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.573136 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.576807 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.584619 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.587975 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.598546 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.602931 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.617091 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.620717 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.627545 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.630722 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.661947 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.665616 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.678306 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.681804 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.691302 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.694488 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E1113 15:49:30.703658 140020120303488 tpu.py:425] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  7  32  19]\n",
            " [ 35 119  62]\n",
            " [ 31 176  97]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.10      0.12      0.11        58\n",
            "     Neutral       0.36      0.55      0.44       216\n",
            "    Positive       0.54      0.32      0.40       304\n",
            "\n",
            "   micro avg       0.39      0.39      0.39       578\n",
            "   macro avg       0.33      0.33      0.32       578\n",
            "weighted avg       0.43      0.39      0.39       578\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception tensorflow.python.framework.errors_impl.CancelledError: CancelledError() in <generator object predict at 0x7f58b4050500> ignored\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kgZn8x0w-2",
        "colab_type": "code",
        "outputId": "e74bbd64-15a5-4964-c056-9f298ea1e165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# chkpth = 'gs://bert_example/bert-tfhub/models/combined/weighted_loss/smallBERT-parDoc_256/kept_models/model.ckpt-5990'\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "iterations_per_epoch = 748\n",
        "for i in range(7480,14961,iterations_per_epoch):\n",
        "  print('starting epoch %s'%str(i/iterations_per_epoch))\n",
        "  predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%s'%str(i))\n",
        "  labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "# chkpnt = 3*iterations_per_epoch\n",
        "\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%s'%str(chkpnt))\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(dev_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,test_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%s'%str(chkpnt))\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,test_features_fixed,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%s'%str(chkpnt))\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_fixed_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "### best model for document level: epoch 7, dev: 47, random test: 48, fixed test: 42\n",
        "\n",
        "### best model for paragraph level: epoch 3, dev: 50, random test: 43, fixed test: 35"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting epoch 10\n",
            "[[ 10  24  24]\n",
            " [ 16 114  86]\n",
            " [ 11 139 154]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.27      0.17      0.21        58\n",
            "     Neutral       0.41      0.53      0.46       216\n",
            "    Positive       0.58      0.51      0.54       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.42      0.40      0.41       578\n",
            "weighted avg       0.49      0.48      0.48       578\n",
            "\n",
            "starting epoch 11\n",
            "[[ 10  26  22]\n",
            " [ 20 124  72]\n",
            " [ 13 145 146]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.17      0.20        58\n",
            "     Neutral       0.42      0.57      0.49       216\n",
            "    Positive       0.61      0.48      0.54       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.42      0.41      0.41       578\n",
            "weighted avg       0.50      0.48      0.48       578\n",
            "\n",
            "starting epoch 12\n",
            "[[  5  32  21]\n",
            " [ 11 124  81]\n",
            " [  6 136 162]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.09      0.13        58\n",
            "     Neutral       0.42      0.57      0.49       216\n",
            "    Positive       0.61      0.53      0.57       304\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       578\n",
            "   macro avg       0.42      0.40      0.39       578\n",
            "weighted avg       0.50      0.50      0.49       578\n",
            "\n",
            "starting epoch 13\n",
            "[[  8  29  21]\n",
            " [ 15 123  78]\n",
            " [ 10 138 156]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.24      0.14      0.18        58\n",
            "     Neutral       0.42      0.57      0.49       216\n",
            "    Positive       0.61      0.51      0.56       304\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       578\n",
            "   macro avg       0.43      0.41      0.41       578\n",
            "weighted avg       0.50      0.50      0.49       578\n",
            "\n",
            "starting epoch 14\n",
            "[[ 15  24  19]\n",
            " [ 25 114  77]\n",
            " [ 14 147 143]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.28      0.26      0.27        58\n",
            "     Neutral       0.40      0.53      0.46       216\n",
            "    Positive       0.60      0.47      0.53       304\n",
            "\n",
            "   micro avg       0.47      0.47      0.47       578\n",
            "   macro avg       0.43      0.42      0.42       578\n",
            "weighted avg       0.49      0.47      0.47       578\n",
            "\n",
            "starting epoch 15\n",
            "[[ 14  28  16]\n",
            " [ 19 134  63]\n",
            " [ 12 162 130]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.31      0.24      0.27        58\n",
            "     Neutral       0.41      0.62      0.50       216\n",
            "    Positive       0.62      0.43      0.51       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.45      0.43      0.42       578\n",
            "weighted avg       0.51      0.48      0.48       578\n",
            "\n",
            "starting epoch 16\n",
            "[[  7  27  24]\n",
            " [ 17 110  89]\n",
            " [  7 135 162]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.12      0.16        58\n",
            "     Neutral       0.40      0.51      0.45       216\n",
            "    Positive       0.59      0.53      0.56       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.41      0.39      0.39       578\n",
            "weighted avg       0.48      0.48      0.48       578\n",
            "\n",
            "starting epoch 17\n",
            "[[ 15  21  22]\n",
            " [ 22 113  81]\n",
            " [ 10 138 156]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.32      0.26      0.29        58\n",
            "     Neutral       0.42      0.52      0.46       216\n",
            "    Positive       0.60      0.51      0.55       304\n",
            "\n",
            "   micro avg       0.49      0.49      0.49       578\n",
            "   macro avg       0.45      0.43      0.43       578\n",
            "weighted avg       0.50      0.49      0.49       578\n",
            "\n",
            "starting epoch 18\n",
            "[[ 12  25  21]\n",
            " [ 21 118  77]\n",
            " [ 14 140 150]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.26      0.21      0.23        58\n",
            "     Neutral       0.42      0.55      0.47       216\n",
            "    Positive       0.60      0.49      0.54       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.43      0.42      0.41       578\n",
            "weighted avg       0.50      0.48      0.49       578\n",
            "\n",
            "starting epoch 19\n",
            "[[  9  22  27]\n",
            " [ 20 107  89]\n",
            " [ 12 128 164]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.22      0.16      0.18        58\n",
            "     Neutral       0.42      0.50      0.45       216\n",
            "    Positive       0.59      0.54      0.56       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.41      0.40      0.40       578\n",
            "weighted avg       0.49      0.48      0.48       578\n",
            "\n",
            "starting epoch 20\n",
            "[[ 10  23  25]\n",
            " [ 22 113  81]\n",
            " [ 12 135 157]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.17      0.20        58\n",
            "     Neutral       0.42      0.52      0.46       216\n",
            "    Positive       0.60      0.52      0.55       304\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       578\n",
            "   macro avg       0.41      0.40      0.40       578\n",
            "weighted avg       0.49      0.48      0.48       578\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fiDTLoKMfg",
        "colab_type": "code",
        "outputId": "f823619b-edf1-4d9c-c48d-b83d17580c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# print(labels_val)\n",
        "# print(list(dev_doc['sentiment']))\n",
        "# print(list(dev_doc['doc_id']))\n",
        "# print(list(dev_doc['uniq_ent']))\n",
        "len(list(labels_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7J13niRqOYj",
        "colab_type": "text"
      },
      "source": [
        "#Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdkAzUuvqbGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Error Analysis\n",
        "### length of paragraph vs accuracy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "ind = 0\n",
        "negative_info = {}\n",
        "positive_info = {}\n",
        "neutral_info  = {}\n",
        "try:\n",
        "  ### if we are predicting in the current file and just read the prediction\n",
        "  doc_ids = list(dev_doc['doc_id'])\n",
        "except:\n",
        "  #### if we are predicting in another file and loadin that from content\n",
        "  dev_doc = pd.read_csv('mask_senti_prediction.csv')\n",
        "  doc_ids= list(dev_doc['doc_id'])\n",
        "  labels_val = list(dev_doc['predicted'])\n",
        "\n",
        "for item in list(dev_doc['sentiment']):\n",
        "  if item == 'Negative':\n",
        "    negative_info[doc_ids[ind]] = labels_val[ind]\n",
        "  elif item == 'Positive':\n",
        "    positive_info[doc_ids[ind]] = labels_val[ind]\n",
        "  elif item == 'Neutral':\n",
        "    neutral_info[doc_ids[ind]] = labels_val[ind]\n",
        "  ind += 1\n",
        "  \n",
        "### input all document in specific category based on the document id and output a map which has the document length and the predicted label\n",
        "def doc_length_label(splitter='\\n',documents =negative_info ):\n",
        "  doc_lengths_label_map = {}\n",
        "  for doc_id in documents.keys():\n",
        "    length = len(list(dev_doc[dev_doc['doc_id']==doc_id]['sentence'])[0].split(splitter))\n",
        "    if length in doc_lengths_label_map:\n",
        "      doc_lengths_label_map[length].append(documents[doc_id])\n",
        "    else:\n",
        "      doc_lengths_label_map[length] = [documents[doc_id]]\n",
        "  return doc_lengths_label_map\n",
        "\n",
        "### input all document in specific category based on the document id and output a map which has the number of unique entities and the predicted label\n",
        "def unique_entity_label(splitter='\\n',documents =negative_info ):\n",
        "  unique_entity_label_map = {}\n",
        "  for doc_id in documents.keys():\n",
        "    num = list(dev_doc[dev_doc['doc_id']==doc_id]['uniq_ent'])[0]\n",
        "    if num in unique_entity_label_map:\n",
        "      unique_entity_label_map[num].append(documents[doc_id])\n",
        "    else:\n",
        "      unique_entity_label_map[num] = [documents[doc_id]]\n",
        "  return unique_entity_label_map\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "### plot the number of correctly classified document in each category based on the number of paragraphs/words in the document\n",
        "### input: splitter: to choose whether to plot based on the number of paragraphs or words\n",
        "########## bar range: to choose the length which should be in the same bar\n",
        "########## label: \"paragraph\" or \"word\" based on the splitter\n",
        "### output: the plots which show what percentage of documents in each range of paragraphs/words length are classified correctly \n",
        "\n",
        "def plot_correctly_classified(splitter='\\n',bar_range= [1,5,7,10,17],label='Number of Paragraphs in a Document',method_plot=doc_length_label,plot='bar'): \n",
        "  doc_lengths_label_negs = method_plot(splitter=splitter,documents =negative_info )\n",
        "  doc_lengths_label_pos = method_plot(splitter=splitter,documents =positive_info )\n",
        "  doc_lengths_label_neut = method_plot(splitter=splitter,documents =neutral_info )\n",
        "#   print('injaaaa>>>',doc_lengths_label_neut)\n",
        "\n",
        "\n",
        "  \n",
        "  maxes = bar_range\n",
        "  print('Positives')\n",
        "  categories_po = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_pos.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_po[j-1].extend(doc_lengths_label_pos[i])\n",
        "  for i in categories_po:\n",
        "    print(len(i))\n",
        "\n",
        "  print('Neutrals')\n",
        "  categories_neu = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_neut.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_neu[j-1].extend(doc_lengths_label_neut[i])\n",
        "  for i in categories_neu:\n",
        "    print(len(i))\n",
        "  print('oooonjaaa>>>',categories_neu)\n",
        "\n",
        "  print('Negatives')\n",
        "  categories_neg = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_negs.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_neg[j-1].extend(doc_lengths_label_negs[i])\n",
        "  for i in categories_neg:\n",
        "    print(len(i))\n",
        "    \n",
        "  print('Total')  \n",
        "  for i,j,k in zip(categories_neg,categories_neu,categories_po):\n",
        "    print(len(i+j+k))\n",
        "\n",
        "  bar_pos = []\n",
        "  bar_neu = []\n",
        "  bar_neg = []\n",
        "  bar_mac_f1= []\n",
        "  for i in range(len(maxes)-1):\n",
        "    bar_pos.append(float(categories_po[i].count('Positive'))/len(categories_po[i]))\n",
        "    bar_neu.append(float(categories_neu[i].count('Neutral'))/len(categories_neu[i]))\n",
        "#     bar_neg.append(float(categories_neg[i].count('Negative'))/len(categories_neg[i]))\n",
        "    true_label = [\"Positive\" for _ in categories_po[i]]\n",
        "    true_label.extend([\"Neutral\" for _ in categories_neu[i]])\n",
        "    true_label.extend([\"Negative\" for _ in categories_neg[i]])\n",
        "    predicted_label = []\n",
        "    predicted_label.extend(categories_po[i])\n",
        "    predicted_label.extend(categories_neu[i]) \n",
        "    predicted_label.extend(categories_neg[i])\n",
        "    bar_mac_f1.append(metrics.f1_score(y_true=true_label,y_pred=predicted_label,average='macro'))\n",
        "#     print(len(true_label))\n",
        "#     print(len(predicted_label))\n",
        "  print('pos:' , bar_pos)\n",
        "  print('neu:' , bar_neu)\n",
        "#   print('neg:' , bar_neg)\n",
        "  print('macro f1', bar_mac_f1)\n",
        "  if plot == 'bar':\n",
        "\n",
        "    bar_width = 0.5  \n",
        "#     plt.bar([i for i in range(1,len(maxes))], bar_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "#     plt.bar([i+bar_width for i in range(1,len(maxes))], bar_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "#     plt.bar([i+2*bar_width for i in range(1,len(maxes))], bar_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "    plt.bar([i for i in range(1,len(maxes))], bar_mac_f1, width = bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='macroF1')\n",
        "\n",
        "  elif plot == 'area':\n",
        "    plt.stackplot([i for i in range(1,len(maxes))],bar_pos,bar_neu,bar_neg,bar_mac_f1,labels=['positive','neutral','negative','macroF1'],colors=['green','yellow','red','blue'])\n",
        "  plt.xticks([r for r in range(1,len(maxes))], ['[%d-%d)'%(maxes[i],maxes[i+1]) for i in range(len(maxes)-1)])\n",
        "#   plt.ylabel('% Correctly Classified Documents')\n",
        "  plt.ylabel('Macro F1')\n",
        "\n",
        "  plt.xlabel(label)\n",
        "#   plt.legend() \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omlS-01FtYnG",
        "colab_type": "code",
        "outputId": "b7f497b9-93d5-4806-ddcb-052ae5833b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# maxes = [-1,3,5,7,9,11,13,15,1000]\n",
        "\n",
        "\n",
        "plot_correctly_classified(splitter=' ',bar_range=[1,180,270,450,1400],label='Number of Words in a Document')\n",
        "plot_correctly_classified(bar_range= [1,5,7,10,17])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "70\n",
            "67\n",
            "81\n",
            "83\n",
            "Neutrals\n",
            "68\n",
            "57\n",
            "49\n",
            "42\n",
            "('oooonjaaa>>>', [['Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Negative', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral'], ['Negative', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral'], ['Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral'], ['Negative', 'Negative', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Negative', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral']])\n",
            "Negatives\n",
            "5\n",
            "20\n",
            "15\n",
            "17\n",
            "Total\n",
            "143\n",
            "144\n",
            "145\n",
            "142\n",
            "('pos:', [0.6285714285714286, 0.5223880597014925, 0.4691358024691358, 0.4578313253012048])\n",
            "('neu:', [0.4411764705882353, 0.543859649122807, 0.6122448979591837, 0.5238095238095238])\n",
            "('macro f1', [0.40948024948024947, 0.42966081871345024, 0.36751228938109354, 0.3926569782054276])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "65\n",
            "76\n",
            "79\n",
            "81\n",
            "Neutrals\n",
            "56\n",
            "75\n",
            "41\n",
            "44\n",
            "('oooonjaaa>>>', [['Neutral', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral'], ['Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral'], ['Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Negative', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral'], ['Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Negative', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive']])\n",
            "Negatives\n",
            "12\n",
            "17\n",
            "13\n",
            "16\n",
            "Total\n",
            "133\n",
            "168\n",
            "133\n",
            "141\n",
            "('pos:', [0.5230769230769231, 0.6052631578947368, 0.4936708860759494, 0.4444444444444444])\n",
            "('neu:', [0.48214285714285715, 0.48, 0.6341463414634146, 0.5454545454545454])\n",
            "('macro f1', [0.3632275132275132, 0.44889663182346107, 0.38207192699946324, 0.40155486432115134])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84LNvYuACUCL",
        "colab_type": "code",
        "outputId": "1fa5b607-7a08-446d-e5b6-939a425ae124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#### Plot percentage of paragraph in a document which has the same label as document vs macro f1 score\n",
        "\n",
        "## input: set of documents with specific true label\n",
        "## output: percentage of paragraphs in the input documents which has the same label as document\n",
        "def par_in_doc(dev_par='',documents = negative_info,label =\"Negative\"):\n",
        "  percent_neg_map = {}\n",
        "  for doc_id in  documents.keys():\n",
        "    neg_pars = list(dev_par[dev_par['doc_id']==doc_id]['sentiment'])\n",
        "    if len(neg_pars)==0:\n",
        "      percent_neg =1.\n",
        "    else:\n",
        "      percent_neg = float(neg_pars.count(label))/len(neg_pars)\n",
        "\n",
        "    if percent_neg in percent_neg_map:\n",
        "      percent_neg_map[percent_neg].append(documents[doc_id])\n",
        "    else:\n",
        "      percent_neg_map[percent_neg] = [documents[doc_id]]\n",
        "#   print(len(percent_neg_map))\n",
        "  return percent_neg_map\n",
        "\n",
        "### based on the different ranges given, compute the f1 score of that specific category in each range ( percentage of paragraphs which has the same label as document)\n",
        "gfgdhfhgf\n",
        "def true_pred_cat(paragraphs = '', ranges = [0,0.26,0.51,0.67 ,0.80,1.],true_tag='Positive'):\n",
        "  categories = [[] for _ in ranges[1:]]\n",
        "#   print(len(paragraphs))\n",
        "#   print(paragraphs)\n",
        "#   for value,label in paragraphs:\n",
        "  for key in paragraphs.keys():\n",
        "    for i in range(1,len(ranges)):\n",
        "      if key <= ranges[i] and key > ranges[i-1] :\n",
        "        categories[i-1].extend(paragraphs[key])\n",
        "  macro_f1 = []   \n",
        "  data_distr = []\n",
        "  for i in range(len(ranges[1:])):\n",
        "    print(categories[i])\n",
        "    macro_f1.append(float(categories[i].count(true_tag))/len(categories[i]))\n",
        "    data_distr.append(len(categories[i]))\n",
        "#   print(macro_f1)\n",
        "  return macro_f1,categories,data_distr\n",
        "  \n",
        "per_po = par_in_doc(dev_par=dev_par,documents = positive_info,label =\"Positive\")\n",
        "per_neu= par_in_doc(dev_par=dev_par,documents = neutral_info,label =\"Neutral\")\n",
        "per_neg = par_in_doc(dev_par=dev_par,documents = negative_info,label =\"Negative\")\n",
        "# print(par_po)\n",
        "# print(par_neu)\n",
        "# print(par_neg)\n",
        "\n",
        "\n",
        "\n",
        "ranges = [0.,0.25,0.5,0.74,1.]\n",
        "# ranges = [0,0.99,1.]\n",
        "print('pos: ')\n",
        "bar_pos,pos_val,data_distr_pos = true_pred_cat(paragraphs = per_po,ranges= ranges,true_tag='Positive')\n",
        "print('neu: ')\n",
        "bar_neu,neu_val,data_distr_neu  = true_pred_cat(paragraphs = per_neu,ranges= ranges,true_tag='Neutral')\n",
        "print('neg: ')\n",
        "bar_neg,neg_val,data_distr_neg = true_pred_cat(paragraphs = per_neg,ranges= ranges,true_tag='Negative')\n",
        "print('total:')\n",
        "for i in range(len(pos_val)):\n",
        "  print(len(pos_val[i])+len(neu_val[i])+len(neg_val[i]))\n",
        "\n",
        "bar_mac_f1 = [float(i+j+k)/3 for i,j,k in zip(bar_pos,bar_neu,bar_neg)]\n",
        "print(bar_pos)\n",
        "print(bar_neu)\n",
        "print(bar_neg)\n",
        "print(bar_mac_f1)\n",
        "data_dist_total = [(i+j+k) for i,j,k in zip(data_distr_pos,data_distr_neu,data_distr_neg)]\n",
        "bar_width = 0.2 \n",
        "# plt.bar([i for i in range(1,len(ranges))], bar_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "# plt.bar([i+bar_width for i in range(1,len(ranges))], bar_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "# plt.bar([i+2*bar_width for i in range(1,len(ranges))], bar_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "plt.bar([i for i in range(1,len(ranges))], bar_mac_f1, width = 3*bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='macroF1')\n",
        "\n",
        "\n",
        "plt.xticks([r for r in range(1,len(ranges))], ['(%s-%s]'%(str(ranges[i]),str(ranges[i+1])) for i in range(len(ranges)-1)])#ranges[1:])\n",
        "plt.ylabel('Macro F1')\n",
        "plt.xlabel('Percentage of paragraphs w/ same label as document')\n",
        "# plt.legend() \n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ### showing the dataset distribution\n",
        "# plt.bar([i for i in range(1,len(ranges))], data_distr_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "# plt.bar([i+bar_width for i in range(1,len(ranges))], data_distr_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "# plt.bar([i+2*bar_width for i in range(1,len(ranges))], data_distr_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "# plt.bar([i+3*bar_width for i in range(1,len(ranges))], data_dist_total, width = bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='Total')\n",
        "\n",
        "\n",
        "# plt.xticks([r for r in range(1,len(ranges))], ['(%s-%s]'%(str(ranges[i]),str(ranges[i+1])) for i in range(len(ranges)-1)])#ranges[1:])\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.xlabel('Percentage of paragraphs w/ same label as document')\n",
        "# plt.legend() \n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: \n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral']\n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive']\n",
            "['Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Negative']\n",
            "['Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral']\n",
            "neu: \n",
            "['Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral']\n",
            "['Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive']\n",
            "['Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive']\n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral']\n",
            "neg: \n",
            "['Neutral', 'Positive']\n",
            "['Neutral', 'Negative', 'Neutral', 'Neutral', 'Negative', 'Neutral']\n",
            "['Positive', 'Negative']\n",
            "['Negative', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Negative', 'Neutral', 'Negative', 'Negative']\n",
            "total:\n",
            "13\n",
            "57\n",
            "51\n",
            "161\n",
            "[0.16666666666666666, 0.5454545454545454, 0.48484848484848486, 0.6521739130434783]\n",
            "[0.6, 0.3333333333333333, 0.6875, 0.5614035087719298]\n",
            "[0.0, 0.3333333333333333, 0.5, 0.5]\n",
            "[0.25555555555555554, 0.404040404040404, 0.5574494949494949, 0.5711924739384694]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IobAZXZvppR3",
        "colab_type": "code",
        "outputId": "373a339c-a411-4d90-ab8a-b7cce91bfcbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "### number of unique entities vs Macro F1\n",
        "plot_correctly_classified(splitter=' ',bar_range=[1,4,7,20],label='Number of Unique Entities in a Document',method_plot=unique_entity_label,plot='bar')\n",
        "# plot_correctly_classified(splitter=' ',bar_range=[0,4,7,15],label='Number of Unique Entities in a Document',method_plot=unique_entity_label,plot='area')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "68\n",
            "52\n",
            "43\n",
            "Neutrals\n",
            "41\n",
            "30\n",
            "24\n",
            "('oooonjaaa>>>', [['Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive'], ['Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive'], ['Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral']])\n",
            "Negatives\n",
            "3\n",
            "8\n",
            "11\n",
            "Total\n",
            "112\n",
            "90\n",
            "78\n",
            "('pos:', [0.7352941176470589, 0.6923076923076923, 0.7209302325581395])\n",
            "('neu:', [0.4146341463414634, 0.4, 0.3333333333333333])\n",
            "('macro f1', [0.5150390980178213, 0.46012673788733177, 0.3789233954451346])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHX+P14nX3X55K1jAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7AYlKDt_EqQ",
        "colab_type": "code",
        "outputId": "05116070-2abd-4d26-baf6-607aad15167f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "source": [
        "##### Compare the classification accuracy in sentiment analysis task and masked LM task\n",
        "dev_predicted_masked = pd.read_csv(open('/content/dev_predicted.csv'))\n",
        "\n",
        "\n",
        "### given and input list of document ids in the classification task, compute the accuracy of that set in masked lm task. it has three methods to compute average in masked lm task\n",
        "### avg: which computes average among different documents, \n",
        "##  one: accuracy is one if one of the documents in masked lm is correctly classified, all: accuracy is one when all of the documents in masked lm is correctly classified\n",
        "def compute_acc(input_list = [],method='avg'):\n",
        "  \n",
        "  cc_masked_lm_acc = 0   \n",
        "  for doc_id in input_list:\n",
        "    predicted = list(dev_predicted_masked[dev_predicted_masked['doc_id']==doc_id]['predicted'])\n",
        "#     print(predicted)\n",
        "    label = list(dev_predicted_masked[dev_predicted_masked['doc_id']==doc_id]['label'])\n",
        "#     print(label)\n",
        "    acc = metrics.accuracy_score(y_pred=predicted,y_true=label)\n",
        "    if method == 'avg':\n",
        "      cc_masked_lm_acc += acc\n",
        "    elif method == 'all':\n",
        "      if acc == 1:\n",
        "        cc_masked_lm_acc +=1\n",
        "    elif method == 'one':\n",
        "      if acc > 0:\n",
        "        cc_masked_lm_acc +=1\n",
        "  return float(cc_masked_lm_acc/len(input_list))\n",
        "        \n",
        "### compare the masked lm accuracy with the sentiment analysis accuracy, given a label, return the correctly classified accuracy in masked lm and  \n",
        "### incorrectly classified ( in sentiment analysis) accuracy ( in masked lm), as we have many documents in masked lm per each document in sentiment analysis,\n",
        "### there are three methods to compute the accuracy in masked lm: \n",
        "###     avg: compute average accuracy among different documents with the same doc id in masked lm, \n",
        "###     all: if all of the documents aggree in masked lm that document is considered correct\n",
        "###     one: if one of the documents is correctly classified in masked lm then that document is considered correct\n",
        "def masked_LM_VS_classifier_acc(negative_info,label='Negative',method = 'avg'):\n",
        "  correctly_classified= []\n",
        "  incorrectly_classified = []\n",
        "  incorrect_labels = {'neg_pos':[],'pos_neg':[],'pos_neu':[],\n",
        "                     'neg_neu':[],'neu_pos':[],'neu_neg':[]}\n",
        "  \n",
        "  for item in negative_info:\n",
        "    pred = negative_info[item] \n",
        "    if negative_info[item] ==label:\n",
        "      correctly_classified.append(item)\n",
        "    else:\n",
        "      incorrectly_classified.append(item)\n",
        "      if label.startswith('Neg') and pred.startswith('Po'):\n",
        "        incorrect_labels['neg_pos'].append(item)\n",
        "      elif label.startswith('Neg') and pred.startswith('Neu'):\n",
        "        incorrect_labels['neg_neu'].append(item)\n",
        "      elif label.startswith('Pos') and pred.startswith('Neg'):\n",
        "        incorrect_labels['pos_neg'].append(item)\n",
        "\n",
        "      elif label.startswith('Pos') and pred.startswith('Neu'):\n",
        "        incorrect_labels['pos_neu'].append(item)\n",
        "\n",
        "      elif label.startswith('Neu') and pred.startswith('Pos'):\n",
        "        incorrect_labels['neu_pos'].append(item)\n",
        "\n",
        "      elif label.startswith('Neu') and pred.startswith('Neg'):\n",
        "        incorrect_labels['neu_neg'].append(item)\n",
        "\n",
        "        \n",
        "  print(incorrect_labels)\n",
        "  cc_masked_lm_acc = compute_acc(correctly_classified,method=method)\n",
        "    \n",
        "  ic_masked_lm_acc = compute_acc(incorrectly_classified,method=method)\n",
        "  incorrect_perclass_acc = {}\n",
        "  for item in incorrect_labels:\n",
        "    if len(incorrect_labels[item])==0:\n",
        "      continue\n",
        "    incorrect_perclass_acc[item] = compute_acc(incorrect_labels[item],method=method)\n",
        "\n",
        "#   print(incorrect_perclass_acc)  \n",
        "  return(cc_masked_lm_acc\n",
        "               ,ic_masked_lm_acc,incorrect_perclass_acc)\n",
        "\n",
        "\n",
        "\n",
        "cc_neg, ic_neg,incorrect_perclass_neg = masked_LM_VS_classifier_acc(negative_info,label='Negative',method='avg')\n",
        "print(cc_neg, ic_neg,incorrect_perclass_neg)\n",
        "\n",
        "cc_pos, ic_pos,incorrect_perclass_pos = masked_LM_VS_classifier_acc(positive_info,label='Positive',method='avg')\n",
        "print(cc_pos, ic_pos,incorrect_perclass_pos)\n",
        "\n",
        "cc_neu, ic_neu,incorrect_perclass_neu = masked_LM_VS_classifier_acc(neutral_info,label='Neutral',method='avg')\n",
        "print(cc_neu, ic_neu,incorrect_perclass_neu)\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(4)\n",
        "bar_width = 0.4\n",
        "\n",
        "\n",
        "# rect1 = plt.bar([i for i in range(3)], [cc_neg,incorrect_perclass_neu['neu_neg'],incorrect_perclass_pos['pos_neg']],bar_width,color = 'r',label='label_neg')\n",
        "# rect2 = plt.bar([i+bar_width for i in range(3)], [incorrect_perclass_neg['neg_neu'],cc_neu,incorrect_perclass_pos['pos_neu']],bar_width,color = 'y',label='label_neu')\n",
        "# rect3 = plt.bar([i+2*bar_width for i in range(3)], [incorrect_perclass_neg['neg_pos'],incorrect_perclass_neu['neu_pos'],cc_pos],bar_width,color = 'g',label='label_pos')\n",
        "# rect4 = plt.bar([i+3*bar_width for i in range(3)], [ic_neg,ic_neu,ic_pos],bar_width,color = 'b',label='label_other')\n",
        "\n",
        "rect1 = plt.bar([i for i in range(3)], [cc_neg,cc_neu,cc_pos],bar_width,color = 'blue',label='Correctly Classified')\n",
        "rect2 = plt.bar([i+bar_width for i in range(3)], [ic_neg,ic_neu,ic_pos],bar_width,color = 'cyan',label='Incorrectly Classified')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "###loc: right, center left, upper right, lower right, best, center, lower left, center right, upper left, upper center, lower center\n",
        "\n",
        "plt.legend(loc='right', bbox_to_anchor=(1.3, .9), ncol=1, fancybox=True, shadow=True)\n",
        "# plt.legend()\n",
        "plt.xticks( index+0.5*bar_width,('Negative','Neutral','Positive'))\n",
        "plt.ylabel('Masked Entity LM Accuracy')\n",
        "plt.xlabel('Sentiment Classification True Label')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pos_neg': [], 'neu_pos': [], 'neu_neg': [], 'pos_neu': [], 'neg_neu': [1929, 1933, 2062, 1944, 1969, 1842, 1976, 1852, 1985, 1884, 1900, 1902, 1907, 1908, 2046], 'neg_pos': [1844, 1854, 1984, 1875]}\n",
            "(0.9058560924369748, 0.8454868231629963, {'neg_neu': 0.8522865299517116, 'neg_pos': 0.819987922705314})\n",
            "{'pos_neg': [1943, 2014, 2028], 'neu_pos': [], 'neu_neg': [], 'pos_neu': [2050, 2052, 2055, 2066, 2071, 2073, 2077, 2078, 2079, 2086, 2089, 2091, 2095, 2098, 2103, 2108, 1822, 1866, 1873, 1881, 1890, 1905, 1906, 1910, 1919, 1920, 1931, 1934, 1952, 1961, 1974, 1982, 1997, 2002, 2004, 2013, 2016, 2020, 2024, 2026, 2031, 2039, 2042], 'neg_neu': [], 'neg_pos': []}\n",
            "(0.8835872680262865, 0.8293638440213555, {'pos_neu': 0.8326637482048473, 'pos_neg': 0.7820652173913043})\n",
            "{'pos_neg': [], 'neu_pos': [2048, 2075, 2080, 2084, 1831, 1838, 1839, 1840, 1845, 1848, 1850, 1855, 1857, 1861, 1862, 1864, 1869, 1871, 1874, 1889, 1892, 1893, 1895, 1896, 1897, 1898, 1911, 1912, 1921, 1949, 1950, 1955, 1957, 1963, 1964, 1966, 1967, 1968, 1972, 1973, 1975, 1977, 1983, 1987, 1988, 1990, 1991, 1992, 2000, 2006, 2021, 2022, 2025, 2027, 2029, 2030, 2037], 'neu_neg': [1836, 1965, 2012], 'pos_neu': [], 'neg_neu': [], 'neg_pos': []}\n",
            "(0.8518476781634678, 0.8782026728717903, {'neu_neg': 0.8579545454545454, 'neu_pos': 0.8792683637884875})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEKCAYAAAAlye1PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkP8Pxr9valW1OCUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HKwE4hrs6LJ",
        "colab_type": "code",
        "outputId": "ab9533a0-3ead-4b0f-b43b-b1b30025f141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cc_neu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8133068696860807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il1l5oLaB7dD",
        "colab_type": "code",
        "outputId": "0f18bb91-7961-4641-a3e0-ca973a175157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "float(masked_lm_acc/len(correctly_classified))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.878968253968254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngem-phNpjuD",
        "colab_type": "text"
      },
      "source": [
        "#End of Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPHXLYLhih5Z",
        "colab_type": "code",
        "outputId": "cd735ef6-cb0c-43b7-f821-5cc71c444302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "random_test_features = run_classifier.convert_examples_to_features(test_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "fixed_test_features = run_classifier.convert_examples_to_features(test_InputExamples_fixed_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "ind = 1\n",
        "for ckpt in range(92,691,46):\n",
        "  ind += 1\n",
        "  print('----------------------starting epoch %d ----------------------'%ind)\n",
        "  print('evaluating trainset: ')\n",
        "  predictions = model_predict(estimator_from_tfhub,train_InputExamples_doc,train_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(train_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('evaluating devset: ')\n",
        "  \n",
        "  predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  print('evaluating random testset: ')\n",
        "  \n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,random_test_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "    \n",
        "  print('evaluating fixed testset: ')\n",
        "\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,fixed_test_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------starting epoch 2 ----------------------\n",
            "evaluating trainset: \n",
            "[[  0   0 145]\n",
            " [  0   0 443]\n",
            " [  0   0 913]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       145\n",
            "     Neutral       0.00      0.00      0.00       443\n",
            "    Positive       0.61      1.00      0.76       913\n",
            "\n",
            "   micro avg       0.61      0.61      0.61      1501\n",
            "   macro avg       0.20      0.33      0.25      1501\n",
            "weighted avg       0.37      0.61      0.46      1501\n",
            "\n",
            "evaluating devset: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  0   0  23]\n",
            " [  0   0  97]\n",
            " [  0   0 164]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.00      0.00      0.00        97\n",
            "    Positive       0.58      1.00      0.73       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.19      0.33      0.24       284\n",
            "weighted avg       0.33      0.58      0.42       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  0   0  28]\n",
            " [  0   0  87]\n",
            " [  0   0 170]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        28\n",
            "     Neutral       0.00      0.00      0.00        87\n",
            "    Positive       0.60      1.00      0.75       170\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       285\n",
            "   macro avg       0.20      0.33      0.25       285\n",
            "weighted avg       0.36      0.60      0.45       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  0   0  50]\n",
            " [  0   0 116]\n",
            " [  0   0 173]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        50\n",
            "     Neutral       0.00      0.00      0.00       116\n",
            "    Positive       0.51      1.00      0.68       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.17      0.33      0.23       339\n",
            "weighted avg       0.26      0.51      0.34       339\n",
            "\n",
            "----------------------starting epoch 3 ----------------------\n",
            "evaluating trainset: \n",
            "[[ 80  57   8]\n",
            " [ 37 268 138]\n",
            " [ 10 102 801]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.63      0.55      0.59       145\n",
            "     Neutral       0.63      0.60      0.62       443\n",
            "    Positive       0.85      0.88      0.86       913\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      1501\n",
            "   macro avg       0.70      0.68      0.69      1501\n",
            "weighted avg       0.76      0.77      0.76      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  7  12   4]\n",
            " [  4  37  56]\n",
            " [  3  35 126]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.30      0.38        23\n",
            "     Neutral       0.44      0.38      0.41        97\n",
            "    Positive       0.68      0.77      0.72       164\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       284\n",
            "   macro avg       0.54      0.48      0.50       284\n",
            "weighted avg       0.58      0.60      0.59       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  6  13   9]\n",
            " [  4  41  42]\n",
            " [  3  39 128]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.46      0.21      0.29        28\n",
            "     Neutral       0.44      0.47      0.46        87\n",
            "    Positive       0.72      0.75      0.73       170\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       285\n",
            "   macro avg       0.54      0.48      0.49       285\n",
            "weighted avg       0.61      0.61      0.61       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  9  25  16]\n",
            " [  8  53  55]\n",
            " [ 10  58 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.18      0.23        50\n",
            "     Neutral       0.39      0.46      0.42       116\n",
            "    Positive       0.60      0.61      0.60       173\n",
            "\n",
            "   micro avg       0.49      0.49      0.49       339\n",
            "   macro avg       0.44      0.41      0.42       339\n",
            "weighted avg       0.49      0.49      0.49       339\n",
            "\n",
            "----------------------starting epoch 4 ----------------------\n",
            "evaluating trainset: \n",
            "[[127  14   4]\n",
            " [ 70 327  46]\n",
            " [  8 112 793]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.88      0.73       145\n",
            "     Neutral       0.72      0.74      0.73       443\n",
            "    Positive       0.94      0.87      0.90       913\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      1501\n",
            "   macro avg       0.76      0.83      0.79      1501\n",
            "weighted avg       0.85      0.83      0.83      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 9 12  2]\n",
            " [ 6 53 38]\n",
            " [ 4 65 95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.47      0.39      0.43        23\n",
            "     Neutral       0.41      0.55      0.47        97\n",
            "    Positive       0.70      0.58      0.64       164\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       284\n",
            "   macro avg       0.53      0.51      0.51       284\n",
            "weighted avg       0.58      0.55      0.56       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 9 13  6]\n",
            " [12 45 30]\n",
            " [ 9 62 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.32      0.31        28\n",
            "     Neutral       0.38      0.52      0.43        87\n",
            "    Positive       0.73      0.58      0.65       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.47      0.47      0.46       285\n",
            "weighted avg       0.58      0.54      0.55       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[16 25  9]\n",
            " [10 67 39]\n",
            " [13 75 85]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.41      0.32      0.36        50\n",
            "     Neutral       0.40      0.58      0.47       116\n",
            "    Positive       0.64      0.49      0.56       173\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       339\n",
            "   macro avg       0.48      0.46      0.46       339\n",
            "weighted avg       0.52      0.50      0.50       339\n",
            "\n",
            "----------------------starting epoch 5 ----------------------\n",
            "evaluating trainset: \n",
            "[[135   7   3]\n",
            " [ 44 350  49]\n",
            " [  7  23 883]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.73      0.93      0.82       145\n",
            "     Neutral       0.92      0.79      0.85       443\n",
            "    Positive       0.94      0.97      0.96       913\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      1501\n",
            "   macro avg       0.86      0.90      0.87      1501\n",
            "weighted avg       0.92      0.91      0.91      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  2  14   7]\n",
            " [  1  37  59]\n",
            " [  3  41 120]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.40      0.38      0.39        97\n",
            "    Positive       0.65      0.73      0.69       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.46      0.40      0.41       284\n",
            "weighted avg       0.54      0.56      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  5  10  13]\n",
            " [ 10  28  49]\n",
            " [  5  38 127]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.18      0.21        28\n",
            "     Neutral       0.37      0.32      0.34        87\n",
            "    Positive       0.67      0.75      0.71       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.43      0.42      0.42       285\n",
            "weighted avg       0.54      0.56      0.55       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[ 10  21  19]\n",
            " [  5  45  66]\n",
            " [  7  45 121]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.45      0.20      0.28        50\n",
            "     Neutral       0.41      0.39      0.40       116\n",
            "    Positive       0.59      0.70      0.64       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.48      0.43      0.44       339\n",
            "weighted avg       0.51      0.52      0.50       339\n",
            "\n",
            "----------------------starting epoch 6 ----------------------\n",
            "evaluating trainset: \n",
            "[[135   7   3]\n",
            " [  8 425  10]\n",
            " [  6  33 874]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.91      0.93      0.92       145\n",
            "     Neutral       0.91      0.96      0.94       443\n",
            "    Positive       0.99      0.96      0.97       913\n",
            "\n",
            "   micro avg       0.96      0.96      0.96      1501\n",
            "   macro avg       0.94      0.95      0.94      1501\n",
            "weighted avg       0.96      0.96      0.96      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 2 18  3]\n",
            " [ 2 57 38]\n",
            " [ 2 72 90]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.39      0.59      0.47        97\n",
            "    Positive       0.69      0.55      0.61       164\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       284\n",
            "   macro avg       0.47      0.41      0.41       284\n",
            "weighted avg       0.56      0.52      0.52       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 2 19  7]\n",
            " [ 4 51 32]\n",
            " [ 1 78 91]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.07      0.11        28\n",
            "     Neutral       0.34      0.59      0.43        87\n",
            "    Positive       0.70      0.54      0.61       170\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       285\n",
            "   macro avg       0.44      0.40      0.38       285\n",
            "weighted avg       0.55      0.51      0.51       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[ 6 32 12]\n",
            " [ 0 74 42]\n",
            " [ 4 78 91]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.12      0.20        50\n",
            "     Neutral       0.40      0.64      0.49       116\n",
            "    Positive       0.63      0.53      0.57       173\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       339\n",
            "   macro avg       0.54      0.43      0.42       339\n",
            "weighted avg       0.55      0.50      0.49       339\n",
            "\n",
            "----------------------starting epoch 7 ----------------------\n",
            "evaluating trainset: \n",
            "[[139   3   3]\n",
            " [  4 432   7]\n",
            " [  6   8 899]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.93      0.96      0.95       145\n",
            "     Neutral       0.98      0.98      0.98       443\n",
            "    Positive       0.99      0.98      0.99       913\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      1501\n",
            "   macro avg       0.97      0.97      0.97      1501\n",
            "weighted avg       0.98      0.98      0.98      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  2  16   5]\n",
            " [  2  48  47]\n",
            " [  2  54 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.41      0.49      0.45        97\n",
            "    Positive       0.68      0.66      0.67       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.47      0.41      0.42       284\n",
            "weighted avg       0.56      0.56      0.55       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  1  20   7]\n",
            " [  6  43  38]\n",
            " [  1  60 109]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.12      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.71      0.64      0.67       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.39      0.39      0.38       285\n",
            "weighted avg       0.54      0.54      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  4  29  17]\n",
            " [  1  59  56]\n",
            " [  2  54 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.57      0.08      0.14        50\n",
            "     Neutral       0.42      0.51      0.46       116\n",
            "    Positive       0.62      0.68      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.53      0.42      0.41       339\n",
            "weighted avg       0.54      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 8 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  3 436   4]\n",
            " [  4   8 901]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.95      0.97      0.96       145\n",
            "     Neutral       0.98      0.98      0.98       443\n",
            "    Positive       0.99      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      1501\n",
            "   macro avg       0.97      0.98      0.98      1501\n",
            "weighted avg       0.98      0.98      0.98      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  1  18   4]\n",
            " [  2  48  47]\n",
            " [  2  59 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.38      0.49      0.43        97\n",
            "    Positive       0.67      0.63      0.65       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.42      0.39      0.38       284\n",
            "weighted avg       0.53      0.54      0.53       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  3  19   6]\n",
            " [  6  43  38]\n",
            " [  1  65 104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.11      0.16        28\n",
            "     Neutral       0.34      0.49      0.40        87\n",
            "    Positive       0.70      0.61      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.45      0.40      0.40       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  29  16]\n",
            " [  1  64  51]\n",
            " [  2  58 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.10      0.17        50\n",
            "     Neutral       0.42      0.55      0.48       116\n",
            "    Positive       0.63      0.65      0.64       173\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       339\n",
            "   macro avg       0.56      0.43      0.43       339\n",
            "weighted avg       0.56      0.54      0.52       339\n",
            "\n",
            "----------------------starting epoch 9 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  3 438   2]\n",
            " [  3   6 904]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.96      0.97      0.96       145\n",
            "     Neutral       0.98      0.99      0.99       443\n",
            "    Positive       0.99      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.98      0.98      0.98      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 4 16  3]\n",
            " [ 2 51 44]\n",
            " [ 3 62 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.44      0.17      0.25        23\n",
            "     Neutral       0.40      0.53      0.45        97\n",
            "    Positive       0.68      0.60      0.64       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.51      0.43      0.45       284\n",
            "weighted avg       0.56      0.54      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  6  47  34]\n",
            " [  1  69 100]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.22      0.07      0.11        28\n",
            "     Neutral       0.34      0.54      0.42        87\n",
            "    Positive       0.72      0.59      0.65       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.43      0.40      0.39       285\n",
            "weighted avg       0.56      0.52      0.52       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  27  18]\n",
            " [  1  63  52]\n",
            " [  2  60 111]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.10      0.17        50\n",
            "     Neutral       0.42      0.54      0.47       116\n",
            "    Positive       0.61      0.64      0.63       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.55      0.43      0.42       339\n",
            "weighted avg       0.55      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 10 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  1 441   1]\n",
            " [  3   3 907]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.97      0.97      0.97       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.98      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  4  15   4]\n",
            " [  2  43  52]\n",
            " [  2  55 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.17      0.26        23\n",
            "     Neutral       0.38      0.44      0.41        97\n",
            "    Positive       0.66      0.65      0.65       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.51      0.42      0.44       284\n",
            "weighted avg       0.55      0.54      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  19   7]\n",
            " [  7  44  36]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.35      0.51      0.41        87\n",
            "    Positive       0.71      0.62      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  25  20]\n",
            " [  0  57  59]\n",
            " [  2  52 119]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.71      0.10      0.18        50\n",
            "     Neutral       0.43      0.49      0.46       116\n",
            "    Positive       0.60      0.69      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.58      0.43      0.42       339\n",
            "weighted avg       0.56      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 11 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  0 442   1]\n",
            " [  1   4 908]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 1 47 49]\n",
            " [ 1 65 98]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.08        23\n",
            "     Neutral       0.36      0.48      0.41        97\n",
            "    Positive       0.65      0.60      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.45      0.38      0.37       284\n",
            "weighted avg       0.53      0.51      0.51       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  1  22   5]\n",
            " [  3  50  34]\n",
            " [  1  67 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.06        28\n",
            "     Neutral       0.36      0.57      0.44        87\n",
            "    Positive       0.72      0.60      0.66       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.43      0.40      0.39       285\n",
            "weighted avg       0.56      0.54      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  2  30  18]\n",
            " [  0  63  53]\n",
            " [  1  58 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.04      0.08        50\n",
            "     Neutral       0.42      0.54      0.47       116\n",
            "    Positive       0.62      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.57      0.41      0.39       339\n",
            "weighted avg       0.56      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 12 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  1 441   1]\n",
            " [  4   2 907]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.97      0.97      0.97       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.98      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 4 16  3]\n",
            " [ 3 50 44]\n",
            " [ 4 66 94]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.36      0.17      0.24        23\n",
            "     Neutral       0.38      0.52      0.44        97\n",
            "    Positive       0.67      0.57      0.62       164\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       284\n",
            "   macro avg       0.47      0.42      0.43       284\n",
            "weighted avg       0.54      0.52      0.52       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 4 19  5]\n",
            " [ 8 46 33]\n",
            " [ 1 70 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.31      0.14      0.20        28\n",
            "     Neutral       0.34      0.53      0.41        87\n",
            "    Positive       0.72      0.58      0.64       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.46      0.42      0.42       285\n",
            "weighted avg       0.57      0.52      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  6  27  17]\n",
            " [  1  66  49]\n",
            " [  3  62 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.12      0.20        50\n",
            "     Neutral       0.43      0.57      0.49       116\n",
            "    Positive       0.62      0.62      0.62       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.55      0.44      0.44       339\n",
            "weighted avg       0.55      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 13 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  3   1 909]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.98      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  1  18   4]\n",
            " [  1  43  53]\n",
            " [  2  60 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.07        23\n",
            "     Neutral       0.36      0.44      0.39        97\n",
            "    Positive       0.64      0.62      0.63       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.42      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.51       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  20   6]\n",
            " [  5  45  37]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.07      0.11        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.71      0.62      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.44      0.40      0.40       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  28  19]\n",
            " [  0  58  58]\n",
            " [  1  55 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.06      0.11        50\n",
            "     Neutral       0.41      0.50      0.45       116\n",
            "    Positive       0.60      0.68      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.59      0.41      0.40       339\n",
            "weighted avg       0.56      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 14 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  2   1 910]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 2 47 48]\n",
            " [ 2 66 96]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.36      0.48      0.41        97\n",
            "    Positive       0.65      0.59      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.40      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.50       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  7  45  35]\n",
            " [  1  67 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.34      0.52      0.41        87\n",
            "    Positive       0.72      0.60      0.65       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.52      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  30  17]\n",
            " [  0  61  55]\n",
            " [  2  56 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.06      0.11        50\n",
            "     Neutral       0.41      0.53      0.46       116\n",
            "    Positive       0.61      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.54      0.42      0.40       339\n",
            "weighted avg       0.54      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 15 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  2   1 910]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 2 46 49]\n",
            " [ 2 65 97]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.35      0.47      0.41        97\n",
            "    Positive       0.65      0.59      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.40      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.50       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  7  45  35]\n",
            " [  1  66 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.34      0.52      0.41        87\n",
            "    Positive       0.72      0.61      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  29  18]\n",
            " [  0  61  55]\n",
            " [  2  56 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.06      0.11        50\n",
            "     Neutral       0.42      0.53      0.47       116\n",
            "    Positive       0.61      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.54      0.42      0.40       339\n",
            "weighted avg       0.54      0.53      0.50       339\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par)\n",
        "# import numpy as np\n",
        "# from sklearn import metrics\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_fixed_par['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_par)\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_par['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqh11HPVAsvx",
        "colab_type": "code",
        "outputId": "9a40b4f2-c66c-4a6b-cd89-e7f156a2a794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "\n",
        "\n",
        "print('-------- Document level Dev Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Fixed Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Random Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Document level Dev Result---------\n",
            "[[  2  11  10]\n",
            " [  0  47  50]\n",
            " [  2  46 116]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.09      0.15        23\n",
            "     Neutral       0.45      0.48      0.47        97\n",
            "    Positive       0.66      0.71      0.68       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.54      0.43      0.43       284\n",
            "weighted avg       0.58      0.58      0.57       284\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  2  27  21]\n",
            " [  4  49  63]\n",
            " [  2  50 121]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.07        50\n",
            "     Neutral       0.39      0.42      0.40       116\n",
            "    Positive       0.59      0.70      0.64       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.41      0.39      0.37       339\n",
            "weighted avg       0.47      0.51      0.48       339\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  11  16]\n",
            " [  2  42  43]\n",
            " [  2  51 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.06        28\n",
            "     Neutral       0.40      0.48      0.44        87\n",
            "    Positive       0.66      0.69      0.68       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.54      0.56      0.54       285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdebsYZ08XI",
        "colab_type": "code",
        "outputId": "6e23d840-1db4-4146-a52c-9690ac4bc483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "print('-------- Document level Dev Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Fixed Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Random Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Document level Dev Result---------\n",
            "[[  2  15   6]\n",
            " [  0  48  49]\n",
            " [  2  52 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.09      0.15        23\n",
            "     Neutral       0.42      0.49      0.45        97\n",
            "    Positive       0.67      0.67      0.67       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.53      0.42      0.42       284\n",
            "weighted avg       0.57      0.56      0.55       284\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  3  28  19]\n",
            " [  5  53  58]\n",
            " [  2  60 111]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.06      0.10        50\n",
            "     Neutral       0.38      0.46      0.41       116\n",
            "    Positive       0.59      0.64      0.61       173\n",
            "\n",
            "   micro avg       0.49      0.49      0.49       339\n",
            "   macro avg       0.42      0.39      0.38       339\n",
            "weighted avg       0.47      0.49      0.47       339\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  2  16  10]\n",
            " [  2  45  40]\n",
            " [  2  60 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.07      0.12        28\n",
            "     Neutral       0.37      0.52      0.43        87\n",
            "    Positive       0.68      0.64      0.66       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.46      0.41      0.40       285\n",
            "weighted avg       0.55      0.54      0.54       285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tq0IPSncltM",
        "colab_type": "code",
        "outputId": "e4ce8ce3-e6b4-4541-a45b-c6f28aaf9520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12189
        }
      },
      "source": [
        "mds = []\n",
        "evs = []\n",
        "pds = []\n",
        "tf.logging.set_verbosity(tf.logging.FATAL) \n",
        "for i in range(1,15):\n",
        "  print('----------------------- Starting Epoch %d-----------------------'%i)\n",
        "\n",
        "  NUM_TRAIN_EPOCHS = i\n",
        "  \n",
        "  num_train_steps = int(len(train_InputExamples_all) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "#   model_fn = model_fn_builder(\n",
        "#   num_labels=len(label_list),\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps,\n",
        "#   use_tpu=True,\n",
        "#   bert_hub_module_handle=BERT_MODEL_HUB)\n",
        "\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "    num_labels=len(label_list),\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    bert_hub_module_handle=BERT_MODEL_HUB\n",
        "  )\n",
        "\n",
        "  estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=model_fn,\n",
        "    config=get_run_config(OUTPUT_DIR),\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE,\n",
        "  )\n",
        "\n",
        "\n",
        "  \n",
        "  md = model_train(estimator_from_tfhub)\n",
        "  \n",
        "  \n",
        "  print('-------- Document level Dev Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('-------- Paragraph level Dev Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,dev_InputExamples_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "  print('-------- Document level Fixed Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('-------- Paragraph level Fixed Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "  print('-------- Document level Random Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  print('-------- Paragraph level Random Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------- Starting Epoch 1-----------------------\n",
            "***** Started training at 2019-05-09 12:07:13.408352 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:11:33.467409 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  17   6]\n",
            " [  1  37  59]\n",
            " [  2  36 126]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.38      0.40        97\n",
            "    Positive       0.66      0.77      0.71       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.36      0.38      0.37       284\n",
            "weighted avg       0.52      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  92  53]\n",
            " [  1 317 437]\n",
            " [  4 244 732]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.17      0.01      0.01       146\n",
            "     Neutral       0.49      0.42      0.45       755\n",
            "    Positive       0.60      0.75      0.66       980\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1881\n",
            "   macro avg       0.42      0.39      0.38      1881\n",
            "weighted avg       0.52      0.56      0.53      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  24  25]\n",
            " [  2  47  67]\n",
            " [  0  49 124]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.02      0.04        50\n",
            "     Neutral       0.39      0.41      0.40       116\n",
            "    Positive       0.57      0.72      0.64       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.43      0.38      0.36       339\n",
            "weighted avg       0.48      0.51      0.47       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 109 128]\n",
            " [  0 259 468]\n",
            " [  0 279 567]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.00      0.01       238\n",
            "     Neutral       0.40      0.36      0.38       727\n",
            "    Positive       0.49      0.67      0.56       846\n",
            "\n",
            "   micro avg       0.46      0.46      0.46      1811\n",
            "   macro avg       0.63      0.34      0.32      1811\n",
            "weighted avg       0.52      0.46      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  0  14  14]\n",
            " [  2  33  52]\n",
            " [  0  42 128]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        28\n",
            "     Neutral       0.37      0.38      0.38        87\n",
            "    Positive       0.66      0.75      0.70       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.34      0.38      0.36       285\n",
            "weighted avg       0.51      0.56      0.53       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  1 103  63]\n",
            " [  1 300 388]\n",
            " [  0 250 727]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.01      0.01       167\n",
            "     Neutral       0.46      0.44      0.45       689\n",
            "    Positive       0.62      0.74      0.67       977\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1833\n",
            "   macro avg       0.53      0.40      0.38      1833\n",
            "weighted avg       0.55      0.56      0.53      1833\n",
            "\n",
            "----------------------- Starting Epoch 2-----------------------\n",
            "***** Started training at 2019-05-09 12:16:39.097780 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:20:50.255185 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  48  49]\n",
            " [  3  49 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.49      0.45        97\n",
            "    Positive       0.68      0.68      0.68       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.36      0.39      0.38       284\n",
            "weighted avg       0.53      0.56      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  2  96  48]\n",
            " [  2 406 347]\n",
            " [  3 362 615]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.01      0.03       146\n",
            "     Neutral       0.47      0.54      0.50       755\n",
            "    Positive       0.61      0.63      0.62       980\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1881\n",
            "   macro avg       0.45      0.39      0.38      1881\n",
            "weighted avg       0.53      0.54      0.53      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  33  16]\n",
            " [  1  64  51]\n",
            " [  0  66 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.55      0.46       116\n",
            "    Positive       0.61      0.62      0.62       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 137 100]\n",
            " [  3 345 379]\n",
            " [  6 376 464]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.10      0.00      0.01       238\n",
            "     Neutral       0.40      0.47      0.44       727\n",
            "    Positive       0.49      0.55      0.52       846\n",
            "\n",
            "   micro avg       0.45      0.45      0.45      1811\n",
            "   macro avg       0.33      0.34      0.32      1811\n",
            "weighted avg       0.40      0.45      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  17  10]\n",
            " [  0  46  41]\n",
            " [  1  59 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.04      0.07        28\n",
            "     Neutral       0.38      0.53      0.44        87\n",
            "    Positive       0.68      0.65      0.66       170\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       285\n",
            "   macro avg       0.52      0.40      0.39       285\n",
            "weighted avg       0.57      0.55      0.54       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  2 106  59]\n",
            " [  5 387 297]\n",
            " [  1 345 631]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.02       167\n",
            "     Neutral       0.46      0.56      0.51       689\n",
            "    Positive       0.64      0.65      0.64       977\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1833\n",
            "   macro avg       0.45      0.41      0.39      1833\n",
            "weighted avg       0.54      0.56      0.54      1833\n",
            "\n",
            "----------------------- Starting Epoch 3-----------------------\n",
            "***** Started training at 2019-05-09 12:25:50.742597 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:30:03.691256 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  51  46]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.43      0.53      0.47        97\n",
            "    Positive       0.69      0.69      0.69       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.38      0.40      0.39       284\n",
            "weighted avg       0.55      0.58      0.56       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  99  46]\n",
            " [  1 411 343]\n",
            " [  3 387 590]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.01      0.01       146\n",
            "     Neutral       0.46      0.54      0.50       755\n",
            "    Positive       0.60      0.60      0.60       980\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1881\n",
            "   macro avg       0.42      0.38      0.37      1881\n",
            "weighted avg       0.51      0.53      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  35  14]\n",
            " [  2  59  55]\n",
            " [  0  57 116]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.02      0.04        50\n",
            "     Neutral       0.39      0.51      0.44       116\n",
            "    Positive       0.63      0.67      0.65       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.45      0.40      0.38       339\n",
            "weighted avg       0.50      0.52      0.49       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 135 102]\n",
            " [  2 345 380]\n",
            " [  5 391 450]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.12      0.00      0.01       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.33      0.34      0.32      1811\n",
            "weighted avg       0.40      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  17  10]\n",
            " [  2  43  42]\n",
            " [  0  62 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.68      0.64      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.45      0.39      0.38       285\n",
            "weighted avg       0.54      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3 102  62]\n",
            " [  4 393 292]\n",
            " [  3 358 616]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.02      0.03       167\n",
            "     Neutral       0.46      0.57      0.51       689\n",
            "    Positive       0.64      0.63      0.63       977\n",
            "\n",
            "   micro avg       0.55      0.55      0.55      1833\n",
            "   macro avg       0.47      0.41      0.39      1833\n",
            "weighted avg       0.54      0.55      0.53      1833\n",
            "\n",
            "----------------------- Starting Epoch 4-----------------------\n",
            "***** Started training at 2019-05-09 12:35:13.854729 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:39:34.033193 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  47  50]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.48      0.45        97\n",
            "    Positive       0.68      0.69      0.68       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.36      0.39      0.38       284\n",
            "weighted avg       0.53      0.56      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1 100  45]\n",
            " [  2 412 341]\n",
            " [  1 408 571]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.01       146\n",
            "     Neutral       0.45      0.55      0.49       755\n",
            "    Positive       0.60      0.58      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.36      1881\n",
            "weighted avg       0.51      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  32  17]\n",
            " [  1  60  55]\n",
            " [  0  61 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.52      0.45       116\n",
            "    Positive       0.61      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.39      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  0 130 108]\n",
            " [  2 341 384]\n",
            " [  5 392 449]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.50       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.29      0.33      0.31      1811\n",
            "weighted avg       0.38      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  0  45  42]\n",
            " [  0  65 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.04      0.07        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.67      0.62      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.67      0.39      0.38       285\n",
            "weighted avg       0.61      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3  94  70]\n",
            " [  5 384 300]\n",
            " [  3 376 598]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.27      0.02      0.03       167\n",
            "     Neutral       0.45      0.56      0.50       689\n",
            "    Positive       0.62      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1833\n",
            "   macro avg       0.45      0.40      0.38      1833\n",
            "weighted avg       0.52      0.54      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 5-----------------------\n",
            "***** Started training at 2019-05-09 12:44:37.466301 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:49:11.558773 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  48  49]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.42      0.49      0.45        97\n",
            "    Positive       0.68      0.69      0.68       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.37      0.39      0.38       284\n",
            "weighted avg       0.54      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  99  46]\n",
            " [  4 407 344]\n",
            " [  2 399 579]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.14      0.01      0.01       146\n",
            "     Neutral       0.45      0.54      0.49       755\n",
            "    Positive       0.60      0.59      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.40      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  33  16]\n",
            " [  1  59  56]\n",
            " [  0  60 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.51      0.44       116\n",
            "    Positive       0.61      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.39      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  2 127 109]\n",
            " [  3 344 380]\n",
            " [  6 383 457]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.18      0.01      0.02       238\n",
            "     Neutral       0.40      0.47      0.44       727\n",
            "    Positive       0.48      0.54      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.36      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  1  43  43]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.34      0.49      0.41        87\n",
            "    Positive       0.67      0.62      0.64       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.45      0.38      0.37       285\n",
            "weighted avg       0.54      0.52      0.51       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3  91  73]\n",
            " [  8 378 303]\n",
            " [  4 374 599]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.02      0.03       167\n",
            "     Neutral       0.45      0.55      0.49       689\n",
            "    Positive       0.61      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.42      0.39      0.38      1833\n",
            "weighted avg       0.51      0.53      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 6-----------------------\n",
            "***** Started training at 2019-05-09 12:54:14.134432 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:58:35.277134 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.42      0.51      0.46        97\n",
            "    Positive       0.68      0.69      0.69       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.37      0.40      0.38       284\n",
            "weighted avg       0.54      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  2  93  51]\n",
            " [  5 389 361]\n",
            " [  4 391 585]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.18      0.01      0.03       146\n",
            "     Neutral       0.45      0.52      0.48       755\n",
            "    Positive       0.59      0.60      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.40      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.50      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  30  19]\n",
            " [  1  60  55]\n",
            " [  0  60 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.52      0.45       116\n",
            "    Positive       0.60      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 119 116]\n",
            " [  4 328 395]\n",
            " [  6 377 463]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.01      0.02       238\n",
            "     Neutral       0.40      0.45      0.42       727\n",
            "    Positive       0.48      0.55      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.37      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  0  45  42]\n",
            " [  1  65 104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.04      0.07        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.67      0.61      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.51      0.39      0.38       285\n",
            "weighted avg       0.56      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  4  89  74]\n",
            " [  8 369 312]\n",
            " [  4 361 612]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.02      0.04       167\n",
            "     Neutral       0.45      0.54      0.49       689\n",
            "    Positive       0.61      0.63      0.62       977\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1833\n",
            "   macro avg       0.44      0.40      0.38      1833\n",
            "weighted avg       0.52      0.54      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 7-----------------------\n",
            "***** Started training at 2019-05-09 13:03:44.372830 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 13:08:37.924851 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  46 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.43      0.51      0.46        97\n",
            "    Positive       0.69      0.70      0.69       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.37      0.40      0.39       284\n",
            "weighted avg       0.54      0.58      0.56       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  5  91  50]\n",
            " [  9 389 357]\n",
            " [  6 385 589]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.03      0.06       146\n",
            "     Neutral       0.45      0.52      0.48       755\n",
            "    Positive       0.59      0.60      0.60       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.38      1881\n",
            "weighted avg       0.51      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  30  19]\n",
            " [  1  60  55]\n",
            " [  0  59 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.52      0.45       116\n",
            "    Positive       0.61      0.66      0.63       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.52      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 124 111]\n",
            " [  5 337 385]\n",
            " [ 11 375 460]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.16      0.01      0.02       238\n",
            "     Neutral       0.40      0.46      0.43       727\n",
            "    Positive       0.48      0.54      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.35      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  2  43  42]\n",
            " [  1  62 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.68      0.63      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.43      0.39      0.37       285\n",
            "weighted avg       0.54      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  7  85  75]\n",
            " [ 12 363 314]\n",
            " [  5 365 607]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.04      0.07       167\n",
            "     Neutral       0.45      0.53      0.48       689\n",
            "    Positive       0.61      0.62      0.62       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.45      0.40      0.39      1833\n",
            "weighted avg       0.52      0.53      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 8-----------------------\n",
            "***** Started training at 2019-05-09 13:13:48.450428 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 13:20:16.284210 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  53 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.40      0.51      0.45        97\n",
            "    Positive       0.68      0.66      0.67       164\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       284\n",
            "   macro avg       0.36      0.39      0.37       284\n",
            "weighted avg       0.53      0.55      0.54       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  4  92  50]\n",
            " [  6 393 356]\n",
            " [  5 400 575]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.27      0.03      0.05       146\n",
            "     Neutral       0.44      0.52      0.48       755\n",
            "    Positive       0.59      0.59      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.50      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  31  18]\n",
            " [  1  63  52]\n",
            " [  0  63 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.54      0.46       116\n",
            "    Positive       0.61      0.64      0.62       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 126 109]\n",
            " [  5 343 379]\n",
            " [  4 390 452]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.02       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.38      0.34      0.32      1811\n",
            "weighted avg       0.42      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  19   8]\n",
            " [  1  46  40]\n",
            " [  1  66 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.35      0.53      0.42        87\n",
            "    Positive       0.68      0.61      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.46      0.39      0.38       285\n",
            "weighted avg       0.55      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  6  89  72]\n",
            " [ 12 366 311]\n",
            " [  6 375 596]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.06       167\n",
            "     Neutral       0.44      0.53      0.48       689\n",
            "    Positive       0.61      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.43      0.39      0.38      1833\n",
            "weighted avg       0.51      0.53      0.51      1833\n",
            "\n",
            "----------------------- Starting Epoch 9-----------------------\n",
            "***** Started training at 2019-05-09 13:25:28.332869 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBRPuijQenOx",
        "colab_type": "code",
        "outputId": "a9c3ddc7-c7c3-4810-893c-f256de98746c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGQUnOsgfA9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}