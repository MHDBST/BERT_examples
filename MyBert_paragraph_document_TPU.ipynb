{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyBert_paragraph_document_TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/MyBert_paragraph_document_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN2obM4ocF_p",
        "colab_type": "code",
        "outputId": "ae54868f-b726-42b6-9898-1034912e0d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python2.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "('TPU address is', 'grpc://10.53.111.42:8470')\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 1914496351663025898),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8276504440342885961),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1549699291246788164),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11972616322496960610),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5397868222561746314),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9579595096658834797),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1798001008874768126),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15950101942496281266),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3525107893051908702),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8327010516390157153),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6726090179694740752)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 19:55:34.531393 140645025617792 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYgxPTVc5u7",
        "colab_type": "code",
        "outputId": "8f750995-cb14-4649-d0e7-ae4610d3ad6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "from bert import modeling\n",
        "from bert import run_classifier_with_tfhub\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 19:55:36.642353 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIQhrSUdE-H",
        "colab_type": "code",
        "outputId": "1b173fb4-3c9b-4acb-858a-d3bb2ecc4ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BUCKET = 'bert_example' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/combined/document/smallBERT-doc_512'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLhRyWYdPN-",
        "colab_type": "code",
        "outputId": "5f089f7d-3d71-4ec4-a067-20b1d9387c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\" ##Small Bert\n",
        "# # BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\" ##Big Bert\n",
        "# def create_tokenizer_from_hub_module():\n",
        "#   \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "#   with tf.Graph().as_default():\n",
        "#     bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "#     tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "#     with tf.Session() as sess:\n",
        "#       vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "#                                             tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "#   return bert.tokenization.FullTokenizer(\n",
        "#       vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "path = 'gs://bert_example/bert/uncased_L-12_H-768_A-12/vocab_tgt.txt'\n",
        "f_in = tf.gfile.GFile('gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt')\n",
        "f_out = tf.gfile.GFile(path,'w')\n",
        "lines = f_in.readlines()\n",
        "\n",
        "\n",
        "lines[1] = 'tgt\\n'\n",
        "for line in lines:\n",
        "  f_out.write(line)\n",
        "f_out.close()\n",
        "\n",
        "VOCAB_FILE = os.path.join('gs://bert_example/bert/uncased_L-12_H-768_A-12', 'vocab_tgt.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 19:56:11.674559 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLjuUqLdR_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 15.\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 20\n",
        "SAVE_SUMMARY_STEPS = 30\n",
        "keep_checkpoint_max = 15\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAPypEOdgpq",
        "colab_type": "code",
        "outputId": "8043c0f6-dbdc-462d-9917-53082e6fddcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# directory = '/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv'\n",
        "# data_train = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_train_v3.csv', encoding='latin-1')\n",
        "# data_dev = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_dev_v3.csv', encoding='latin-1')\n",
        "# data_test = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_random_test_v3.csv', encoding='latin-1')\n",
        "# data_test_fixed = pd.read_csv('/content/alldata_3Dec_7Dec_PS_reindex_fixed_test_v3.csv', encoding='latin-1')\n",
        "def fix_doc_tgt(doc):\n",
        "  new_doc = doc.replace('.[TGT]',' .\\n[TGT] ')\n",
        "  return new_doc\n",
        "\n",
        "## if pl is true, append  paragraph leve data and label to the output\n",
        "## if dl is true, append document leve data and label to the output\n",
        "#### Both pl and dl can not be false\n",
        "def load_paragraphs_documents(df,pl=True,dl=True,ent=False):\n",
        "    if not dl and not pl:\n",
        "      print('both document level label and paragaph level label is false, choose one of them a True')\n",
        "      return \n",
        "    labels = []\n",
        "    texts = []\n",
        "    doc_ids = []\n",
        "    uniq_ents = []\n",
        "    num_doc = 0\n",
        "    index = -1\n",
        "    for doc in df['MASKED_DOCUMENT']:\n",
        "      index += 1\n",
        "      docs = doc.split('\\n')\n",
        "      doc_length = len(docs)\n",
        "\n",
        "      if pd.isnull(df['Paragraph0'].iloc[index]):\n",
        "      # add documents with no paragraph labels as one document and its label to the input data dataframe\n",
        "        if dl:\n",
        "          labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "          texts.append(doc)\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "        num_doc +=1\n",
        "        \n",
        "        continue\n",
        "      try:\n",
        "        if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "\n",
        "          doc = fix_doc_tgt(doc)\n",
        "          docs = doc.split('\\n')\n",
        "          doc_length = len(docs)\n",
        "          if  doc_length != 16 and pd.notnull(df['Paragraph%s'%str(doc_length)].iloc[index]):\n",
        "            print('error on document %d'% df['DOCUMENT_INDEX'].iloc[index])\n",
        "            print('document length is %s'%str(doc_length))\n",
        "            continue\n",
        "      except:\n",
        "        print('this document has %d paragraphs %d' %(doc_length,df['DOCUMENT_INDEX'].iloc[index]))\n",
        "\n",
        "      if pl:\n",
        "        for i in range(doc_length):\n",
        "          doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "          if ent:\n",
        "            uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "          \n",
        "          texts.append(docs[i])\n",
        "          label_i = df['Paragraph%d'%i].iloc[index]\n",
        "          labels.append(label_i)\n",
        "        ### increase the effect of documents by adding the whole document per each paragraph :D\n",
        "          if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "      else:\n",
        "         if dl:\n",
        "          ## add the document text and its label to the input data after adding each paragraph and their labels\n",
        "            labels.append(df['TRUE_SENTIMENT'].iloc[index])\n",
        "            texts.append(doc)\n",
        "            doc_ids.append(df['DOCUMENT_INDEX'].iloc[index])\n",
        "            if ent:\n",
        "              uniq_ents.append(df['Unique_Entities'].iloc[index])\n",
        "         \n",
        "    print('number of one-paragraph docs: %d'%num_doc)\n",
        "    return(texts,labels,doc_ids,uniq_ents)\n",
        "  \n",
        "\n",
        "# train_file = open('/content/train_v5.csv')\n",
        "# # train_file = open('/content/train_v4.csv')\n",
        "# train_df = pd.read_csv(train_file)\n",
        "\n",
        "# # dev_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_dev_v3.csv')\n",
        "# dev_file = open('/content/dev_v5.csv')\n",
        "# dev_df = pd.read_csv(dev_file)\n",
        "\n",
        "# # test_random_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_random_test_v3.csv')\n",
        "# test_random_file = open('/content/random_test_v5.csv')\n",
        "# test_random_df = pd.read_csv(test_random_file)\n",
        "\n",
        "# # test_fixed_file = open('/content/alldata_3Dec_7Dec_PS_reindex_enclosed_masked_fixed_test_v3.csv')\n",
        "# test_fixed_file = open('/content/fixed_test_v5.csv')\n",
        "# test_fixed_df = pd.read_csv(test_fixed_file)\n",
        "\n",
        "\n",
        "# train_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/train_v5.csv'), encoding='latin-1')\n",
        "# dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/dev_v5.csv'), encoding='latin-1')\n",
        "# test_random_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/random_test_v5.csv'), encoding='latin-1')\n",
        "# test_fixed_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/fixed_test_v5.csv'), encoding='latin-1')\n",
        "\n",
        "train_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/train_v5.csv'), encoding='latin-1')\n",
        "dev_df = pd.read_csv(tf.gfile.GFile('gs://bert_example/data/dev_entities_v5.csv'), encoding='latin-1')\n",
        "test_random_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/random_test_entities_v5.csv'), encoding='latin-1')\n",
        "test_fixed_df= pd.read_csv(tf.gfile.GFile('gs://bert_example/data/fix_test_entities_v5.csv'), encoding='latin-1')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_file(df,dl=True,pl=True,ent=False):\n",
        "  data = {}\n",
        "  (texts,labels,doc_ids,uniq_ent ) = load_paragraphs_documents(df,dl=dl,pl=pl)\n",
        "  data[\"sentence\"] = texts\n",
        "  data[\"sentiment\"] =labels\n",
        "  data[\"doc_id\"] = doc_ids\n",
        "  if ent:\n",
        "    data[\"uniq_ent\"] = uniq_ent\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(df,index = None,dl=True,pl=True):\n",
        "  df_new = load_file(df,dl,pl)\n",
        "  pos_df = df_new[df_new['sentiment'] == 'Positive']\n",
        "  neg_df = df_new[df_new['sentiment'] == 'Negative']\n",
        "  neu_df = df_new[df_new['sentiment'] == 'Neutral']\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = -1\n",
        "  neu_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df,neu_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "### train should consist both paragraph level and document level labels\n",
        "# train_all = load_dataset(train_df,dl=True,pl=True)\n",
        "# train_par = load_dataset(train_df,dl=False,pl=True)\n",
        "train_doc = load_dataset(train_df,dl=True,pl=False)\n",
        "### two dev set, to test the model with paragraph level dev and document level dev\n",
        "dev_par = load_dataset(dev_df,pl=True,dl=False)\n",
        "dev_doc = load_dataset(dev_df,pl=False,dl=True)\n",
        "### two random tests like above\n",
        "test_par = load_dataset(test_random_df,pl=True,dl=False)\n",
        "test_doc = load_dataset(test_random_df,pl=False,dl=True)\n",
        "### two fixed tests like above\n",
        "test_fixed_par = load_dataset(test_fixed_df,pl=True,dl=False)\n",
        "test_fixed_doc = load_dataset(test_fixed_df,pl=False,dl=True)\n",
        "\n",
        "# print('Here the input consists of both paragraph level data and document level data')\n",
        "# print('Number of train inputs: %d\\n Number of paragraph dev inputs: %d,Number of document level dev inputs: %d\\n  \\\\\n",
        "#        Number of paragraph level fixed test inputs: %d, Number of document level fixed test inputs: %d \\n \\\\\n",
        "#        Number of paragraph level random test inputs: %d,  Number of document level random test inputs: %d'\n",
        "#       %(len(train_all),len(dev_par),len(dev_doc),len(test_par),len(test_doc),len(test_fixed_par),len(test_fixed_doc)))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of one-paragraph docs: 252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of one-paragraph docs: 45\n",
            "number of one-paragraph docs: 45\n",
            "number of one-paragraph docs: 52\n",
            "number of one-paragraph docs: 52\n",
            "error on document 2171\n",
            "document length is 6\n",
            "number of one-paragraph docs: 114\n",
            "error on document 2171\n",
            "document length is 6\n",
            "number of one-paragraph docs: 114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMNSsUHdsbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "label_list = [-1, 0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjkshmOdyI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "# train_InputExamples_all = train_all.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# train_InputExamples_par = train_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "train_InputExamples_doc = train_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "dev_InputExamples_par = dev_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_InputExamples_doc = dev_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_InputExamples_par = test_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_InputExamples_doc = test_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples_fixed_par = test_fixed_par.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "test_InputExamples_fixed_doc = test_fixed_doc.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n",
        "# num_train_steps = int(len(train_InputExamples_par) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "# num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEcWYt6jjaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_training, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels, bert_hub_module_handle):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use\n",
        "  # bert_outputs[\"sequence_output\"] instead.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps, use_tpu, bert_hub_module_handle):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,\n",
        "        bert_hub_module_handle)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
        "        loss = tf.metrics.mean(per_example_loss)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities})\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROg74SGti91n",
        "colab_type": "code",
        "outputId": "f91e7882-70e2-4aa9-82d5-07c48b0b52b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "num_train_steps = int(len(train_InputExamples_doc) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "# num_train_steps = int(1500 / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "# ITERATIONS_PER_LOOP = 1000 # I don't know what it is doing just decrease it to smaller value\n",
        "ITERATIONS_PER_LOOP = int(len(train_InputExamples_doc) / TRAIN_BATCH_SIZE) ## set as the number of iterations in each epoch \n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    keep_checkpoint_max=keep_checkpoint_max,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 19:56:39.966927 140645025617792 estimator.py:1984] Estimator's model_fn (<function model_fn at 0x7fea34f43a28>) includes params argument, but params are not passed to Estimator.\n",
            "I0802 19:56:39.972182 140645025617792 estimator.py:209] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.53.111.42:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 15, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fea419d0b90>, '_model_dir': 'gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512', '_protocol': None, '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=46, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fea34f21150>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.53.111.42:8470', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': u'grpc://10.53.111.42:8470'}\n",
            "I0802 19:56:39.974210 140645025617792 tpu_context.py:209] _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIFIsqg3I2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "ccb46b4a-25dd-4a22-a9e8-66cc789f37a1"
      },
      "source": [
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "def model_train(estimator):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  \n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_InputExamples_doc)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 19:56:42.008516 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0802 19:56:42.016154 140645025617792 run_classifier.py:774] Writing example 0 of 1501\n",
            "I0802 19:56:42.034332 140645025617792 run_classifier.py:461] *** Example ***\n",
            "I0802 19:56:42.035686 140645025617792 run_classifier.py:462] guid: None\n",
            "I0802 19:56:42.038002 140645025617792 run_classifier.py:464] tokens: [CLS] a dramatic video of the incident shows [ tgt ] at first standing calmly and calling nas ##sar a \" son of a bitch . \" the judge says she understands his anger but asks him not to use prof ##ani ##ty . two of the man ' s daughters lauren and madison mar ##grave ##s had just delivered victim impact statements . their sister morgan mar ##grave ##s wrote a statement that had been read aloud during nas ##sar ' s previous sentencing hearing in ing ##ham county mic ##h . where he was sentenced to up to 175 years in prison . \" i would ask you to as part of the sentencing to grant me five minutes in a locked room with this demon \" mar ##grave ##s says . the judge says she can ' t do that . \" would you give me one minute ? \" mar ##grave ##s asks . mar ##grave ##s then rushes across the courtroom toward nas ##sar . he is pushed back by nas ##sar ' s attorney . deputies tackle mar ##grave ##s to the floor his head striking a table . the officers tell him to put his hands behind his back as mar ##grave ##s continues protesting \" i want that son of a bitch ! \" and \" give me one minute with that bastard . \" the deputies pull him off the floor and begin to lead him out of the room . \" what if this had happened to you guys ' daughters ? \" mar ##grave ##s asks them . mar ##grave ##s turns back and says \" you haven ' t lived through it lady ! \" \" i understand mr . mar ##grave ##s ' frustration \" she says . \" but you cannot do this . this is not helping your children this is not helping your community this is not helping us . . . . use your words use your experiences to get him and to change . do not use physical violence . \" [SEP]\n",
            "I0802 19:56:42.041311 140645025617792 run_classifier.py:465] input_ids: 101 1037 6918 2678 1997 1996 5043 3065 1031 1 1033 2012 2034 3061 12885 1998 4214 17235 10286 1037 1000 2365 1997 1037 7743 1012 1000 1996 3648 2758 2016 19821 2010 4963 2021 5176 2032 2025 2000 2224 11268 7088 3723 1012 2048 1997 1996 2158 1005 1055 5727 10294 1998 7063 9388 12830 2015 2018 2074 5359 6778 4254 8635 1012 2037 2905 5253 9388 12830 2015 2626 1037 4861 2008 2018 2042 3191 12575 2076 17235 10286 1005 1055 3025 23280 4994 1999 13749 3511 2221 23025 2232 1012 2073 2002 2001 7331 2000 2039 2000 12862 2086 1999 3827 1012 1000 1045 2052 3198 2017 2000 2004 2112 1997 1996 23280 2000 3946 2033 2274 2781 1999 1037 5299 2282 2007 2023 5698 1000 9388 12830 2015 2758 1012 1996 3648 2758 2016 2064 1005 1056 2079 2008 1012 1000 2052 2017 2507 2033 2028 3371 1029 1000 9388 12830 2015 5176 1012 9388 12830 2015 2059 18545 2408 1996 20747 2646 17235 10286 1012 2002 2003 3724 2067 2011 17235 10286 1005 1055 4905 1012 11964 11147 9388 12830 2015 2000 1996 2723 2010 2132 8478 1037 2795 1012 1996 3738 2425 2032 2000 2404 2010 2398 2369 2010 2067 2004 9388 12830 2015 4247 21248 1000 1045 2215 2008 2365 1997 1037 7743 999 1000 1998 1000 2507 2033 2028 3371 2007 2008 8444 1012 1000 1996 11964 4139 2032 2125 1996 2723 1998 4088 2000 2599 2032 2041 1997 1996 2282 1012 1000 2054 2065 2023 2018 3047 2000 2017 4364 1005 5727 1029 1000 9388 12830 2015 5176 2068 1012 9388 12830 2015 4332 2067 1998 2758 1000 2017 4033 1005 1056 2973 2083 2009 3203 999 1000 1000 1045 3305 2720 1012 9388 12830 2015 1005 9135 1000 2016 2758 1012 1000 2021 2017 3685 2079 2023 1012 2023 2003 2025 5094 2115 2336 2023 2003 2025 5094 2115 2451 2023 2003 2025 5094 2149 1012 1012 1012 1012 2224 2115 2616 2224 2115 6322 2000 2131 2032 1998 2000 2689 1012 2079 2025 2224 3558 4808 1012 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.042808 140645025617792 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.044424 140645025617792 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.045685 140645025617792 run_classifier.py:468] label: 1 (id = 2)\n",
            "I0802 19:56:42.047880 140645025617792 run_classifier.py:461] *** Example ***\n",
            "I0802 19:56:42.049341 140645025617792 run_classifier.py:462] guid: None\n",
            "I0802 19:56:42.050410 140645025617792 run_classifier.py:464] tokens: [CLS] \" no , not the same , \" romney wrote . [SEP]\n",
            "I0802 19:56:42.052232 140645025617792 run_classifier.py:465] input_ids: 101 1000 2053 1010 2025 1996 2168 1010 1000 19615 2626 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.053673 140645025617792 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.055535 140645025617792 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.056885 140645025617792 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0802 19:56:42.097611 140645025617792 run_classifier.py:461] *** Example ***\n",
            "I0802 19:56:42.098829 140645025617792 run_classifier.py:462] guid: None\n",
            "I0802 19:56:42.101385 140645025617792 run_classifier.py:464] tokens: [CLS] an alien star in distant space seems to be acting very strangely . nasa ' s kepler space telescope caught sight of the star as part of the spacecraft ' s hunt for planets outside the solar system which are also known as ex ##op ##lane ##ts . kepler looks for ex ##op ##lane ##ts by seeing a dip in the light of a star presumably when a planet passes between its host star and the spacecraft . this eclipse produces a regular periodic dip in the star ##light that can help scientists character ##ize the planet . but when kepler caught sight of this star a called ki ##c 84 ##6 ##28 ##52 a it didn ' t see anything resembling a regular periodic dip in star ##light . instead the probe saw a more erratic dim ##ming and bright ##ening of the star nothing like what you ' d expect to see when a planet eclipse ##s . a we a d never seen anything like this star a tab ##eth ##a boy ##aj ##ian a post ##do ##c researcher at yale told the atlantic one of the first publications to report on the finding . a it was really weird . we thought it might be bad data or movement on the spacecraft but everything checked out . a so what could this weird star be ? numerous media outlets including to some extent the atlantic have latch ##ed onto the idea that it could be an a alien mega ##st ##ru ##cture a constructed by an extra ##ter ##rest ##rial civilization orbiting that star . such a structure would be large enough to periodically block light from the star . but the truth is that scientists aren ' t yet sure what could be causing the star to flicker from our perspective and an a alien mega ##st ##ru ##cture a is pretty far down on their list of possible explanations for the light curve . a it a s kind of funny i don a t really know how this ended up going from weird dip ##s in a light curve to alien structures a [ tgt ] told mas ##hab ##le in an interview . a the alien hypothesis should be a last resort . something we consider for fun rather than out of seriousness a mit ex ##op ##lane ##t scientist sara sea ##ger told mas ##hab ##le in an email . in reality there could be other very cool explanations for why the star seems so odd . some people have suggested that there could be a huge swarm of comets surrounding the star causing its light to dip in and out from kepler a s perspective . a planet that has broken up in orbit around the star [ tgt ] added could even cause the odd light dip ##s . [ tgt ] was also reminded of another star system [ tgt ] and [ tgt ] colleagues working with kepler have been studying recently . the strange solar system [SEP]\n",
            "I0802 19:56:42.103197 140645025617792 run_classifier.py:465] input_ids: 101 2019 7344 2732 1999 6802 2686 3849 2000 2022 3772 2200 13939 1012 9274 1005 1055 28219 2686 12772 3236 4356 1997 1996 2732 2004 2112 1997 1996 12076 1005 1055 5690 2005 11358 2648 1996 5943 2291 2029 2024 2036 2124 2004 4654 7361 20644 3215 1012 28219 3504 2005 4654 7361 20644 3215 2011 3773 1037 16510 1999 1996 2422 1997 1037 2732 10712 2043 1037 4774 5235 2090 2049 3677 2732 1998 1996 12076 1012 2023 13232 7137 1037 3180 15861 16510 1999 1996 2732 7138 2008 2064 2393 6529 2839 4697 1996 4774 1012 2021 2043 28219 3236 4356 1997 2023 2732 1037 2170 11382 2278 6391 2575 22407 25746 1037 2009 2134 1005 1056 2156 2505 15525 1037 3180 15861 16510 1999 2732 7138 1012 2612 1996 15113 2387 1037 2062 24122 11737 6562 1998 4408 7406 1997 1996 2732 2498 2066 2054 2017 1005 1040 5987 2000 2156 2043 1037 4774 13232 2015 1012 1037 2057 1037 1040 2196 2464 2505 2066 2023 2732 1037 21628 11031 2050 2879 13006 2937 1037 2695 3527 2278 10753 2012 7996 2409 1996 4448 2028 1997 1996 2034 5523 2000 3189 2006 1996 4531 1012 1037 2009 2001 2428 6881 1012 2057 2245 2009 2453 2022 2919 2951 2030 2929 2006 1996 12076 2021 2673 7039 2041 1012 1037 2061 2054 2071 2023 6881 2732 2022 1029 3365 2865 11730 2164 2000 2070 6698 1996 4448 2031 25635 2098 3031 1996 2801 2008 2009 2071 2022 2019 1037 7344 13164 3367 6820 14890 1037 3833 2011 2019 4469 3334 28533 14482 10585 26472 2008 2732 1012 2107 1037 3252 2052 2022 2312 2438 2000 18043 3796 2422 2013 1996 2732 1012 2021 1996 3606 2003 2008 6529 4995 1005 1056 2664 2469 2054 2071 2022 4786 1996 2732 2000 17909 2013 2256 7339 1998 2019 1037 7344 13164 3367 6820 14890 1037 2003 3492 2521 2091 2006 2037 2862 1997 2825 17959 2005 1996 2422 7774 1012 1037 2009 1037 1055 2785 1997 6057 1045 2123 1037 1056 2428 2113 2129 2023 3092 2039 2183 2013 6881 16510 2015 1999 1037 2422 7774 2000 7344 5090 1037 1031 1 1033 2409 16137 25459 2571 1999 2019 4357 1012 1037 1996 7344 10744 2323 2022 1037 2197 7001 1012 2242 2057 5136 2005 4569 2738 2084 2041 1997 27994 1037 10210 4654 7361 20644 2102 7155 7354 2712 4590 2409 16137 25459 2571 1999 2019 10373 1012 1999 4507 2045 2071 2022 2060 2200 4658 17959 2005 2339 1996 2732 3849 2061 5976 1012 2070 2111 2031 4081 2008 2045 2071 2022 1037 4121 21708 1997 27138 4193 1996 2732 4786 2049 2422 2000 16510 1999 1998 2041 2013 28219 1037 1055 7339 1012 1037 4774 2008 2038 3714 2039 1999 8753 2105 1996 2732 1031 1 1033 2794 2071 2130 3426 1996 5976 2422 16510 2015 1012 1031 1 1033 2001 2036 6966 1997 2178 2732 2291 1031 1 1033 1998 1031 1 1033 8628 2551 2007 28219 2031 2042 5702 3728 1012 1996 4326 5943 2291 102\n",
            "I0802 19:56:42.105299 140645025617792 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0802 19:56:42.109124 140645025617792 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.110169 140645025617792 run_classifier.py:468] label: 1 (id = 2)\n",
            "I0802 19:56:42.120893 140645025617792 run_classifier.py:461] *** Example ***\n",
            "I0802 19:56:42.124340 140645025617792 run_classifier.py:462] guid: None\n",
            "I0802 19:56:42.127893 140645025617792 run_classifier.py:464] tokens: [CLS] a viral image shows [ tgt ] wearing a t - shirt that reads , \" we march , ya all mad . the photo was from warm ##ups for [ tgt ] ' dec . 8 , 2014 , game in brooklyn . a viral image of [ tgt ] wearing a t - shirt about racial in ##e ##qui ##ty gained lots of traction on social media , but the word ##ing has been digitally replaced . pol ##iti ##fa ##ct first saw an image of [ tgt ] wearing the shirt on oct . 9 , 2017 , on the facebook page of occupy democrats , but the photo has spread all over the internet . we have to be better for one another , [ tgt ] said in 2014 . more recently , [ tgt ] chi ##ded president donald trump , who lashed out at the golden state warriors ' stephen curry for refusing to visit the white house with his championship team . [ tgt ] called trump a \" bum \" on twitter and later added that trump \" does n ' t understand the power that [ tgt ] has \" as president . the older photo of [ tgt ] has been altered to make it look like [ tgt ] was wearing something [ tgt ] was n ' t . [ tgt ] was n ' t wearing that shirt in that photo - l ##rb - although you can buy it from sc ##ads of online retailers - rr ##b - . [SEP]\n",
            "I0802 19:56:42.131323 140645025617792 run_classifier.py:465] input_ids: 101 1037 13434 3746 3065 1031 1 1033 4147 1037 1056 1011 3797 2008 9631 1010 1000 2057 2233 1010 8038 2035 5506 1012 1996 6302 2001 2013 4010 22264 2005 1031 1 1033 1005 11703 1012 1022 1010 2297 1010 2208 1999 6613 1012 1037 13434 3746 1997 1031 1 1033 4147 1037 1056 1011 3797 2055 5762 1999 2063 15549 3723 4227 7167 1997 16493 2006 2591 2865 1010 2021 1996 2773 2075 2038 2042 18397 2999 1012 14955 25090 7011 6593 2034 2387 2019 3746 1997 1031 1 1033 4147 1996 3797 2006 13323 1012 1023 1010 2418 1010 2006 1996 9130 3931 1997 11494 8037 1010 2021 1996 6302 2038 3659 2035 2058 1996 4274 1012 2057 2031 2000 2022 2488 2005 2028 2178 1010 1031 1 1033 2056 1999 2297 1012 2062 3728 1010 1031 1 1033 9610 5732 2343 6221 8398 1010 2040 25694 2041 2012 1996 3585 2110 6424 1005 4459 15478 2005 11193 2000 3942 1996 2317 2160 2007 2010 2528 2136 1012 1031 1 1033 2170 8398 1037 1000 26352 1000 2006 10474 1998 2101 2794 2008 8398 1000 2515 1050 1005 1056 3305 1996 2373 2008 1031 1 1033 2038 1000 2004 2343 1012 1996 3080 6302 1997 1031 1 1033 2038 2042 8776 2000 2191 2009 2298 2066 1031 1 1033 2001 4147 2242 1031 1 1033 2001 1050 1005 1056 1012 1031 1 1033 2001 1050 1005 1056 4147 2008 3797 1999 2008 6302 1011 1048 15185 1011 2348 2017 2064 4965 2009 2013 8040 19303 1997 3784 16629 1011 25269 2497 1011 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.134597 140645025617792 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.138408 140645025617792 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.142710 140645025617792 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0802 19:56:42.157921 140645025617792 run_classifier.py:461] *** Example ***\n",
            "I0802 19:56:42.159528 140645025617792 run_classifier.py:462] guid: None\n",
            "I0802 19:56:42.161281 140645025617792 run_classifier.py:464] tokens: [CLS] in [ tgt ] said va ##as ability to research medical marijuana is hampered by the fact that the drug is illegal federally . [ tgt ] came in response to an inquiry by 10 democrats on the house veterans ##a affairs committee . [ tgt ] asks shu ##lk ##in to commit the va to investigating whether medical marijuana can help veterans suffering from pts ##d and chronic pain and identify barriers to doing so . ava is committed to researching and developing effective ways to help veterans cope with post - traumatic stress disorder and chronic pain conditions a shu ##lk ##in wrote in a response to the members of congress . ah ##owe ##ver federal law restrict ##s va ##as ability to conduct research involving medical marijuana or to refer veterans to such projects . a ava ##as response not only failed to answer our simple question but they made a dish ##ear ##ten ##ing attempt to mis ##lea ##d me my colleagues and the veteran community in the process ##a by stating that the va is restricted from conducting marijuana research . wal ##z a veteran said he plans to send another letter to shu ##lk ##in asking for further cl ##ari ##fication . a spokesman for shu ##lk ##in pointed to the secretary ##as past comments on medical marijuana . shu ##lk ##in said in may amy opinion is is that some of the states that have put in appropriate controls there may be some evidence that this is beginning to be helpful . and wear ##e interested in looking at that and learning from that . but until the time that federal law changes we are not able a ##¦ to pre ##scribe medical marijuana for conditions that may be helpful . a shu ##lk ##in said va is offering a suite of alternative treatments for patients with pts ##d including yoga meditation ac ##up ##un ##cture and h ##yp ##nosis . the letter also said va has a program to reduce the amount of op ##io ##ids prescribed to patients with chronic pain ; since 2013 shu ##lk ##in wrote 33 percent fewer patients were receiving op ##io ##ids . [SEP]\n",
            "I0802 19:56:42.166382 140645025617792 run_classifier.py:465] input_ids: 101 1999 1031 1 1033 2056 12436 3022 3754 2000 2470 2966 16204 2003 22532 2011 1996 2755 2008 1996 4319 2003 6206 20892 1012 1031 1 1033 2234 1999 3433 2000 2019 9934 2011 2184 8037 2006 1996 2160 8244 2050 3821 2837 1012 1031 1 1033 5176 18454 13687 2378 2000 10797 1996 12436 2000 11538 3251 2966 16204 2064 2393 8244 6114 2013 19637 2094 1998 11888 3255 1998 6709 13500 2000 2725 2061 1012 10927 2003 5462 2000 20059 1998 4975 4621 3971 2000 2393 8244 11997 2007 2695 1011 19686 6911 8761 1998 11888 3255 3785 1037 18454 13687 2378 2626 1999 1037 3433 2000 1996 2372 1997 3519 1012 6289 29385 6299 2976 2375 21573 2015 12436 3022 3754 2000 6204 2470 5994 2966 16204 2030 2000 6523 8244 2000 2107 3934 1012 1037 10927 3022 3433 2025 2069 3478 2000 3437 2256 3722 3160 2021 2027 2081 1037 9841 14644 6528 2075 3535 2000 28616 19738 2094 2033 2026 8628 1998 1996 8003 2451 1999 1996 2832 2050 2011 5517 2008 1996 12436 2003 7775 2013 9283 16204 2470 1012 24547 2480 1037 8003 2056 2002 3488 2000 4604 2178 3661 2000 18454 13687 2378 4851 2005 2582 18856 8486 10803 1012 1037 14056 2005 18454 13687 2378 4197 2000 1996 3187 3022 2627 7928 2006 2966 16204 1012 18454 13687 2378 2056 1999 2089 6864 5448 2003 2003 2008 2070 1997 1996 2163 2008 2031 2404 1999 6413 7711 2045 2089 2022 2070 3350 2008 2023 2003 2927 2000 2022 14044 1012 1998 4929 2063 4699 1999 2559 2012 2008 1998 4083 2013 2008 1012 2021 2127 1996 2051 2008 2976 2375 3431 2057 2024 2025 2583 1037 29649 2000 3653 29234 2966 16204 2005 3785 2008 2089 2022 14044 1012 1037 18454 13687 2378 2056 12436 2003 5378 1037 7621 1997 4522 13441 2005 5022 2007 19637 2094 2164 13272 13804 9353 6279 4609 14890 1998 1044 22571 27109 1012 1996 3661 2036 2056 12436 2038 1037 2565 2000 5547 1996 3815 1997 6728 3695 9821 16250 2000 5022 2007 11888 3255 1025 2144 2286 18454 13687 2378 2626 3943 3867 8491 5022 2020 4909 6728 3695 9821 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.171058 140645025617792 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.173506 140645025617792 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:56:42.176356 140645025617792 run_classifier.py:468] label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOlSCl1sj18V",
        "colab_type": "code",
        "outputId": "a6412310-1880-4f50-cf80-743b6701f44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_train(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0802 19:56:57.364675 140645025617792 <ipython-input-11-872e781834da>:8]   Num steps = 690\n",
            "I0802 19:56:57.471960 140645025617792 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.53.111.42:8470) for TPU system metadata.\n",
            "I0802 19:56:57.492042 140645025617792 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0802 19:56:57.493510 140645025617792 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0802 19:56:57.494524 140645025617792 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0802 19:56:57.500462 140645025617792 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0802 19:56:57.502768 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1914496351663025898)\n",
            "I0802 19:56:57.506453 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1549699291246788164)\n",
            "I0802 19:56:57.508485 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11972616322496960610)\n",
            "I0802 19:56:57.510829 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5397868222561746314)\n",
            "I0802 19:56:57.511949 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9579595096658834797)\n",
            "I0802 19:56:57.513752 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1798001008874768126)\n",
            "I0802 19:56:57.515546 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15950101942496281266)\n",
            "I0802 19:56:57.517730 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3525107893051908702)\n",
            "I0802 19:56:57.519998 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8327010516390157153)\n",
            "I0802 19:56:57.522867 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6726090179694740752)\n",
            "I0802 19:56:57.524636 140645025617792 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8276504440342885961)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Started training at 2019-08-02 19:56:57.364440 *****\n",
            "  Num examples = 1501\n",
            "  Batch size = 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0802 19:56:57.575002 140645025617792 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0802 19:56:57.591480 140645025617792 estimator.py:1145] Calling model_fn.\n",
            "I0802 19:57:01.661158 140645025617792 <ipython-input-9-803c1b1dc593>:58] *** Features ***\n",
            "I0802 19:57:01.662622 140645025617792 <ipython-input-9-803c1b1dc593>:60]   name = input_ids, shape = (4, 512)\n",
            "I0802 19:57:01.667887 140645025617792 <ipython-input-9-803c1b1dc593>:60]   name = input_mask, shape = (4, 512)\n",
            "I0802 19:57:01.669009 140645025617792 <ipython-input-9-803c1b1dc593>:60]   name = label_ids, shape = (4,)\n",
            "I0802 19:57:01.672405 140645025617792 <ipython-input-9-803c1b1dc593>:60]   name = segment_ids, shape = (4, 512)\n",
            "E0802 19:57:24.516083 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.518553 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.520189 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.521444 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.526405 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.535067 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.543380 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.548151 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.551381 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.572348 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.575962 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.583728 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.587584 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.593230 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.596398 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.614001 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.617136 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.627441 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.631373 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.639247 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.642885 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.651403 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.654576 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.663525 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.668243 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.682738 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.686135 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.693850 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.697354 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.704751 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.708705 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.727916 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.731138 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.742773 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.746653 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.759346 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.763376 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.777348 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.780663 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.796379 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.799904 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.810733 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.813838 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.822220 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.825409 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.833853 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.837923 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.856808 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.860030 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.874052 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.878715 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.891588 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.898689 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.908277 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.911839 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.922198 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.925472 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.938122 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.942131 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.949990 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.953990 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.962647 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.967497 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.987238 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:24.996474 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.005351 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.010481 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.020071 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.026889 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.036086 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.040064 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.050307 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.055131 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.067080 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.071836 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.078789 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.082562 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.090060 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.094130 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.112888 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.119162 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.129348 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.132761 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.144578 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.149204 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.160736 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.165488 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.177000 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.181395 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.196613 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.199841 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.209075 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.213522 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.221168 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.224845 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.244647 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.248730 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.259588 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.262388 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.274077 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.278073 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.291452 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.298506 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.307178 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.312210 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.323796 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.330030 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.336607 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.341134 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.347596 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.351716 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.371944 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.377038 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.392908 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.397152 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.412373 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.418792 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.428399 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.431690 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.442672 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.447398 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.459188 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.462930 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.470784 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.475802 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.482883 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.486763 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.509469 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.512639 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.524825 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.530106 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.539894 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.544965 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.555124 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.559125 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.569156 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.572647 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.585572 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.588872 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.596864 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.600867 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.614077 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.617949 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.636872 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.641042 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.651995 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.656574 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.668376 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.673891 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.684619 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.688743 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.702526 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.708173 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.719439 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.723578 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.728959 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.732481 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.737963 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.747004 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.765988 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.773585 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.782989 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.787589 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.795944 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.800590 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.810590 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.815036 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.825870 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.829984 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.843322 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.846923 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.854378 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.858160 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.865077 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.869687 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.892966 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.896352 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.907243 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.911216 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.921422 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.926584 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.936748 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.939996 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.950361 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.955092 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.966150 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.970124 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.979499 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.983221 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.991703 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:25.995417 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.013164 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.017383 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.028112 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.031985 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.041521 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.046655 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.055922 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.060367 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.070938 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.074311 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.109986 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.113436 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.127664 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.131750 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.141938 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.145323 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0802 19:57:26.154495 140645025617792 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "I0802 19:57:26.268928 140645025617792 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0802 19:57:26.450069 140645025617792 deprecation.py:506] From <ipython-input-9-803c1b1dc593>:36: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0802 19:57:26.508500 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0802 19:57:26.511792 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0802 19:57:26.527564 140645025617792 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0802 19:57:26.549671 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0802 19:57:26.849981 140645025617792 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1205: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "W0802 19:57:31.851170 140645025617792 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "I0802 19:57:40.652828 140645025617792 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0802 19:57:41.042643 140645025617792 estimator.py:1147] Done calling model_fn.\n",
            "I0802 19:57:45.000101 140645025617792 tpu_estimator.py:499] TPU job name worker\n",
            "I0802 19:57:46.415446 140645025617792 monitored_session.py:240] Graph was finalized.\n",
            "I0802 19:58:01.923536 140645025617792 session_manager.py:500] Running local_init_op.\n",
            "I0802 19:58:03.057082 140645025617792 session_manager.py:502] Done running local_init_op.\n",
            "I0802 19:58:19.285173 140645025617792 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512/model.ckpt.\n",
            "W0802 19:58:40.936234 140645025617792 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0802 19:58:43.562367 140645025617792 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0802 19:58:43.564721 140645025617792 session_support.py:332] Installing graceful shutdown hook.\n",
            "I0802 19:58:43.576339 140645025617792 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0802 19:58:43.580745 140645025617792 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0802 19:58:43.588061 140645025617792 tpu_estimator.py:557] Init TPU system\n",
            "I0802 19:58:52.030422 140645025617792 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0802 19:58:52.032529 140643606648576 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0802 19:58:52.034609 140643580172032 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0802 19:58:53.169528 140645025617792 tpu_estimator.py:590] Enqueue next (46) batch(es) of data to infeed.\n",
            "I0802 19:58:53.171427 140645025617792 tpu_estimator.py:594] Dequeue next (46) batch(es) of data from outfeed.\n",
            "I0802 19:59:31.427819 140643580172032 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0802 19:59:37.632522 140645025617792 basic_session_run_hooks.py:606] Saving checkpoints for 46 into gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512/model.ckpt.\n",
            "I0802 20:00:02.505605 140645025617792 basic_session_run_hooks.py:262] loss = 1.0788243, step = 46\n",
            "I0802 20:00:02.510924 140645025617792 tpu_estimator.py:590] Enqueue next (46) batch(es) of data to infeed.\n",
            "I0802 20:00:02.513829 140645025617792 tpu_estimator.py:594] Dequeue next (46) batch(es) of data from outfeed.\n",
            "I0802 20:00:17.784354 140645025617792 basic_session_run_hooks.py:606] Saving checkpoints for 92 into gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512/model.ckpt.\n",
            "I0802 20:00:34.091583 140645025617792 tpu_estimator.py:2159] global_step/sec: 1.4565\n",
            "I0802 20:00:34.094058 140645025617792 tpu_estimator.py:2160] examples/sec: 46.608\n",
            "I0802 20:00:34.102812 140645025617792 tpu_estimator.py:590] Enqueue next (46) batch(es) of data to infeed.\n",
            "I0802 20:00:34.104136 140645025617792 tpu_estimator.py:594] Dequeue next (46) batch(es) of data from outfeed.\n",
            "I0802 20:00:35.367341 140643580172032 tpu_estimator.py:275] Outfeed finished for iteration (2, 0)\n",
            "I0802 20:00:41.495110 140645025617792 basic_session_run_hooks.py:606] Saving checkpoints for 138 into gs://bert_example/bert-tfhub/models/combined/document/smallBERT-doc_512/model.ckpt.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatznvlRsQ-2",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation and Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmmTPdYsTSE",
        "colab_type": "code",
        "outputId": "c125610e-20f7-4481-b929-d2e6bd9ee2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "eval_examples = dev_InputExamples_doc\n",
        "eval_features = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 19:33:27.530390 140578879141760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0802 19:33:27.532691 140578879141760 run_classifier.py:774] Writing example 0 of 284\n",
            "I0802 19:33:27.555948 140578879141760 run_classifier.py:461] *** Example ***\n",
            "I0802 19:33:27.557746 140578879141760 run_classifier.py:462] guid: None\n",
            "I0802 19:33:27.561686 140578879141760 run_classifier.py:464] tokens: [CLS] \" people are very nervous very concerned that the person who did this crime is still loose \" said the rev . [ tgt ] a member of the city ' s african american pastoral alliance . \" we ' re talking about organizing a meeting and inviting the whole community to come and share their concerns with counselor ##s to try to relieve people of stress and anxiety . \" robinson said jeremiah who was called jj loved basketball and was a regular at the boys and girls club . \" everyone down there is crying \" robinson said . \" he was well - manner ##ed respectful happy all the time . \" \" he was everybody ' s friend he always had a smile on his face and he played a major role in the club he was a qui ##ntes ##sen ##tial club kid he came here every day often times he was the last to leave \" bun ##tic ##h said . robinson said \" everyone in the neighborhood loved and respected \" shan ##ta myers who he said worked as a bus monitor at one time . [SEP]\n",
            "I0802 19:33:27.564507 140578879141760 run_classifier.py:465] input_ids: 101 1000 2111 2024 2200 6091 2200 4986 2008 1996 2711 2040 2106 2023 4126 2003 2145 6065 1000 2056 1996 7065 1012 1031 1 1033 1037 2266 1997 1996 2103 1005 1055 3060 2137 13645 4707 1012 1000 2057 1005 2128 3331 2055 10863 1037 3116 1998 15085 1996 2878 2451 2000 2272 1998 3745 2037 5936 2007 17220 2015 2000 3046 2000 15804 2111 1997 6911 1998 10089 1012 1000 6157 2056 17526 2040 2001 2170 29017 3866 3455 1998 2001 1037 3180 2012 1996 3337 1998 3057 2252 1012 1000 3071 2091 2045 2003 6933 1000 6157 2056 1012 1000 2002 2001 2092 1011 5450 2098 26438 3407 2035 1996 2051 1012 1000 1000 2002 2001 7955 1005 1055 2767 2002 2467 2018 1037 2868 2006 2010 2227 1998 2002 2209 1037 2350 2535 1999 1996 2252 2002 2001 1037 21864 17340 5054 20925 2252 4845 2002 2234 2182 2296 2154 2411 2335 2002 2001 1996 2197 2000 2681 1000 21122 4588 2232 2056 1012 6157 2056 1000 3071 1999 1996 5101 3866 1998 9768 1000 17137 2696 13854 2040 2002 2056 2499 2004 1037 3902 8080 2012 2028 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.566065 140578879141760 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.571643 140578879141760 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.573312 140578879141760 run_classifier.py:468] label: 1 (id = 2)\n",
            "I0802 19:33:27.584528 140578879141760 run_classifier.py:461] *** Example ***\n",
            "I0802 19:33:27.586141 140578879141760 run_classifier.py:462] guid: None\n",
            "I0802 19:33:27.589013 140578879141760 run_classifier.py:464] tokens: [CLS] \" oracle had ample opportunity to question [ tgt ] during [ tgt ] sworn deposition in october 2008 and chose not to include [ tgt ] as a trial witness until [ tgt ] was named ceo of hp \" hp said in a statement . \" given [ tgt ] ' s limited knowledge of and role in the matter oracle ' s last - minute effort to require [ tgt ] to appear live at trial is no more than an effort to hara ##ss [ tgt ] and interfere with [ tgt ] duties and responsibilities as hp ' s ceo \" hp said . ellison said in his statement tuesday that a major portion of the theft occurred while [ tgt ] was ceo of sap and hp chairman ray lane may try to keep him from testify ##ing in the trial . \" if hp keeps [ tgt ] far from hp headquarters we cannot sub ##po ##ena [ tgt ] to testify at that trial \" ellison wrote . [SEP]\n",
            "I0802 19:33:27.591306 140578879141760 run_classifier.py:465] input_ids: 101 1000 14721 2018 20851 4495 2000 3160 1031 1 1033 2076 1031 1 1033 10741 19806 1999 2255 2263 1998 4900 2025 2000 2421 1031 1 1033 2004 1037 3979 7409 2127 1031 1 1033 2001 2315 5766 1997 6522 1000 6522 2056 1999 1037 4861 1012 1000 2445 1031 1 1033 1005 1055 3132 3716 1997 1998 2535 1999 1996 3043 14721 1005 1055 2197 1011 3371 3947 2000 5478 1031 1 1033 2000 3711 2444 2012 3979 2003 2053 2062 2084 2019 3947 2000 18820 4757 1031 1 1033 1998 15115 2007 1031 1 1033 5704 1998 10198 2004 6522 1005 1055 5766 1000 6522 2056 1012 21513 2056 1999 2010 4861 9857 2008 1037 2350 4664 1997 1996 11933 4158 2096 1031 1 1033 2001 5766 1997 20066 1998 6522 3472 4097 4644 2089 3046 2000 2562 2032 2013 19919 2075 1999 1996 3979 1012 1000 2065 6522 7906 1031 1 1033 2521 2013 6522 4075 2057 3685 4942 6873 8189 1031 1 1033 2000 19919 2012 2008 3979 1000 21513 2626 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.593274 140578879141760 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.595304 140578879141760 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.597765 140578879141760 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0802 19:33:27.605863 140578879141760 run_classifier.py:461] *** Example ***\n",
            "I0802 19:33:27.608165 140578879141760 run_classifier.py:462] guid: None\n",
            "I0802 19:33:27.609850 140578879141760 run_classifier.py:464] tokens: [CLS] image copyright af ##p / get ##ty images image capt ##ion the couple announced their engagement in november prince harry and meg ##han mark ##le ' s wedding will be held on saturday 19 may 2018 kensington palace has announced . media playback is un ##su ##pp ##orted on your device media capt ##ion harry and meg ##han : engagement interview in full prince harry made a public appearance at sand ##hurst earlier on friday - 11 years after he graduated from the military academy - for the sovereign ' s parade . image copyright pa image capt ##ion [ tgt ] on [ tgt ] first official engagement in nottingham reacting to the clash with the cup match an fa spokesman said the organisation was \" delighted \" for prince harry and ms mark ##le . [SEP]\n",
            "I0802 19:33:27.611663 140578879141760 run_classifier.py:465] input_ids: 101 3746 9385 21358 2361 1013 2131 3723 4871 3746 14408 3258 1996 3232 2623 2037 8147 1999 2281 3159 4302 1998 12669 4819 2928 2571 1005 1055 5030 2097 2022 2218 2006 5095 2539 2089 2760 17775 4186 2038 2623 1012 2865 18245 2003 4895 6342 9397 15613 2006 2115 5080 2865 14408 3258 4302 1998 12669 4819 1024 8147 4357 1999 2440 3159 4302 2081 1037 2270 3311 2012 5472 10510 3041 2006 5958 1011 2340 2086 2044 2002 3852 2013 1996 2510 2914 1011 2005 1996 11074 1005 1055 7700 1012 3746 9385 6643 3746 14408 3258 1031 1 1033 2006 1031 1 1033 2034 2880 8147 1999 11331 24868 2000 1996 13249 2007 1996 2452 2674 2019 6904 14056 2056 1996 5502 2001 1000 15936 1000 2005 3159 4302 1998 5796 2928 2571 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.618949 140578879141760 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.620754 140578879141760 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.623230 140578879141760 run_classifier.py:468] label: 1 (id = 2)\n",
            "I0802 19:33:27.656685 140578879141760 run_classifier.py:461] *** Example ***\n",
            "I0802 19:33:27.658351 140578879141760 run_classifier.py:462] guid: None\n",
            "I0802 19:33:27.661051 140578879141760 run_classifier.py:464] tokens: [CLS] as [ tgt ] are praising the bravery of the women who came forward a in particular ashley judd who was the first celebrity to speak on the record in last week ' s new york times report . ai said no a lot of ways a lot of times and he always came back at me with some new ask a the actress told the new york times describing shooting down a persistent wei ##nstein . \" it was all this bargaining this coe ##rc ##ive bargaining . a in wake of judd ' s courageous decision to speak up about wei ##nstein ' s ha ##rs ##sm ##ent and reveal her identity women in hollywood are praising her bravery and strength . ( we ' ve contacted judd for comment . ) huge respect for @ ashley ##ju ##dd and all the women who broke their silence for the article on [ tgt ] . brave . a susan sara ##ndon ( @ susan ##sar ##ando ##n ) october 7 2017 very brave of @ ashley ##ju ##dd to tell her story of harassment by [ tgt ] to the ny ##t . no upside except making the world a better place . a jake tap ##per ( @ jake ##ta ##pper ) october 5 2017 since the original report a which featured accounts from eight women a came out the new yorker published a sc ##athing expo ##sa ##© in which more women accused wei ##nstein of not just harassment but sexual assault . ( he ' s denied rape allegations through a spokesperson . ) on tuesday afternoon the new york times published another article featuring interviews from a more celebrities a g ##wyn ##eth pal ##tro ##w angelina jo ##lie rosa ##nna ar ##quette katherine kendall and more a who all accused wei ##nstein of sexual harassment . in response to the women revealing their chilling accounts of wei ##nstein ' s behavior other women were eager to publicly offer their support and celebrate the survivors ' bravery . the t ##wee ##ts were notable both for their supportive nature in a cut ##th ##ro ##at industry and on a social media platform known more and more for bullying and hate speech than expressions of solidarity . very proud of my sisters in spirit who had the courage to break the silence . very hard for me - more so for others . [ tgt ] took [ tgt ] power back ! ! https : / / t . co / tu ##6 ##hr ##1 ##e ##6 ##q ##m a mira so ##r ##vino ( @ mira ##sor ##vino ) october 10 2017 statement from secretary clinton on [ tgt ] : pic . twitter . com / l ##1 ##l ##2 ##wl ##9 ##l ##0 ##i a nick merrill ( @ nick ##mer ##rill ) october 10 2017 following the original report four members of [ tgt ] ' s ( all - male ) board resigned and [ tgt ] was fired [SEP]\n",
            "I0802 19:33:27.663085 140578879141760 run_classifier.py:465] input_ids: 101 2004 1031 1 1033 2024 15838 1996 16534 1997 1996 2308 2040 2234 2830 1037 1999 3327 9321 20128 2040 2001 1996 2034 8958 2000 3713 2006 1996 2501 1999 2197 2733 1005 1055 2047 2259 2335 3189 1012 9932 2056 2053 1037 2843 1997 3971 1037 2843 1997 2335 1998 2002 2467 2234 2067 2012 2033 2007 2070 2047 3198 1037 1996 3883 2409 1996 2047 2259 2335 7851 5008 2091 1037 14516 11417 15493 1012 1000 2009 2001 2035 2023 21990 2023 24873 11890 3512 21990 1012 1037 1999 5256 1997 20128 1005 1055 26103 3247 2000 3713 2039 2055 11417 15493 1005 1055 5292 2869 6491 4765 1998 7487 2014 4767 2308 1999 5365 2024 15838 2014 16534 1998 3997 1012 1006 2057 1005 2310 11925 20128 2005 7615 1012 1007 4121 4847 2005 1030 9321 9103 14141 1998 2035 1996 2308 2040 3631 2037 4223 2005 1996 3720 2006 1031 1 1033 1012 9191 1012 1037 6294 7354 19333 1006 1030 6294 10286 28574 2078 1007 2255 1021 2418 2200 9191 1997 1030 9321 9103 14141 2000 2425 2014 2466 1997 16011 2011 1031 1 1033 2000 1996 6396 2102 1012 2053 14961 3272 2437 1996 2088 1037 2488 2173 1012 1037 5180 11112 4842 1006 1030 5180 2696 18620 1007 2255 1019 2418 2144 1996 2434 3189 1037 2029 2956 6115 2013 2809 2308 1037 2234 2041 1996 2047 19095 2405 1037 8040 22314 16258 3736 29652 1999 2029 2062 2308 5496 11417 15493 1997 2025 2074 16011 2021 4424 6101 1012 1006 2002 1005 1055 6380 9040 9989 2083 1037 15974 1012 1007 2006 9857 5027 1996 2047 2259 2335 2405 2178 3720 3794 7636 2013 1037 2062 12330 1037 1043 11761 11031 14412 13181 2860 23847 8183 8751 9508 9516 12098 29416 9477 14509 1998 2062 1037 2040 2035 5496 11417 15493 1997 4424 16011 1012 1999 3433 2000 1996 2308 8669 2037 27017 6115 1997 11417 15493 1005 1055 5248 2060 2308 2020 9461 2000 7271 3749 2037 2490 1998 8439 1996 8643 1005 16534 1012 1996 1056 28394 3215 2020 3862 2119 2005 2037 16408 3267 1999 1037 3013 2705 3217 4017 3068 1998 2006 1037 2591 2865 4132 2124 2062 1998 2062 2005 18917 1998 5223 4613 2084 11423 1997 14657 1012 2200 7098 1997 2026 5208 1999 4382 2040 2018 1996 8424 2000 3338 1996 4223 1012 2200 2524 2005 2033 1011 2062 2061 2005 2500 1012 1031 1 1033 2165 1031 1 1033 2373 2067 999 999 16770 1024 1013 1013 1056 1012 2522 1013 10722 2575 8093 2487 2063 2575 4160 2213 1037 18062 2061 2099 26531 1006 1030 18062 21748 26531 1007 2255 2184 2418 4861 2013 3187 7207 2006 1031 1 1033 1024 27263 1012 10474 1012 4012 1013 1048 2487 2140 2475 13668 2683 2140 2692 2072 1037 4172 16239 1006 1030 4172 5017 24714 1007 2255 2184 2418 2206 1996 2434 3189 2176 2372 1997 1031 1 1033 1005 1055 1006 2035 1011 3287 1007 2604 5295 1998 1031 1 1033 2001 5045 102\n",
            "I0802 19:33:27.665327 140578879141760 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0802 19:33:27.667473 140578879141760 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.668992 140578879141760 run_classifier.py:468] label: 1 (id = 2)\n",
            "I0802 19:33:27.706068 140578879141760 run_classifier.py:461] *** Example ***\n",
            "I0802 19:33:27.707691 140578879141760 run_classifier.py:462] guid: None\n",
            "I0802 19:33:27.709944 140578879141760 run_classifier.py:464] tokens: [CLS] the washington post ' s margaret sullivan discusses the legacy of [ tgt ] . ( thomas johnson / the washington post ) [ tgt ] who died wednesday at the playboy mansion in los angeles after spending most of his life there in a silk robe grew up in the 1920s on a quiet chicago street lit with gas lamps . horse - drawn wagons delivered milk and ice . trains whistled in the distance . ao ##ne of the love ##liest sounds in the world a [ tgt ] said decades later . it was there during the great depression that he ##f ##ner as extraordinary life took shape a not so much because of what happened there but what didn ##at : affection . emotion . love . and certainly not sex although his parents did manage to have two children . the rules of the home were strict a no swearing no drinking no playing cards no radio on sundays a and reflected the pu ##ritan upbringing of [ tgt ] parents who grew up on nebraska farms . his father glenn was an accountant at an aluminum company . his mother grace was a school teacher . grace was sexually dem ##ure to a degree notable even for that time gay tales ##e wrote in at ##hy neighbor ##as wife a his best ##sell ##ing book about sex in america which chronicle ##d [ tgt ] rise . as for glenn he was aa remote rep ##ressed man who seldom revealed his feelings to his family and spent most of his time working quietly ##a ##¦ six days a week sometimes seven . a [ tgt ] was au ##nat ##tra ##ctive and shy a tales ##e wrote and he often slipped away from the present even though he was right there : if the boys became restless indoors they were permitted to sit at the work ##ben ##ch in the backyard where they could draw pictures or sc ##ul ##pt with the colored clay that [ tgt ] provided . [ tgt ] who was fa ##ci ##le in drawing and sc ##ul ##pt ##uring was more than diverted by these activities ##ah ##e often seemed entrance ##d by the clay figures of [ tgt ] creation relating to them with a special intimacy and if at such times [ tgt ] mother called to [ tgt ] from the back door [ tgt ] would not hear her . though [ tgt ] was bright [ tgt ] did not excel in school often sitting at [ tgt ] desk and drawing cartoons for hours . [ tgt ] seemed so totally di ##sen ##ga ##ged that teachers wondered if [ tgt ] could hear them . not knowing what to do [ tgt ] had [ tgt ] examined by child psychologists at the illinois institute for juvenile research . [ donald trump was proud of his 1990 playboy cover . [ tgt ] not so much . ] [ tgt ] [SEP]\n",
            "I0802 19:33:27.713584 140578879141760 run_classifier.py:465] input_ids: 101 1996 2899 2695 1005 1055 5545 7624 15841 1996 8027 1997 1031 1 1033 1012 1006 2726 3779 1013 1996 2899 2695 1007 1031 1 1033 2040 2351 9317 2012 1996 18286 7330 1999 3050 3349 2044 5938 2087 1997 2010 2166 2045 1999 1037 6953 11111 3473 2039 1999 1996 6641 2006 1037 4251 3190 2395 5507 2007 3806 14186 1012 3586 1011 4567 14525 5359 6501 1998 3256 1012 4499 26265 1999 1996 3292 1012 20118 2638 1997 1996 2293 21292 4165 1999 1996 2088 1037 1031 1 1033 2056 5109 2101 1012 2009 2001 2045 2076 1996 2307 6245 2008 2002 2546 3678 2004 9313 2166 2165 4338 1037 2025 2061 2172 2138 1997 2054 3047 2045 2021 2054 2134 4017 1024 12242 1012 7603 1012 2293 1012 1998 5121 2025 3348 2348 2010 3008 2106 6133 2000 2031 2048 2336 1012 1996 3513 1997 1996 2188 2020 9384 1037 2053 25082 2053 5948 2053 2652 5329 2053 2557 2006 14803 1037 1998 7686 1996 16405 25279 24615 1997 1031 1 1033 3008 2040 3473 2039 2006 8506 8623 1012 2010 2269 9465 2001 2019 17907 2012 2019 13061 2194 1012 2010 2388 4519 2001 1037 2082 3836 1012 4519 2001 12581 17183 5397 2000 1037 3014 3862 2130 2005 2008 2051 5637 7122 2063 2626 1999 2012 10536 11429 3022 2564 1037 2010 2190 23836 2075 2338 2055 3348 1999 2637 2029 9519 2094 1031 1 1033 4125 1012 2004 2005 9465 2002 2001 9779 6556 16360 16119 2158 2040 15839 3936 2010 5346 2000 2010 2155 1998 2985 2087 1997 2010 2051 2551 5168 2050 29649 2416 2420 1037 2733 2823 2698 1012 1037 1031 1 1033 2001 8740 19833 6494 15277 1998 11004 1037 7122 2063 2626 1998 2002 2411 5707 2185 2013 1996 2556 2130 2295 2002 2001 2157 2045 1024 2065 1996 3337 2150 15035 24274 2027 2020 7936 2000 4133 2012 1996 2147 10609 2818 1999 1996 16125 2073 2027 2071 4009 4620 2030 8040 5313 13876 2007 1996 6910 5726 2008 1031 1 1033 3024 1012 1031 1 1033 2040 2001 6904 6895 2571 1999 5059 1998 8040 5313 13876 12228 2001 2062 2084 18356 2011 2122 3450 4430 2063 2411 2790 4211 2094 2011 1996 5726 4481 1997 1031 1 1033 4325 8800 2000 2068 2007 1037 2569 20893 1998 2065 2012 2107 2335 1031 1 1033 2388 2170 2000 1031 1 1033 2013 1996 2067 2341 1031 1 1033 2052 2025 2963 2014 1012 2295 1031 1 1033 2001 4408 1031 1 1033 2106 2025 24970 1999 2082 2411 3564 2012 1031 1 1033 4624 1998 5059 13941 2005 2847 1012 1031 1 1033 2790 2061 6135 4487 5054 3654 5999 2008 5089 4999 2065 1031 1 1033 2071 2963 2068 1012 2025 4209 2054 2000 2079 1031 1 1033 2018 1031 1 1033 8920 2011 2775 25428 2012 1996 4307 2820 2005 11799 2470 1012 1031 6221 8398 2001 7098 1997 2010 2901 18286 3104 1012 1031 1 1033 2025 2061 2172 1012 1033 1031 1 1033 102\n",
            "I0802 19:33:27.716350 140578879141760 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0802 19:33:27.718522 140578879141760 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0802 19:33:27.720401 140578879141760 run_classifier.py:468] label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmyd1fRsUjr",
        "colab_type": "code",
        "outputId": "40162e9d-999d-4ec0-a1ce-f685d66cb80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_eval(estimator_from_tfhub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0719 02:05:45.675978 140373645940608 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.82.167.234:8470) for TPU system metadata.\n",
            "I0719 02:05:45.698952 140373645940608 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0719 02:05:45.701061 140373645940608 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0719 02:05:45.703181 140373645940608 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0719 02:05:45.704525 140373645940608 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0719 02:05:45.705892 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7564403420079188309)\n",
            "I0719 02:05:45.710959 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12156707590594037807)\n",
            "I0719 02:05:45.712759 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16051020770761821062)\n",
            "I0719 02:05:45.713984 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18059181317935523536)\n",
            "I0719 02:05:45.716727 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15445962129192916691)\n",
            "I0719 02:05:45.718995 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2621913768033490377)\n",
            "I0719 02:05:45.722189 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6107338540638761086)\n",
            "I0719 02:05:45.724347 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5836450958984659141)\n",
            "I0719 02:05:45.727519 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16885552318047667012)\n",
            "I0719 02:05:45.729695 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8229738340488899913)\n",
            "I0719 02:05:45.733422 140373645940608 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4658655054781291384)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Started evaluation at 2019-07-19 02:05:45.673116 *****\n",
            "  Num examples = 284\n",
            "  Batch size = 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0719 02:05:46.101896 140373645940608 estimator.py:1145] Calling model_fn.\n",
            "I0719 02:05:46.983405 140373645940608 <ipython-input-9-803c1b1dc593>:58] *** Features ***\n",
            "I0719 02:05:46.984807 140373645940608 <ipython-input-9-803c1b1dc593>:60]   name = input_ids, shape = (1, 512)\n",
            "I0719 02:05:46.986227 140373645940608 <ipython-input-9-803c1b1dc593>:60]   name = input_mask, shape = (1, 512)\n",
            "I0719 02:05:46.989458 140373645940608 <ipython-input-9-803c1b1dc593>:60]   name = label_ids, shape = (1,)\n",
            "I0719 02:05:46.991205 140373645940608 <ipython-input-9-803c1b1dc593>:60]   name = segment_ids, shape = (1, 512)\n",
            "E0719 02:05:53.560471 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/input_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.563992 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/input_mask) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.568542 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/segment_ids) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.576977 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/mlm_positions) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.589092 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/word_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.601831 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/token_type_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.614360 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/position_embeddings) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.622792 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.626487 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/embeddings/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.650171 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.654042 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.665330 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.669128 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.679894 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.683489 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.700787 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.704394 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.711698 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.717181 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.729579 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.734828 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.747514 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.751056 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.759588 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.767695 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_0/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.783500 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.788141 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.796972 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.801744 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.811198 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.816067 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.836699 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.840940 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.852461 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.857310 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.871875 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.877806 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.890122 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.894983 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.903723 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.909668 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_1/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.924365 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.928232 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.941194 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.947694 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.956459 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.960382 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.978257 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.982449 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.989190 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:53.993227 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.006203 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.010698 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.022367 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.026201 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.032250 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.037868 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_2/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.052409 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.059396 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.070811 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.074831 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.082135 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.086069 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.106914 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.111200 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.119601 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.125066 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.142858 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.148062 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.161917 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.165368 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.171313 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.175339 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_3/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.190021 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.194170 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.203960 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.208234 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.216721 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.220505 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.240974 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.249444 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.260797 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.264415 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.278139 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.283171 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.295546 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.299335 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.305680 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.316351 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_4/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.331058 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.336133 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.351809 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.355624 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.364923 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.369556 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.388477 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.392488 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.402287 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.406797 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.419440 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.424386 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.437011 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.441119 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.448380 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.453877 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_5/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.469655 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.473968 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.486836 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.495126 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.504956 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.512923 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.529942 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.537931 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.546325 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.552892 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.564485 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.569973 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.582573 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.587600 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.595932 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.602333 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_6/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.621176 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.626858 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.636698 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.644377 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.652721 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.659873 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.677046 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.684588 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.696619 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.701217 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.716317 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.723614 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.735686 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.742649 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.749397 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.754252 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_7/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.769634 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.774770 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.789338 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.792855 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.803246 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.807905 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.826688 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.831856 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.840688 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.845654 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.858532 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.863814 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.876018 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.879865 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.888381 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.893853 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_8/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.910279 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.916296 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.928723 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.933245 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.945720 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.949563 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.968203 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.972305 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.982413 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.986296 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:54.999772 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.009069 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.020252 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.024945 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.037796 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.043518 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_9/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.057499 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.064790 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.074507 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.078566 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.090130 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.097481 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.119131 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.123599 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.133399 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.139206 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.152482 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.157900 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.173042 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.182629 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.192960 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.197438 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.212735 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.217593 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/query/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.564776 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.571024 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/key/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.581491 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.585977 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/self/value/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.603982 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.608268 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.614320 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.618506 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/attention/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.632719 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.637800 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/intermediate/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.650099 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.654167 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.658772 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.665091 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/encoder/layer_11/output/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.701734 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.706000 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/bert/pooler/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.722541 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/kernel) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.727025 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/dense/bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.739794 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/beta) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.744117 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/transform/LayerNorm/gamma) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "E0719 02:05:55.755770 140373645940608 tpu.py:376] Operation of type Placeholder (module_apply_tokens/cls/predictions/output_bias) is not supported on the TPU. Execution will fail if this op is used in the graph. \n",
            "I0719 02:05:55.849598 140373645940608 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0719 02:05:56.079304 140373645940608 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3154: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "I0719 02:05:57.535214 140373645940608 estimator.py:1147] Done calling model_fn.\n",
            "I0719 02:05:57.562954 140373645940608 evaluation.py:255] Starting evaluation at 2019-07-19T02:05:57Z\n",
            "I0719 02:05:57.564284 140373645940608 tpu_estimator.py:499] TPU job name worker\n",
            "W0719 02:05:57.706979 140373645940608 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py:1354: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0719 02:05:58.137306 140373645940608 monitored_session.py:240] Graph was finalized.\n",
            "W0719 02:05:58.141379 140373645940608 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0719 02:05:58.225130 140373645940608 saver.py:1280] Restoring parameters from gs://bert_example/bert-tfhub/models/combined/document/testii/smallBERT-parDoc_256/model.ckpt-690\n",
            "I0719 02:06:24.063473 140373645940608 session_manager.py:500] Running local_init_op.\n",
            "I0719 02:06:24.335582 140373645940608 session_manager.py:502] Done running local_init_op.\n",
            "W0719 02:06:24.670474 140373645940608 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:792: load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0719 02:06:24.916676 140373645940608 tpu_estimator.py:557] Init TPU system\n",
            "I0719 02:06:28.561284 140373645940608 tpu_estimator.py:566] Initialized TPU in 3 seconds\n",
            "I0719 02:06:28.567405 140372409247488 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0719 02:06:28.571316 140372400854784 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0719 02:06:28.902909 140373645940608 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0719 02:06:29.168848 140373645940608 tpu_estimator.py:590] Enqueue next (35) batch(es) of data to infeed.\n",
            "I0719 02:06:29.171041 140373645940608 tpu_estimator.py:594] Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0719 02:06:34.405256 140372400854784 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0719 02:06:34.713804 140373645940608 evaluation.py:167] Evaluation [35/35]\n",
            "I0719 02:06:34.715955 140373645940608 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0719 02:06:34.718425 140373645940608 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0719 02:06:34.724694 140372409247488 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0719 02:06:34.725991 140372409247488 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0719 02:06:34.727960 140373645940608 error_handling.py:96] infeed marked as finished\n",
            "I0719 02:06:34.729870 140373645940608 tpu_estimator.py:602] Stop output thread controller\n",
            "I0719 02:06:34.731203 140373645940608 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0719 02:06:34.733557 140372400854784 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0719 02:06:34.737798 140372400854784 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0719 02:06:34.743536 140373645940608 error_handling.py:96] outfeed marked as finished\n",
            "I0719 02:06:34.745640 140373645940608 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0719 02:06:35.430501 140373645940608 evaluation.py:275] Finished evaluation at 2019-07-19-02:06:35\n",
            "I0719 02:06:35.432275 140373645940608 estimator.py:2039] Saving dict for global step 690: eval_accuracy = 0.5642857, eval_loss = 1.9365361, global_step = 690, loss = 2.3901148\n",
            "I0719 02:06:38.451384 140373645940608 estimator.py:2099] Saving 'checkpoint_path' summary for global step 690: gs://bert_example/bert-tfhub/models/combined/document/testii/smallBERT-parDoc_256/model.ckpt-690\n",
            "I0719 02:06:38.767031 140373645940608 error_handling.py:96] evaluation_loop marked as finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Finished evaluation at 2019-07-19 02:06:38.769443 *****\n",
            "***** Eval results *****\n",
            "  eval_accuracy = 0.5642857\n",
            "  eval_loss = 1.9365361\n",
            "  global_step = 690\n",
            "  loss = 2.3901148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCrUx2Esa7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model_predict(estimator,prediction_examples,input_features,checkpoint_path=None):\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascfbXXbzEpw",
        "colab_type": "code",
        "outputId": "b92576da-3fc2-4b4b-e3e8-0e2a06c8ae9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "test_features = run_classifier.convert_examples_to_features(test_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features_fixed = run_classifier.convert_examples_to_features(test_InputExamples_fixed_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=OUTPUT_DIR+'/'+'Best_Model_51Dev'+'/model.ckpt-184')\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,test_features,checkpoint_path=OUTPUT_DIR+'/'+'Best_Model_51Dev'+'/model.ckpt-184')\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,test_features_fixed,checkpoint_path=OUTPUT_DIR+'/'+'Best_Model_51Dev'+'/model.ckpt-184')\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_fixed_doc['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 9 12  2]\n",
            " [ 6 53 38]\n",
            " [ 4 65 95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.47      0.39      0.43        23\n",
            "     Neutral       0.41      0.55      0.47        97\n",
            "    Positive       0.70      0.58      0.64       164\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       284\n",
            "   macro avg       0.53      0.51      0.51       284\n",
            "weighted avg       0.58      0.55      0.56       284\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kgZn8x0w-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "6b4c7a13-7042-4900-baa1-91fbdc4fa29e"
      },
      "source": [
        "chkpth = 'gs://bert_example/bert-tfhub/models/combined/weighted_loss/smallBERT-parDoc_256/kept_models/model.ckpt-5990'\n",
        "predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=chkpth)\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,test_features,checkpoint_path=chkpth)\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,test_features_fixed,checkpoint_path=chkpth)\n",
        "labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  2  11  10]\n",
            " [  2  37  58]\n",
            " [  2  35 127]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.45      0.38      0.41        97\n",
            "    Positive       0.65      0.77      0.71       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.48      0.41      0.42       284\n",
            "weighted avg       0.56      0.58      0.56       284\n",
            "\n",
            "[[  3  11  14]\n",
            " [  3  36  48]\n",
            " [  0  46 124]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.11      0.18        28\n",
            "     Neutral       0.39      0.41      0.40        87\n",
            "    Positive       0.67      0.73      0.70       170\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       285\n",
            "   macro avg       0.52      0.42      0.42       285\n",
            "weighted avg       0.56      0.57      0.55       285\n",
            "\n",
            "[[  3  23  24]\n",
            " [  4  46  66]\n",
            " [  1  42 130]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.38      0.06      0.10        50\n",
            "     Neutral       0.41      0.40      0.41       116\n",
            "    Positive       0.59      0.75      0.66       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.46      0.40      0.39       339\n",
            "weighted avg       0.50      0.53      0.49       339\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fiDTLoKMfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(labels_val)\n",
        "# print(list(dev_doc['sentiment']))\n",
        "# print(list(dev_doc['doc_id']))\n",
        "# print(list(dev_doc['uniq_ent']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7J13niRqOYj",
        "colab_type": "text"
      },
      "source": [
        "#Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdkAzUuvqbGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Error Analysis\n",
        "### length of paragraph vs accuracy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "ind = 0\n",
        "negative_info = {}\n",
        "positive_info = {}\n",
        "neutral_info  = {}\n",
        "try:\n",
        "  ### if we are predicting in the current file and just read the prediction\n",
        "  doc_ids = list(dev_doc['doc_id'])\n",
        "except:\n",
        "  #### if we are predicting in another file and loadin that from content\n",
        "  dev_doc = pd.read_csv('mask_senti_prediction.csv')\n",
        "  doc_ids= list(dev_doc['doc_id'])\n",
        "  labels_val = list(dev_doc['predicted'])\n",
        "\n",
        "for item in list(dev_doc['sentiment']):\n",
        "  if item == 'Negative':\n",
        "    negative_info[doc_ids[ind]] = labels_val[ind]\n",
        "  elif item == 'Positive':\n",
        "    positive_info[doc_ids[ind]] = labels_val[ind]\n",
        "  elif item == 'Neutral':\n",
        "    neutral_info[doc_ids[ind]] = labels_val[ind]\n",
        "  ind += 1\n",
        "  \n",
        "### input all document in specific category based on the document id and output a map which has the document length and the predicted label\n",
        "def doc_length_label(splitter='\\n',documents =negative_info ):\n",
        "  doc_lengths_label_map = {}\n",
        "  for doc_id in documents.keys():\n",
        "    length = len(list(dev_doc[dev_doc['doc_id']==doc_id]['sentence'])[0].split(splitter))\n",
        "    if length in doc_lengths_label_map:\n",
        "      doc_lengths_label_map[length].append(documents[doc_id])\n",
        "    else:\n",
        "      doc_lengths_label_map[length] = [documents[doc_id]]\n",
        "  return doc_lengths_label_map\n",
        "\n",
        "### input all document in specific category based on the document id and output a map which has the number of unique entities and the predicted label\n",
        "def unique_entity_label(splitter='\\n',documents =negative_info ):\n",
        "  unique_entity_label_map = {}\n",
        "  for doc_id in documents.keys():\n",
        "    num = list(dev_doc[dev_doc['doc_id']==doc_id]['uniq_ent'])[0]\n",
        "    if num in unique_entity_label_map:\n",
        "      unique_entity_label_map[num].append(documents[doc_id])\n",
        "    else:\n",
        "      unique_entity_label_map[num] = [documents[doc_id]]\n",
        "  return unique_entity_label_map\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "### plot the number of correctly classified document in each category based on the number of paragraphs/words in the document\n",
        "### input: splitter: to choose whether to plot based on the number of paragraphs or words\n",
        "########## bar range: to choose the length which should be in the same bar\n",
        "########## label: \"paragraph\" or \"word\" based on the splitter\n",
        "### output: the plots which show what percentage of documents in each range of paragraphs/words length are classified correctly \n",
        "\n",
        "def plot_correctly_classified(splitter='\\n',bar_range= [1,5,7,10,17],label='Number of Paragraphs in a Document',method_plot=doc_length_label,plot='bar'): \n",
        "  doc_lengths_label_negs = method_plot(splitter=splitter,documents =negative_info )\n",
        "  doc_lengths_label_pos = method_plot(splitter=splitter,documents =positive_info )\n",
        "  doc_lengths_label_neut = method_plot(splitter=splitter,documents =neutral_info )\n",
        "#   print('injaaaa>>>',doc_lengths_label_neut)\n",
        "\n",
        "\n",
        "  \n",
        "  maxes = bar_range\n",
        "  print('Positives')\n",
        "  categories_po = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_pos.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_po[j-1].extend(doc_lengths_label_pos[i])\n",
        "  for i in categories_po:\n",
        "    print(len(i))\n",
        "\n",
        "  print('Neutrals')\n",
        "  categories_neu = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_neut.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_neu[j-1].extend(doc_lengths_label_neut[i])\n",
        "  for i in categories_neu:\n",
        "    print(len(i))\n",
        "  print('oooonjaaa>>>',categories_neu)\n",
        "\n",
        "  print('Negatives')\n",
        "  categories_neg = [[] for _ in range(len(maxes)-1)]\n",
        "  for i in doc_lengths_label_negs.keys():\n",
        "    for j in range(1,len(maxes)):\n",
        "      if i < maxes[j] and i >= maxes[j-1]:\n",
        "        categories_neg[j-1].extend(doc_lengths_label_negs[i])\n",
        "  for i in categories_neg:\n",
        "    print(len(i))\n",
        "    \n",
        "  print('Total')  \n",
        "  for i,j,k in zip(categories_neg,categories_neu,categories_po):\n",
        "    print(len(i+j+k))\n",
        "\n",
        "  bar_pos = []\n",
        "  bar_neu = []\n",
        "  bar_neg = []\n",
        "  bar_mac_f1= []\n",
        "  for i in range(len(maxes)-1):\n",
        "    bar_pos.append(float(categories_po[i].count('Positive'))/len(categories_po[i]))\n",
        "    bar_neu.append(float(categories_neu[i].count('Neutral'))/len(categories_neu[i]))\n",
        "#     bar_neg.append(float(categories_neg[i].count('Negative'))/len(categories_neg[i]))\n",
        "    true_label = [\"Positive\" for _ in categories_po[i]]\n",
        "    true_label.extend([\"Neutral\" for _ in categories_neu[i]])\n",
        "    true_label.extend([\"Negative\" for _ in categories_neg[i]])\n",
        "    predicted_label = []\n",
        "    predicted_label.extend(categories_po[i])\n",
        "    predicted_label.extend(categories_neu[i]) \n",
        "    predicted_label.extend(categories_neg[i])\n",
        "    bar_mac_f1.append(metrics.f1_score(y_true=true_label,y_pred=predicted_label,average='macro'))\n",
        "#     print(len(true_label))\n",
        "#     print(len(predicted_label))\n",
        "  print('pos:' , bar_pos)\n",
        "  print('neu:' , bar_neu)\n",
        "#   print('neg:' , bar_neg)\n",
        "  print('macro f1', bar_mac_f1)\n",
        "  if plot == 'bar':\n",
        "\n",
        "    bar_width = 0.5  \n",
        "#     plt.bar([i for i in range(1,len(maxes))], bar_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "#     plt.bar([i+bar_width for i in range(1,len(maxes))], bar_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "#     plt.bar([i+2*bar_width for i in range(1,len(maxes))], bar_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "    plt.bar([i for i in range(1,len(maxes))], bar_mac_f1, width = bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='macroF1')\n",
        "\n",
        "  elif plot == 'area':\n",
        "    plt.stackplot([i for i in range(1,len(maxes))],bar_pos,bar_neu,bar_neg,bar_mac_f1,labels=['positive','neutral','negative','macroF1'],colors=['green','yellow','red','blue'])\n",
        "  plt.xticks([r for r in range(1,len(maxes))], ['[%d-%d)'%(maxes[i],maxes[i+1]) for i in range(len(maxes)-1)])\n",
        "#   plt.ylabel('% Correctly Classified Documents')\n",
        "  plt.ylabel('Macro F1')\n",
        "\n",
        "  plt.xlabel(label)\n",
        "#   plt.legend() \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omlS-01FtYnG",
        "colab_type": "code",
        "outputId": "9f2a332b-363b-4c96-ef8f-9041bb30bc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# maxes = [-1,3,5,7,9,11,13,15,1000]\n",
        "\n",
        "\n",
        "plot_correctly_classified(splitter=' ',bar_range=[1,175,260,430,1400],label='Number of Words in a Document')\n",
        "plot_correctly_classified(bar_range= [1,5,7,10,17])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "45\n",
            "38\n",
            "40\n",
            "41\n",
            "Neutrals\n",
            "26\n",
            "29\n",
            "21\n",
            "21\n",
            "('oooonjaaa>>>', [['Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral'], ['Negative', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral'], ['Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral'], ['Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive']])\n",
            "Negatives\n",
            "1\n",
            "5\n",
            "8\n",
            "9\n",
            "Total\n",
            "72\n",
            "72\n",
            "69\n",
            "71\n",
            "('pos:', [0.5555555555555556, 0.7105263157894737, 0.875, 0.7560975609756098])\n",
            "('neu:', [0.46153846153846156, 0.41379310344827586, 0.42857142857142855, 0.19047619047619047])\n",
            "('macro f1', [0.33634373289545705, 0.5064814814814814, 0.5394113549285963, 0.291304347826087])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n93tRjz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "41\n",
            "43\n",
            "40\n",
            "40\n",
            "Neutrals\n",
            "30\n",
            "34\n",
            "16\n",
            "17\n",
            "('oooonjaaa>>>', [['Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive'], ['Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral'], ['Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive'], ['Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive']])\n",
            "Negatives\n",
            "4\n",
            "7\n",
            "4\n",
            "8\n",
            "Total\n",
            "75\n",
            "84\n",
            "60\n",
            "65\n",
            "('pos:', [0.6341463414634146, 0.7209302325581395, 0.75, 0.775])\n",
            "('neu:', [0.4666666666666667, 0.4411764705882353, 0.25, 0.23529411764705882])\n",
            "('macro f1', [0.3594771241830066, 0.49206349206349204, 0.4576305220883534, 0.3828282828282828])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnbZTIgsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84LNvYuACUCL",
        "colab_type": "code",
        "outputId": "1fa5b607-7a08-446d-e5b6-939a425ae124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#### Plot percentage of paragraph in a document which has the same label as document vs macro f1 score\n",
        "\n",
        "## input: set of documents with specific true label\n",
        "## output: percentage of paragraphs in the input documents which has the same label as document\n",
        "def par_in_doc(dev_par='',documents = negative_info,label =\"Negative\"):\n",
        "  percent_neg_map = {}\n",
        "  for doc_id in  documents.keys():\n",
        "    neg_pars = list(dev_par[dev_par['doc_id']==doc_id]['sentiment'])\n",
        "    if len(neg_pars)==0:\n",
        "      percent_neg =1.\n",
        "    else:\n",
        "      percent_neg = float(neg_pars.count(label))/len(neg_pars)\n",
        "\n",
        "    if percent_neg in percent_neg_map:\n",
        "      percent_neg_map[percent_neg].append(documents[doc_id])\n",
        "    else:\n",
        "      percent_neg_map[percent_neg] = [documents[doc_id]]\n",
        "#   print(len(percent_neg_map))\n",
        "  return percent_neg_map\n",
        "\n",
        "### based on the different ranges given, compute the f1 score of that specific category in each range ( percentage of paragraphs which has the same label as document)\n",
        "\n",
        "def true_pred_cat(paragraphs = '', ranges = [0,0.26,0.51,0.67 ,0.80,1.],true_tag='Positive'):\n",
        "  categories = [[] for _ in ranges[1:]]\n",
        "#   print(len(paragraphs))\n",
        "#   print(paragraphs)\n",
        "#   for value,label in paragraphs:\n",
        "  for key in paragraphs.keys():\n",
        "    for i in range(1,len(ranges)):\n",
        "      if key <= ranges[i] and key > ranges[i-1] :\n",
        "        categories[i-1].extend(paragraphs[key])\n",
        "  macro_f1 = []   \n",
        "  data_distr = []\n",
        "  for i in range(len(ranges[1:])):\n",
        "    print(categories[i])\n",
        "    macro_f1.append(float(categories[i].count(true_tag))/len(categories[i]))\n",
        "    data_distr.append(len(categories[i]))\n",
        "#   print(macro_f1)\n",
        "  return macro_f1,categories,data_distr\n",
        "  \n",
        "per_po = par_in_doc(dev_par=dev_par,documents = positive_info,label =\"Positive\")\n",
        "per_neu= par_in_doc(dev_par=dev_par,documents = neutral_info,label =\"Neutral\")\n",
        "per_neg = par_in_doc(dev_par=dev_par,documents = negative_info,label =\"Negative\")\n",
        "# print(par_po)\n",
        "# print(par_neu)\n",
        "# print(par_neg)\n",
        "\n",
        "\n",
        "\n",
        "ranges = [0.,0.25,0.5,0.74,1.]\n",
        "# ranges = [0,0.99,1.]\n",
        "print('pos: ')\n",
        "bar_pos,pos_val,data_distr_pos = true_pred_cat(paragraphs = per_po,ranges= ranges,true_tag='Positive')\n",
        "print('neu: ')\n",
        "bar_neu,neu_val,data_distr_neu  = true_pred_cat(paragraphs = per_neu,ranges= ranges,true_tag='Neutral')\n",
        "print('neg: ')\n",
        "bar_neg,neg_val,data_distr_neg = true_pred_cat(paragraphs = per_neg,ranges= ranges,true_tag='Negative')\n",
        "print('total:')\n",
        "for i in range(len(pos_val)):\n",
        "  print(len(pos_val[i])+len(neu_val[i])+len(neg_val[i]))\n",
        "\n",
        "bar_mac_f1 = [float(i+j+k)/3 for i,j,k in zip(bar_pos,bar_neu,bar_neg)]\n",
        "print(bar_pos)\n",
        "print(bar_neu)\n",
        "print(bar_neg)\n",
        "print(bar_mac_f1)\n",
        "data_dist_total = [(i+j+k) for i,j,k in zip(data_distr_pos,data_distr_neu,data_distr_neg)]\n",
        "bar_width = 0.2 \n",
        "# plt.bar([i for i in range(1,len(ranges))], bar_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "# plt.bar([i+bar_width for i in range(1,len(ranges))], bar_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "# plt.bar([i+2*bar_width for i in range(1,len(ranges))], bar_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "plt.bar([i for i in range(1,len(ranges))], bar_mac_f1, width = 3*bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='macroF1')\n",
        "\n",
        "\n",
        "plt.xticks([r for r in range(1,len(ranges))], ['(%s-%s]'%(str(ranges[i]),str(ranges[i+1])) for i in range(len(ranges)-1)])#ranges[1:])\n",
        "plt.ylabel('Macro F1')\n",
        "plt.xlabel('Percentage of paragraphs w/ same label as document')\n",
        "# plt.legend() \n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ### showing the dataset distribution\n",
        "# plt.bar([i for i in range(1,len(ranges))], data_distr_pos, width = bar_width, color = 'green', edgecolor = 'black',  capsize=4, label='positive')\n",
        "# plt.bar([i+bar_width for i in range(1,len(ranges))], data_distr_neu, width = bar_width, color = 'yellow', edgecolor = 'black', capsize=4, label='neutral')\n",
        "# plt.bar([i+2*bar_width for i in range(1,len(ranges))], data_distr_neg, width = bar_width, color = 'red', edgecolor = 'black', capsize=4, label='negative')\n",
        "# plt.bar([i+3*bar_width for i in range(1,len(ranges))], data_dist_total, width = bar_width, color = 'blue', edgecolor = 'black', capsize=4, label='Total')\n",
        "\n",
        "\n",
        "# plt.xticks([r for r in range(1,len(ranges))], ['(%s-%s]'%(str(ranges[i]),str(ranges[i+1])) for i in range(len(ranges)-1)])#ranges[1:])\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.xlabel('Percentage of paragraphs w/ same label as document')\n",
        "# plt.legend() \n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: \n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral']\n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive']\n",
            "['Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Negative']\n",
            "['Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral']\n",
            "neu: \n",
            "['Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral']\n",
            "['Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive']\n",
            "['Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive']\n",
            "['Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral']\n",
            "neg: \n",
            "['Neutral', 'Positive']\n",
            "['Neutral', 'Negative', 'Neutral', 'Neutral', 'Negative', 'Neutral']\n",
            "['Positive', 'Negative']\n",
            "['Negative', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Negative', 'Neutral', 'Negative', 'Negative']\n",
            "total:\n",
            "13\n",
            "57\n",
            "51\n",
            "161\n",
            "[0.16666666666666666, 0.5454545454545454, 0.48484848484848486, 0.6521739130434783]\n",
            "[0.6, 0.3333333333333333, 0.6875, 0.5614035087719298]\n",
            "[0.0, 0.3333333333333333, 0.5, 0.5]\n",
            "[0.25555555555555554, 0.404040404040404, 0.5574494949494949, 0.5711924739384694]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAHIRJREFUeJzt3XucHlWd5/HPl4T7/dIQJMEEJspE\nRZAm6I66QUGDM5PggJiMjkRniThGQJZRXF0m4AtXyDq4ahQichHl7rBEiQQGERhmkHS4Jxhpwy1R\nQ0QQYcIl8Js/zulK0Ty37nT10935vl+v59V1OVV16vR56vdUnapTigjMzMwANmt3BszMbOhwUDAz\ns4KDgpmZFRwUzMys4KBgZmYFBwUzMytUGhQkTZW0QlK3pFPrpDlG0nJJyyRdWmV+zMysMVX1nIKk\nUcCvgMOBVcASYGZELC+lmQhcCbwnIp6StHtEPFFJhszMrKkqzxQmA90RsTIiXgQuB6b3SnMcMD8i\nngJwQDAza6/RFa57L+Dx0vgq4JBead4AIOl2YBQwNyKu770iSbOB2QDbbrvtQfvtt18lGTYzG6mW\nLl36+4joaJauyqDQitHARGAKMBa4VdJbIuLpcqKIWAAsAOjs7Iyurq7BzqeZ2bAm6dFW0lV5+Wg1\nMK40PjZPK1sFLIyIlyLiYVIbxMQK82RmZg1UGRSWABMlTZC0BTADWNgrzf8nnSUgaTfS5aSVFebJ\nzMwaqCwoRMR6YA6wGHgQuDIilkk6Q9K0nGwx8KSk5cDNwD9GxJNV5cnMzBqr7JbUqrhNwcys7yQt\njYjOZun8RLOZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMRrQxY8YjaUR8xowZX3l5tbubCzOz\nSq1Z8ygwvG69r2fNGlW+DZ8pmJlZwUHBzMwKDgpmZlZwUDAbQtwoau3mhmazIcSNotZuPlMwM7OC\ng4KZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMzAoOCmZmVnBQMDOzgoOCmZkVHBTMzKzgoGBm\nZgUHBTMzKzgomJlZwUHBzMwKlQYFSVMlrZDULenUGvNnSVor6Z78+R9V5sfMzBqr7CU7kkYB84HD\ngVXAEkkLI2J5r6RXRMScqvJhZmatq/JMYTLQHRErI+JF4HJgeoXbMzOzjVRlUNgLeLw0vipP6+0o\nSfdJulrSuArzY2ZmTbS7ofnHwPiI2B+4Ebi4ViJJsyV1Sepau3btoGbQzGxTUmVQWA2Uf/mPzdMK\nEfFkRLyQR88HDqq1oohYEBGdEdHZ0dFRSWbNzKzaoLAEmChpgqQtgBnAwnICSXuWRqcBD1aYHzMz\na6Kyu48iYr2kOcBiYBRwQUQsk3QG0BURC4ETJE0D1gN/AGZVlR8zM2tOEdHuPPRJZ2dndHV1tTsb\nZpWQBAyv72R9YigcX1ymeUlpaUR0NkvX7oZmMzMbQhwUzMys4KBgZmYFBwUzMys4KJiZWcFBwczM\nCg4KZmZWcFAwM7OCg4KZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMzAoOCmZmVnBQMDOzgoOC\nmZkVHBTMzKzgoGBmZgUHBTMzKzgomJlZwUHBzMwKDgpmZlZwUDAzs4KDgpmZFRwUzMys4KBgZmYF\nBwUzMytUGhQkTZW0QlK3pFMbpDtKUkjqrDI/ZmbWWGVBQdIoYD5wBDAJmClpUo102wMnAr+oKi9W\nnTFjxiNpRHzGjBnf7uI0a7sqzxQmA90RsTIiXgQuB6bXSPdl4Czg+QrzYhVZs+ZRIEbEJ+2L2aat\nyqCwF/B4aXxVnlaQ9DZgXERc12hFkmZL6pLUtXbt2oHPqZmZAW1saJa0GfDPwP9sljYiFkREZ0R0\ndnR0VJ85M7NNVJVBYTUwrjQ+Nk/rsT3wZuDnkh4B3g4sdGOzmVn7VBkUlgATJU2QtAUwA1jYMzMi\n/hgRu0XE+IgYD9wBTIuIrgrzZGZmDVQWFCJiPTAHWAw8CFwZEcsknSFpWlXbNTOz/htd5cojYhGw\nqNe00+qknVJlXszMrDk/0WxmZgUHBTMzKzgomJlZwUHBzMwKDgpmZlZwUDAzs4KDgpmZFRwUzMys\n4KBgZmYFBwUzMyv0KyhIWjDQGTEzs/ar2/eRpF3qzQI+UE12zMysnRp1iLcWeJQUBHpEHt+9ykyZ\nmVl7NAoKK4H3RsRjvWdIerxGejMzG+YatSl8Hdi5zryzK8iLmZm1Wd0zhYiY32DeN6vJjpmZtVPd\nMwVJXykNHz442TEzs3ZqdPloamn4rKozYmZm7eeH18zMrNDo7qPdJZ1MvgU1Dxci4p8rzZmZmQ26\nRkHhu8D2NYbNzGyEanT30emDmREzM2s/tymYmVnBQcHMzAoOCmZmVmgaFCTtKOkcSV358zVJOw5G\n5szMbHC1cqZwAfAMcEz+PANcWGWmzMysPVoJCvtGxD9FxMr8OR3Yp5WVS5oqaYWkbkmn1ph/vKT7\nJd0j6d8kTerrDpiZ2cBpJSisk/TOnhFJfwGsa7aQpFHAfOAIYBIws8ZB/9KIeEtEHEDqedUPxJmZ\ntVGjh9d6HA98v9SO8BRwbAvLTQa6I2IlgKTLgenA8p4EEfFMKf22pJf4mJlZmzQMCpI2A94YEW+V\ntAO85kDeyF5A+WU8q4BDamzj08DJwBbAe+rkYzYwG2DvvfducfNmZtZXDS8fRcQrwOfy8DN9CAgt\ni4j5EbEv8HngS3XSLIiIzojo7OjoGOgsmJlZ1kqbwr9KOkXSOEm79HxaWG41MK40PjZPq+dy4MgW\n1mtmZhVppU3hw/nvp0vTguZ3IC0BJkqaQAoGM4C/LSeQNDEiHsqjfwk8hJmZtU3ToBARE/qz4ohY\nL2kOsBgYBVwQEcsknQF0RcRCYI6kw4CXaL0B28zMKtI0KOSG4B9GxNN5fGdgZkR8u9myEbEIWNRr\n2mml4RP7nGMzM6tMK20Kx/UEBICIeAo4rrosmZlZu7QSFEZJUs9Ifihti+qyZGZm7dJKQ/P1wBWS\nzsvjn8zTzMxshGklKHyeFAg+lcdvBM6vLEdmZtY2rdx99ArwnfwxM7MRrJW7jyYC/4fUqd1WPdMj\noqWeUs3MbPhopaH5QtJZwnrgUOD7wA+qzJSZmbVHK0Fh64i4CVBEPBoRc0lPH5uZ2QjTSkPzC7m3\n1IfyE8qrge2qzZaZmbVDK2cKJwLbACcABwF/h7ujMDMbkVq5+2hJHnwW+Hi12anWmDHjWbPm0XZn\nY0Dsscfr+d3vHml3NsxshKkbFCQtbLRgREwb+OxUKwWEkfFytzVr1DyRmVkfNTpTeAfpzWmXAb8A\nfBQyMxvhGgWFMcDhwEzSexCuAy6LiGWDkTEzMxt8dRuaI+LliLg+Io4F3g50Az/PdyCZmdkI1LCh\nWdKWpGcSZgLjgW8A11SfLTMza4dGDc3fB95MeknO6RHxwKDlyszM2qLRmcJHgedIzymcUH6lAhAR\nsUPFeTMzs0FWNyhERCsPtpmZ2QjiA7+ZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMzAoOCmZm\nVnBQMDOzQqVBQdJUSSskdUs6tcb8kyUtl3SfpJskvb7K/JiZWWOVBQVJo4D5wBHAJGCmpEm9kt0N\ndEbE/sDVwNlV5cfMzJqr8kxhMtAdESsj4kXgcmB6OUFE3BwR/5lH7wDGVpgfMzNrosqgsBfpzW09\nVuVp9fw98NNaMyTNltQlqWvt2rUDmEUzMysbEg3Nkj4KdALzas2PiAUR0RkRnR0dHYObOTOzTUjD\nl+xspNXAuNL42DztVSQdBnwR+O8R8UKF+TEzsyaqPFNYAkyUNEHSFsAMYGE5gaQDgfOAaRHxRIV5\nMTOzFlQWFCJiPTAHWAw8CFwZEcsknSFpWk42D9gOuErSPZIW1lmdmZkNgiovHxERi0iv8yxPO600\nfFiV2zczs74ZEg3NZmY2NDgomJlZwUHBzMwKDgpmZlZwUDAzs4KDgpmZFRwUzMys4KBgZmYFBwUz\nMys4KJiZWcFBwczMCg4KZmZWcFAwM7OCg4KZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMzAoO\nCmZmVnBQMDOzgoOCmZkVHBTMzKzgoGBmZgUHBTMzKzgomJlZwUHBzMwKlQYFSVMlrZDULenUGvPf\nLekuSeslHV1lXszMrLnKgoKkUcB84AhgEjBT0qReyR4DZgGXVpUPMzNr3egK1z0Z6I6IlQCSLgem\nA8t7EkTEI3neKxXmw8zMWlTl5aO9gMdL46vyNDMzG6KGRUOzpNmSuiR1rV27tt3ZMTMbsaoMCquB\ncaXxsXlan0XEgojojIjOjo6OAcmcmZm9VpVBYQkwUdIESVsAM4CFFW7PzMw2UmVBISLWA3OAxcCD\nwJURsUzSGZKmAUg6WNIq4EPAeZKWVZUfMzNrrsq7j4iIRcCiXtNOKw0vIV1WMjOzIWBYNDSbmdng\ncFAwM7OCg4KZmRUcFMzMrOCgYGZmBQcFMzMrOCiYmVnBQcHMzAoOCmZmVnBQMDOzgoOCmZkVHBTM\nzKzgoGBmZgUHBTMzKzgomJlZwUHBzMwKDgpmZlZwUDAzs4KDgpmZFRwUzMys4KBgZmYFBwUzMys4\nKJiZWcFBwczMCg4KZmZWcFAwM7OCg4KZmRUcFMzMrFBpUJA0VdIKSd2STq0xf0tJV+T5v5A0vsr8\nmJlZY5UFBUmjgPnAEcAkYKakSb2S/T3wVET8GXAOcFZV+TEzs+aqPFOYDHRHxMqIeBG4HJjeK810\n4OI8fDXwXkmqME9mZtbA6ArXvRfweGl8FXBIvTQRsV7SH4Fdgd+XE0maDczOo89KWtH/bA1KzNmN\nXvtQhaETPyvPx6CUJwyVMnUdHXiuo8DrW0lUZVAYMBGxAFjQ7ny0SlJXRHS2Ox8jhctz4LlMB9ZI\nKs8qLx+tBsaVxsfmaTXTSBoN7Ag8WWGezMysgSqDwhJgoqQJkrYAZgALe6VZCBybh48GfhYRUWGe\nzMysgcouH+U2gjnAYmAUcEFELJN0BtAVEQuB7wGXSOoG/kAKHCPBsLnUNUy4PAeey3RgjZjylH+Y\nm5lZDz/RbGZmBQcFMzMrOCiYmVlhkw0KkraWdIukUZKOlfRQ/hxbJ/0ukm7MaW6UtHOddBNyP07d\nuV+nLWqkOVzSUkn357/vKc37ee4v6p782T1P/6ykxyR9a6DKYCD1ozznSfqlpPskXSNppzx9vKR1\npf0/t8E2v5DLeYWk99dJc5Gkh0vrOyBP/3Be9icDsf9V6EeZzpW0urSvH6iTrmldlnRoaT33SHpe\n0pG90nxD0rOl8ZFWR68o7f8jku7pNX9vSc9KOqXO8vtJ+g9JL9RLk9PVPGa0rTwjYpP8AJ8GTgR2\nAVbmvzvn4Z1rpD8bODUPnwqcVWe9VwIz8vC5wKdqpDkQeF0efjOwujTv50BnnXXPAr7V7rIboPJ8\nHzA6D5/VU57AeOCBFrY3CbgX2BKYAPwaGFUj3UXA0XXWMQX4SbvLbgDLdC5wSgvrbakul9LvQro7\ncJvStE7gEuDZkVpHey37NeC0XtOuBq6qV+bA7sDBwJmN/i+NjhntKM9N9kwB+AhwLfB+4MaI+ENE\nPAXcCEytkb7cT9PFwJG9E0gS8B5SZambLiLujojf5NFlwNaSttyIfRkK+lSeEXFDRKzPo3eQHm7s\ni+nA5RHxQkQ8DHST+tsaSfpaR1vVtC73cjTw04j4Tyg6u5wHfG4j8tAO/SrP/L0+BrisNO1I4GHS\n97emiHgiIpYALzVZd9NjxmDaJINCPj3bJyIeoXYfTXvVWGyPiPhtHv4dsEeNNLsCT5cOdvXWVXYU\ncFdEvFCadmE+Zf3fudIMaf0sz7JPAD8tjU+QdHc+1X9XnWX6sp0z82Wqc4ZL8N2IMp2T9/WCWpeF\nslbqctkMSgdEYA6wsLSOIW8j6+i7gDUR8VBe13bA54HTByBr/TlmVGqTDAqkzque7u/Ckc7rNvoB\nD0lvIl06+WRp8kci4i2kivgu4O82djuDoN/lKemLwHrgh3nSb4G9I+JA4GTgUkk7bETevgDsRzqN\n34X0ZR4O+lOm3wH2BQ4glePXmi3QrC5L2hN4C+khVCS9DvgQ8M0+5q3dNuY7P5NXB8W5wDkR8Wzt\n5MPbphoU1gFb5eFW+mgCWJO/ID1flCfy8OL8q/58Ur9NOyn149RoXUgaC1wDfCwift0zPSJW579/\nAi5leFwS6U95ImkW8FekQBgA+XLQk3l4Kamt4A2SPlhq9OtsdTsR8dtIXgAuZHiUJ/SjTCNiTUS8\nHBGvAN8l76uknjPPRTlpzbpcxzHANRHRcwnkQODPgG5JjwDbKPVIMNT1t46OBv4GuKI0+RDg7Lz/\nJwH/S9IcSZ8u1dHXtZivlo8Zg2YwGzCG0od0+rgV6dfjw6QGp53z8C410s/j1Y1zZ9dZ71W8utHo\nH2qk2YnUSPo3vaaPBnbLw5uTrjMeX5o/i6HbiNfX8pwKLAc6ek3vIDcYA/uQviC1ln8Tr25oXknt\nhuY9818BXwe+Wpo3haHd0NzXMt2zNPxZUptLrfW2VJfz/DuAQxvMH04NzX0qz1I9vaXBOufSpHG/\nWZpGx4x2lGfb/1FtrCDfAw7Lw58gNVR2Ax8vpTmffCcQ6drfTcBDwL82qET7AHfmdV0FbJmnTwPO\nyMNfAp4D7il9dge2BZYC95EasP5f+UA3xL9wfS3P7vwl7dn/c/P0o/K+3wPcBfx1g21+kXQmsQI4\nojR9ERvu7voZcD/wAPADYLtSuikM7aDQ1zK9JO/rfaTOJvess96adZl0R9H5pXTjSUF5swZ5HE5B\noU/lmccvovTDrMY651L/7qMxpDaCZ0iXrlYBO9SoozWPGe0qz0227yNJbwM+GxHD4Zo9UFxu6YyI\nOe3OS2/DtDynkL7Qf9XuvNQyTMt0Fq6jA6Yd5bmptikQEXcBN+fb64Y8SZ8lNZo+0+681DIMy/PD\nwLeBp9qdl3qGYZm6jg6gdpXnJnumYGZmr7XJnimYmdlrOSiYmVnBQaGfJL2c70d+QNJVkrZpUz5O\nate28/bnSVomaV678jCQyh28VbT+cyX9RZXbaJWkKWrSIaCkWX3tkC13HrfbxuWufesfKJJ2kvQP\n7c5HXzko9N+6iDggIt4MvAgc3+qCA9zQdRLQtqAAzAb2j4h/HKgVKtmoull6GGioeTvp3n8b+XYC\nHBQ2UbeRnvJE0kcl3ZnPIs7rCQC5i92vSboXeIekgyX9u6R7c/rtlbr0nSdpSe6/5pN52SlKXWpf\nrdTd9A/zgfME4HWkOypuzmm/I6kr/3ov+maR9IG87FKlLo9/kqdvm/vJuVOpv6HpvXcub2tePiu6\nP9+5g6SFwHbA0p5ppWXmSrpEqevghyQdl6dvJ+kmSXfldU3P08crdYH9fdIzBeP6sS8927yd9O7v\n8ZJuy9u6S9J/K5XnrZKuy9s8txyEJJ2Z/y93SNojT/tQ3v97Jd1ao4zmS5qWh6+RdEEe/oSkM/Pw\nnwO/ioiXey37mnU3yfstkq6VtFLSVyV9JP//7pe0b07XIelHuS4tUZOzE0mT8//q7lwv31iaPS7X\nv4ck/VNpmZp1vcE26v0/vyppuVKd/781lttV0g15ufNJDyL2zDs5l90Dkk4qTf9YXt+9ki7J0y6S\ndHQpzbMDUaa53l2Qy2il0vcS4KvAvrl8hs+ZdLsfKBmuH/JDO6SnkK8FPgX8OfBjYPM879ukbiwg\n9S9zTB7egvQE7sF5fIe8ntnAl/K0LYEu0tO6U4A/kh6B3wz4D+CdOd0j5Keg83jPg0ijSN1w7096\nivNxYEKedxn5oS3gK8BH8/BOwK+AbXvt61GkniRHkTpPe4wNTwo/W6d85pKeON6a1O/M46QANpoN\nD/DsRnpgR6QHpV4B3r4R+zKX9PDf1nl8G2CrPDwR6MrDU4DnSQ8Njcr7dnTp//TXefjs0v/jfmCv\nnnKqsb8zgHl5+E7gjjx8IfD+PHwy8Ikay75m3U3y/jSwJ6mOrAZOz/NOBL6ehy9lQx3ZG3iwxnan\nlMpuBzZ0ZX4Y8KM8PIvUj9Ku+X/5AOkht0Z1/RFKdbLJ/3NX0sOHKu9/r+W+Qe62GvjL/D/aDTgo\nl922pB8ny0jdcLyJVI9367Xdiyh1o86G7/BGlSmp3v17XnY3UtcVm9NiN/BD7TNUT7GHg6214aUb\nt5GelpxNqqhLlDo33ZoN/cq8DPwoD78R+G2kbnWJiGcAJL0P2L/0a2ZH0gHhReDOiFiV091DqnD/\nViNfx0iaTTr47kl678BmwMpIXUxDOpDOzsPvA6Zpw0tAtiJX+NI63wlcFukX7hpJt5A6mFvYpIyu\njYh1wDqlM5nJwHXAVyS9mxQE9mJDL52PRkT50kpf9wVS753r8vDmwLeUXqzzMvCGUro7I2IlgKTL\n8j5eTSrrnuvsS4HD8/DtwEWSrgT+pca+3gacJGkSqfuOnZX6FXoH0PPL8f3Ax2ssW2vdjfK+JHIP\npZJ+DdyQp98PHJqHDwMmaUMnuztI2i7qd+K2I3CxpImkg+7mpXk3Ru6PStK/kMpqPfXrej21/p/L\nSQH6e0pnfLXaON5N6n+IiLhOUs+zJe8k9cv0XClv78r5vyoifp+X+UOTfMFGlGkevi5S/1ovSHqC\n5j3PDlkOCv23LiIOKE9Qqi0XR8QXaqR/PnpdNqhBwGciYnGv9U4Byl1rv0yN/52kCcAppDOQpyRd\nxIZOwBpt86iIWNEkXX/0fggmSH3adwAHRcRLSp2K9eTxuSJT/duXV62D1P/PGuCtpGDyfJO8AbwU\n+ecfpXKOiOMlHUL6pbpU0kE9B8o8f7XS2+OmAreS+tc5hvRr9E9KNwPsFBveo0Fp2desG/hMg7yX\n68IrpfFX2FAvNiOddZWXa+TLwM0R8UFJ40m/5Iss9s4yqd7Uq+uvUe//GRHrJU0G3kt6b8Mc0vsF\nqrCefMlc6XJh+a2I/S7THCSafj+HC7cpDKybgKO14RWau0h6fY10K4A9JR2c022v1DC6GPiUpM3z\n9DdI2rbJNv8EbJ+HdyAdFP+odC38iNL29slfdoDy9f/FwGdyQEPSgTW2cRvwYaU2jw7SL7c7m+QL\nYLqkrSTtSjpFX0L6RfpEDgiHArXKp7/70tuOpDOyV0hdkJeveU9Weg3iZnkdtc66CpL2jYhfRMRp\nwFpe3ctmjztIDf+3ksrslPwX0q/Nm/uw7kZ5b8UNpMDSs40DGqQlb6+nd85ZveYdnuvy1qQXwNxO\n63W9R83/Z/6lvWNELCIF8bfWWPZW4G9z+iNIndhBKtsjJW2TvycfzNN+Bnwo1zsk7ZLTP0I6u4HU\nF1n5bKgVfS3T8ndz2Bi20Wwoiojlkr4E3JAPNi+RXgH4aK90Lyo1zH4zf9HWkU5NzyddFrorH6TX\n0vwtTAuA6yX9JiIOlXQ38EvSdffb8/bWKd0ad72k50gH5x5fJvUeel/O88Ok7qzLriFdBrmX9Cvx\ncxHxuxaK5D7SgXA34MsR8RtJPwR+LOl+UpvJL2stGBH39mNfevs28CNJHwOu59VnEUuAb5FuELg5\n72Mj8/KlFZEOiPfWSHMb8L6I6Jb0KOlsoScoHMGGt2u1su5GeW/FCcB8SfeRvue30vgOubNJl4++\nRLrEV3Yn6dLnWOAHEdEF0Epd71Hv/0k6aF4raSvS/p9cY/HTgcskLSNdu38sr/OufMbR8wPl/Ii4\nO+ftTOAWSS8Dd5MC3Xfztu5lEMo0Ip6UdLukB0hvrhuwO/Sq5G4uNhE915NzsJkPPBQR51S4vbmk\nSyevuZtkANa9UfuiNnSEJ+ku4JDY8F4CsyHJl482HccpNVAvI10qOK/N+dkYw25fIuJtDgg2HPhM\nwczMCj5TMDOzgoOCmZkVHBTMzKzgoGBmZgUHBTMzK/wXQBVK2fLn+O4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IobAZXZvppR3",
        "colab_type": "code",
        "outputId": "373a339c-a411-4d90-ab8a-b7cce91bfcbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "### number of unique entities vs Macro F1\n",
        "plot_correctly_classified(splitter=' ',bar_range=[1,4,7,20],label='Number of Unique Entities in a Document',method_plot=unique_entity_label,plot='bar')\n",
        "# plot_correctly_classified(splitter=' ',bar_range=[0,4,7,15],label='Number of Unique Entities in a Document',method_plot=unique_entity_label,plot='area')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positives\n",
            "68\n",
            "52\n",
            "43\n",
            "Neutrals\n",
            "41\n",
            "30\n",
            "24\n",
            "('oooonjaaa>>>', [['Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive'], ['Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive'], ['Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral']])\n",
            "Negatives\n",
            "3\n",
            "8\n",
            "11\n",
            "Total\n",
            "112\n",
            "90\n",
            "78\n",
            "('pos:', [0.7352941176470589, 0.6923076923076923, 0.7209302325581395])\n",
            "('neu:', [0.4146341463414634, 0.4, 0.3333333333333333])\n",
            "('macro f1', [0.5150390980178213, 0.46012673788733177, 0.3789233954451346])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHX+P14nX3X55K1jAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7AYlKDt_EqQ",
        "colab_type": "code",
        "outputId": "05116070-2abd-4d26-baf6-607aad15167f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "source": [
        "##### Compare the classification accuracy in sentiment analysis task and masked LM task\n",
        "dev_predicted_masked = pd.read_csv(open('/content/dev_predicted.csv'))\n",
        "\n",
        "\n",
        "### given and input list of document ids in the classification task, compute the accuracy of that set in masked lm task. it has three methods to compute average in masked lm task\n",
        "### avg: which computes average among different documents, \n",
        "##  one: accuracy is one if one of the documents in masked lm is correctly classified, all: accuracy is one when all of the documents in masked lm is correctly classified\n",
        "def compute_acc(input_list = [],method='avg'):\n",
        "  \n",
        "  cc_masked_lm_acc = 0   \n",
        "  for doc_id in input_list:\n",
        "    predicted = list(dev_predicted_masked[dev_predicted_masked['doc_id']==doc_id]['predicted'])\n",
        "#     print(predicted)\n",
        "    label = list(dev_predicted_masked[dev_predicted_masked['doc_id']==doc_id]['label'])\n",
        "#     print(label)\n",
        "    acc = metrics.accuracy_score(y_pred=predicted,y_true=label)\n",
        "    if method == 'avg':\n",
        "      cc_masked_lm_acc += acc\n",
        "    elif method == 'all':\n",
        "      if acc == 1:\n",
        "        cc_masked_lm_acc +=1\n",
        "    elif method == 'one':\n",
        "      if acc > 0:\n",
        "        cc_masked_lm_acc +=1\n",
        "  return float(cc_masked_lm_acc/len(input_list))\n",
        "        \n",
        "### compare the masked lm accuracy with the sentiment analysis accuracy, given a label, return the correctly classified accuracy in masked lm and  \n",
        "### incorrectly classified ( in sentiment analysis) accuracy ( in masked lm), as we have many documents in masked lm per each document in sentiment analysis,\n",
        "### there are three methods to compute the accuracy in masked lm: \n",
        "###     avg: compute average accuracy among different documents with the same doc id in masked lm, \n",
        "###     all: if all of the documents aggree in masked lm that document is considered correct\n",
        "###     one: if one of the documents is correctly classified in masked lm then that document is considered correct\n",
        "def masked_LM_VS_classifier_acc(negative_info,label='Negative',method = 'avg'):\n",
        "  correctly_classified= []\n",
        "  incorrectly_classified = []\n",
        "  incorrect_labels = {'neg_pos':[],'pos_neg':[],'pos_neu':[],\n",
        "                     'neg_neu':[],'neu_pos':[],'neu_neg':[]}\n",
        "  \n",
        "  for item in negative_info:\n",
        "    pred = negative_info[item] \n",
        "    if negative_info[item] ==label:\n",
        "      correctly_classified.append(item)\n",
        "    else:\n",
        "      incorrectly_classified.append(item)\n",
        "      if label.startswith('Neg') and pred.startswith('Po'):\n",
        "        incorrect_labels['neg_pos'].append(item)\n",
        "      elif label.startswith('Neg') and pred.startswith('Neu'):\n",
        "        incorrect_labels['neg_neu'].append(item)\n",
        "      elif label.startswith('Pos') and pred.startswith('Neg'):\n",
        "        incorrect_labels['pos_neg'].append(item)\n",
        "\n",
        "      elif label.startswith('Pos') and pred.startswith('Neu'):\n",
        "        incorrect_labels['pos_neu'].append(item)\n",
        "\n",
        "      elif label.startswith('Neu') and pred.startswith('Pos'):\n",
        "        incorrect_labels['neu_pos'].append(item)\n",
        "\n",
        "      elif label.startswith('Neu') and pred.startswith('Neg'):\n",
        "        incorrect_labels['neu_neg'].append(item)\n",
        "\n",
        "        \n",
        "  print(incorrect_labels)\n",
        "  cc_masked_lm_acc = compute_acc(correctly_classified,method=method)\n",
        "    \n",
        "  ic_masked_lm_acc = compute_acc(incorrectly_classified,method=method)\n",
        "  incorrect_perclass_acc = {}\n",
        "  for item in incorrect_labels:\n",
        "    if len(incorrect_labels[item])==0:\n",
        "      continue\n",
        "    incorrect_perclass_acc[item] = compute_acc(incorrect_labels[item],method=method)\n",
        "\n",
        "#   print(incorrect_perclass_acc)  \n",
        "  return(cc_masked_lm_acc\n",
        "               ,ic_masked_lm_acc,incorrect_perclass_acc)\n",
        "\n",
        "\n",
        "\n",
        "cc_neg, ic_neg,incorrect_perclass_neg = masked_LM_VS_classifier_acc(negative_info,label='Negative',method='avg')\n",
        "print(cc_neg, ic_neg,incorrect_perclass_neg)\n",
        "\n",
        "cc_pos, ic_pos,incorrect_perclass_pos = masked_LM_VS_classifier_acc(positive_info,label='Positive',method='avg')\n",
        "print(cc_pos, ic_pos,incorrect_perclass_pos)\n",
        "\n",
        "cc_neu, ic_neu,incorrect_perclass_neu = masked_LM_VS_classifier_acc(neutral_info,label='Neutral',method='avg')\n",
        "print(cc_neu, ic_neu,incorrect_perclass_neu)\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(4)\n",
        "bar_width = 0.4\n",
        "\n",
        "\n",
        "# rect1 = plt.bar([i for i in range(3)], [cc_neg,incorrect_perclass_neu['neu_neg'],incorrect_perclass_pos['pos_neg']],bar_width,color = 'r',label='label_neg')\n",
        "# rect2 = plt.bar([i+bar_width for i in range(3)], [incorrect_perclass_neg['neg_neu'],cc_neu,incorrect_perclass_pos['pos_neu']],bar_width,color = 'y',label='label_neu')\n",
        "# rect3 = plt.bar([i+2*bar_width for i in range(3)], [incorrect_perclass_neg['neg_pos'],incorrect_perclass_neu['neu_pos'],cc_pos],bar_width,color = 'g',label='label_pos')\n",
        "# rect4 = plt.bar([i+3*bar_width for i in range(3)], [ic_neg,ic_neu,ic_pos],bar_width,color = 'b',label='label_other')\n",
        "\n",
        "rect1 = plt.bar([i for i in range(3)], [cc_neg,cc_neu,cc_pos],bar_width,color = 'blue',label='Correctly Classified')\n",
        "rect2 = plt.bar([i+bar_width for i in range(3)], [ic_neg,ic_neu,ic_pos],bar_width,color = 'cyan',label='Incorrectly Classified')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "###loc: right, center left, upper right, lower right, best, center, lower left, center right, upper left, upper center, lower center\n",
        "\n",
        "plt.legend(loc='right', bbox_to_anchor=(1.3, .9), ncol=1, fancybox=True, shadow=True)\n",
        "# plt.legend()\n",
        "plt.xticks( index+0.5*bar_width,('Negative','Neutral','Positive'))\n",
        "plt.ylabel('Masked Entity LM Accuracy')\n",
        "plt.xlabel('Sentiment Classification True Label')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pos_neg': [], 'neu_pos': [], 'neu_neg': [], 'pos_neu': [], 'neg_neu': [1929, 1933, 2062, 1944, 1969, 1842, 1976, 1852, 1985, 1884, 1900, 1902, 1907, 1908, 2046], 'neg_pos': [1844, 1854, 1984, 1875]}\n",
            "(0.9058560924369748, 0.8454868231629963, {'neg_neu': 0.8522865299517116, 'neg_pos': 0.819987922705314})\n",
            "{'pos_neg': [1943, 2014, 2028], 'neu_pos': [], 'neu_neg': [], 'pos_neu': [2050, 2052, 2055, 2066, 2071, 2073, 2077, 2078, 2079, 2086, 2089, 2091, 2095, 2098, 2103, 2108, 1822, 1866, 1873, 1881, 1890, 1905, 1906, 1910, 1919, 1920, 1931, 1934, 1952, 1961, 1974, 1982, 1997, 2002, 2004, 2013, 2016, 2020, 2024, 2026, 2031, 2039, 2042], 'neg_neu': [], 'neg_pos': []}\n",
            "(0.8835872680262865, 0.8293638440213555, {'pos_neu': 0.8326637482048473, 'pos_neg': 0.7820652173913043})\n",
            "{'pos_neg': [], 'neu_pos': [2048, 2075, 2080, 2084, 1831, 1838, 1839, 1840, 1845, 1848, 1850, 1855, 1857, 1861, 1862, 1864, 1869, 1871, 1874, 1889, 1892, 1893, 1895, 1896, 1897, 1898, 1911, 1912, 1921, 1949, 1950, 1955, 1957, 1963, 1964, 1966, 1967, 1968, 1972, 1973, 1975, 1977, 1983, 1987, 1988, 1990, 1991, 1992, 2000, 2006, 2021, 2022, 2025, 2027, 2029, 2030, 2037], 'neu_neg': [1836, 1965, 2012], 'pos_neu': [], 'neg_neu': [], 'neg_pos': []}\n",
            "(0.8518476781634678, 0.8782026728717903, {'neu_neg': 0.8579545454545454, 'neu_pos': 0.8792683637884875})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEKCAYAAAAlye1PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xl8FeXZ//HPlbBLUDbZMRRIQgiL\nEqnghlh91LoUpUWKAi1KrUUf90ef9metWiv6SC1Vqti6tNUCpS5AEUVlUXEhgECI7AWEgqIgsgVJ\ncv3+mDn2ELOcYA6H5Hzfr9d5MXPPzD3XzDmvXMzMPfdt7o6IiIgceSmJDkBERCRZKQmLiIgkiJKw\niIhIgigJi4iIJIiSsIiISIIoCYuIiCSIkrCIiEiCKAmLiIgkiJKwiIhIgtRJdABV1aJFC09PT090\nGCIiNcqiRYs+dfeWiY5DDlXjknB6ejp5eXmJDkNEpEYxs42JjkG+TrejRUREEkRJWEREJEGUhEVE\nRBJESVhERCRBalzDLBGRRDt48CCbN2+msLAw0aHEbPbs2T2WLl26IdFxJJkSIL+oqOiqPn36fFLW\nCkrCIiJVtHnzZtLS0khPT8fMEh1OTIqLi4tycnI+TXQcyaSkpMS2b9+evW3btj8CF5e1jm5Hi4hU\nUWFhIc2bN68xCVgSIyUlxVu2bLkLyCl3nSMYj4hIraEELLFISUlxKsi1SsIiIjXQtm3buPzyy+nc\nuTN9+vThggsuYPXq1Uds/wsWLGg4efLkYyPz48ePbz58+PCOVanjkUcead61a9fuGRkZ2d26dcu+\n8847WwFcdtll6U899VTT6ohzw4YNdc8777xvReYvuuiiThkZGdm/+tWvjr/hhhvavvjii2mx1rVq\n1ap6Xbt27V4dcUUkxTPho/U/rO6JjkBEqkN1/42p7G+DuzNo0CBGjBjBpEmTAFi6dCkff/wxGRkZ\nldZ/8OBB6tat+9V8SUkJ7k5qamrMMebl5TXKy8s7ZsiQIbti3ijKlClTmkyYMOH42bNnr05PTz+4\nf/9+mzBhQvPDqasi6enpB2fNmrUeYNOmTXWWLl16zKZNm/Krez+HS1fCIiI1zJw5c6hbty7XXHPN\nV2W9evXi9NNPx9259dZbycnJoUePHkyePBmA9957L6VPnz6ZAwcO7NK1a9ecVatW1UtPT88ZNGhQ\nekZGRvd169bVe/7555v07t07Kzs7u9v555//rV27dqUAzJs3r9GJJ56YlZmZmd2jR49un332Wepv\nfvObttOnT2+alZWV/cQTT3x11bpz586Udu3a9Thw4IAB7Nix45D5iAceeKDN/fffvzk9Pf0gQMOG\nDf3mm2/+WsOxW265pU1OTk63rl27dh86dOgJJSUlANx7773Hd+7cuXtGRkb2hRde+C2Af/7zn42z\nsrKys7Kysrt165a9c+fOlOir1+985zsZn3zySb2srKzsWbNmNY6+4n7zzTcbnXzyyZndu3fvdtpp\np3XduHFj3Uh5ZmZmdmZmZva4ceOOr7YvMaQkLCJSw+Tn59OnT58ylz3//PN88MEHLF26lNdee41b\nb72VrVu3AlBQUNBowoQJmzZs2JAPsGnTpvpjxozZvnbt2hVpaWkl9913X5v58+evLigo+PCkk07a\nd88997QqLCy0YcOGdX744Yc3rVq1qmDevHmrmjRpUnzHHXf8+6KLLtq5cuXKgquvvnpnZP9NmzYt\n6dev3+4pU6YcC/Dkk082u+CCC3bWr1//kOv7NWvWNDz11FP3VXast9566yf5+fkfrlmzZsX+/ftT\nJk2adCzA+PHjW+fn5xesXr264Omnn94I8NBDD7UeP378xpUrVxa8++67Kxs3blwSXdf06dPXdujQ\n4cDKlSsLzjvvvD2R8gMHDtj111/f8aWXXlq3YsWKD0eMGPHpLbfc0g5g1KhR6ZFjj+W7qaqkuB0t\nIpIs3nrrLYYOHUpqaiqtWrXizDPPZOHChQD07Nlzb1ZW1peRddu0afPl2WefvRdg7ty5x6xbt65B\n3759swAOHjxoffr02bNs2bIGxx9//MEzzzxzH0CzZs1KytjtIUaPHr197Nixra+88srP//rXv7Z4\n4oknNhzu8bz88stp48aNa11YWJjy+eef18nOzt4P7MrMzNw/aNCgThdffPHnw4YN+xzglFNO2XPL\nLbd0+MEPfrBj6NChOzt37lxprADLli2rv2bNmoYDBw7MgOD2fMuWLQ9++umnqbt37049//zz9wD8\n+Mc//uyNN944tuLaqkZXwiIiNUz37t1ZtGhRlbdr1KhRSXnz7s5pp532xcqVKwtWrlxZsG7duhVT\npkw5rJGXzj333L2bN2+uP2PGjLTi4mI7+eSTv9arSZcuXfa//fbbjSqqZ9++fXbzzTef8Pzzz69b\nvXp1wRVXXPFpYWFhCsCcOXPW/OxnP9u+ePHiRieeeGK3gwcPct9992374x//uHH//v0pp59+etaS\nJUsaxBKvu1uXLl32R4599erVBW+//faawzn2qlISFqmFzI7Oj1SPgQMHcuDAASZOnPhV2bJly3jz\nzTc5/fTTmTx5MsXFxWzfvp358+fTt2/fSuscMGDA3ry8vMb5+fn1Ab744ouUZcuW1e/Zs2fhJ598\nUnfevHmNIHjme/DgQZo0aVK8Z8+ecnPI5Zdf/tmPf/zjTldccUWZHYTcdttt2+644472mzZtqgNQ\nWFho48aNaxG9zr59+1IAWrduXbRr166U6dOnNwUoLi5m3bp19S666KLdjz766JY9e/ak7tq1K3XF\nihX1+/btu//Xv/71tp49e+7Nz8+PKQn37NmzcMeOHXVee+21YyC4PZ2Xl9egRYsWxWlpacWvvPJK\nY4Cnn366WSz1VYVuR0tSOlrzgRrMSyzMjBdeeIEbbriBsWPH0qBBA9LT03n44Yc57bTTeOedd+jV\nqxdmxgMPPEDr1q0rrbNt27ZFjz/++IbLL7/8W19++aUB/PKXv9zSs2fPA88+++y666+/vmNhYWFK\ngwYNSubPn7/6/PPP3/1///d/bbKysrJvvvnmraXrGzVq1Gdjx45tN2rUqB1l7W/IkCG7tm3bVufs\ns8/OdHfMjGHDhh2SsFu0aFE8bNiw7d26devesmXLol69eu0FKCoqsh/+8Ieddu/enerudtVVV33S\nokWL4ptvvrntggULmpiZZ2Zm7h88ePCuTZs21S1r/9EaNGjgkyZNWnf99dd33L17d2pxcbH99Kc/\n/Tg3N7fwT3/604arrroq3cwYMGDAF5WeyCoyr2HvyeTm5npeXl6Vtjla/wdew059rXKU/iSqLQnr\nNx9fH374Id26dUt0GFWSn5+/Lycn58Mjtb+nnnqq6UsvvXTciy+++K8jtc+j1dKlS1v06tUrvaxl\nuhIWEZFqNWLEiA5z5sw5dsaMGUfkuWpNpiQsIiLV6plnnvkI+CjRcdQEapglIiKSIErCIiIiCaLb\n0RJXR2sDITVDFpGjga6ERUREEkRJOIHsKP2IyNGvcePGiQ6hQjNmzEibPXv2MZH5m266qW1kqMJY\n3Xnnna06derUPSsrKzsnJ6fbI4880hygb9++mfPnz6+wt61YzZ8/v9HIkSM7AOzfv9/69++fERmU\nYsiQIScsWrQopg4/IDjms846q0tV9q/b0SIi31B1/+c10U9LSg91WHo+Fm+88UZa48aNi88555y9\nhxPDAw880PKNN95osmjRog+bNWtWsmPHjpRnn322WsYYjnbGGWfsO+OMM/YBLFiwoBHAypUrCwCi\nB6aIF10Ji4jUYHPnzmXAgAEMHjyYrKwshg0bRqQTpoULF9K/f3969erFkCFDGuzcuTNl3759Nnjw\n4PSMjIzsbt26ZU+fPj0NYPz48c0HDhzY5ZRTTsno379/5owZM9Kihz4EmDBhQrMePXp0y8rKyv7h\nD394QlFREQBTp05tkp2d3S0zMzO7X79+GatWrar35z//ueVjjz3WKjJsYCTeFStW1M/Ozv6qp5Pl\ny5cfMh/x29/+tvXEiRM3RgaMaNasWcl11133Wen1hg0b1jEnJ6dbly5dut94441tI+XXXnttu8hQ\nh6NHj24P8OSTTzbt2rVr98zMzOzc3NxM+M/V65YtW+r86Ec/6rR8+fJGWVlZ2WEXmF9dcZc3zOPU\nqVObdOrUqXt2dna3qVOnHlfV709XwiJyxBytjzsSfeX5TS1ZsoQVK1bQtm1bTj31VN5++2369u3L\nkCFDmDx5MieffDLvvPNOYePGjUvuvffeVmbG6tWrC5YsWdLgggsu6Lpu3bp8gBUrVjRatmzZilat\nWhXPmDEjraCgoNGSJUtWZGVlfbl48eIGU6dObZaXl7eyfv36fsUVV3R87LHHml966aW7xowZkz53\n7tyVWVlZX3788ceprVq1Kh4+fPj2xo0bF999990fA7z66qtNALp3734gLS2teMGCBQ379++///HH\nH28xbNiwQ5Lrjh07Uvbu3ZuanZ395deP9lDjxo3b0qpVq+KioiL69++f+d577zU84YQTvpw5c2bT\n9evX56ekpPDpp5+mAtx///1tXn311dWdOnU6GCmLaNeuXdGECRM2PvTQQ63mzJmzNnrZ1q1b60SG\neWzSpEnJz3/+89b33HNPq7vvvnvbmDFj0mfPnr2qe/fuByLjGleFroRFRGq4vn370r59e1JSUujd\nuzcbNmxg1apVtGnThpNPPhmAtLQ06taty4IFCxpfeeWVnwGceOKJhW3btv1y+fLlDQBOP/30L1q1\nalUcqTd66MNZs2al5efnN+rVq1e3rKys7LfeeqvJ+vXr68+dO/eYvn377o6sF719eUaOHPnpE088\n0aKoqIiXXnqp6ahRo752hRurZ555pll2dna37Ozs7DVr1jRYunRpg+bNmxfXr1+/ZMiQIenPPPPM\ncZFxhXNzc/cMGzYs/aGHHmoRuYqPRfQwj1lZWdmTJk1qvmnTpnoffPBBg/bt2x/o0aPHgZSUFEr/\nZyIWSsIiIjVc/fr1v5pOTU2lKgkmWiVDHdr3v//9zyLD/W3YsCF/3Lhx/z6c/YwYMWLnnDlzjp00\nadJxPXr02Ne6detDEnezZs1KGjVqVFJQUFCvonpWrlxZ75FHHmk1b9681atXry4YOHDgrsLCwpS6\ndevywQcffDh48OCdM2bMOG7AgAFdAZ577rlN9957778/+uijen369Mnetm1bakX1Rx17tQ3zWFpc\nk7CZnWdmq8xsrZndXsbyjmY2x8yWmNkyM7sgnvGIiCSLzMxMtm7dysKFCwHYs2cPBw8e5NRTT93z\n17/+tRkEg9lv3bq1Xs+ePb823m9p55133hczZsxoumXLljoAH3/8cerq1avrDRgwYO/777+ftnLl\nynqRcoC0tLTi3bt3l5nkGjVq5Geeeeaum266qePIkSPLHOrwhhtu2HrNNdecsGPHjhSAXbt2pURa\nR0fs3LkztWHDhiXNmjUr/uijj+rMnTv32Mi6O3bsSB0yZMiuxx577KOVK1c2guB59MCBA/c+/PDD\n/27atGnR+vXrK0zyEeUN89i7d+/CLVu21FuxYkV9gEmTJlV5qMO4PRM2s1TgUeAcYDOw0MymuXtB\n1Gq/AKa4+x/MLBuYCaTHKyYRkWRRr149Jk+ezHXXXcf+/fspKSlp8NZbb6XcdtttnwwfPvyEjIyM\n7NTUVB5//PENDRs2rPSxeJ8+fQp/8YtfbDn77LMzSkpKqFu3ro8fP37T2WefvXf8+PEbBg0a1KWk\npITmzZsfXLBgwZrLLrvs88GDB3d++eWXj3v44Yc3la5v+PDhO2bNmtX00ksvLXN4wNtuu237nj17\nUk466aTsunXrep06dfy6667bFr1Ov3799ufk5Ozr3LlzTps2bb7s06fPHoDPP/889cILL+xy4MAB\nA7jnnns+Arjxxhvbb9iwob6722mnnfbFKaecsn/mzJlplR17RcM8/v73v9944YUXdmnYsGHJt7/9\n7T179uyJ6eo6Im5DGZpZP+Aud/+vcP4OAHf/TdQ6jwPr3X1suP5D7t6/onpr01CGR2trkOoMS+e+\namr7UIa15bxrKMNv7s4772y1a9eu1N/97neHdUu7JknUUIbtOHQUjc3At0utcxfwqpldBxwDfCeO\n8YiIyFHgnHPO6bxx48b68+bNW53oWBIt0a8oDQWedveHwivhv5hZjrsf0jjAzEYDowE6duyYgDBF\nRKS6zJ49e12iYzhaxLNh1hagQ9R8+7As2ihgCoC7vwM0AFqUrsjdJ7p7rrvntmzZMk7hioiIHFnx\nTMILga5m1snM6gGXA9NKrbMJOBvAzLoRJOHtcYxJRKRaxKs9jdQuJSUlBpSUtzxuSdjdi4AxwCvA\nhwStoFeY2d1mdnG42s3A1Wa2FPgbMNL1yxaRo1yDBg347LPPlIilQiUlJbZ9+/Zjgfzy1onrM2F3\nn0nw2lF02Z1R0wXAqfGMQUSkurVv357NmzezfXvNuXG3bdu2OsXFxV973CdxVQLkFxUVXVXeColu\nmCUiUuPUrVuXTp06JTqMKsnOzl7u7rmJjkMOpW4rRUREEqTSJGxmF5mZkrWIiEg1iyW5DgHWmNkD\nZpYV74BERESSRaVJ2N2vAE4E1gFPm9k7ZjbazCrtb1NERETKF9NtZnf/ApgKTALaAIOAxWF3kyIi\nInIYYnkmfLGZvQDMBeoCfd39fKAXwXu+IiIichhieUXpMuC37j4/utDd95nZqPiEJSIiUvvFkoTv\nArZGZsysIdDK3Te4++vxCkxERKS2i+WZ8N85tN/L4rBMREREvoFYknAdd/8yMhNO14tfSCIiIskh\nliS8PWrABczsEuDT+IUkIiKSHGJ5JnwN8KyZPQIY8BEwPK5RiYiIJIFKk7C7rwNOMbPG4fyeuEcl\nIiKSBGIaRcnMvgt0BxqYGQDufncc4xIREan1Yums4zGC/qOvI7gd/X3ghDjHJSIiUuvF0jCrv7sP\nB3a6+6+AfkBGfMMSERGp/WJJwoXhv/vMrC1wkKD/aBEREfkGYnkmPN3MjgMeBBYDDjwR16hERESS\nQIVJ2MxSgNfd/XPgH2Y2A2jg7ruOSHQiIiK1WIW3o929BHg0av6AErCIiEj1iOWZ8OtmdplF3k0S\nERGRahFLEv4JwYANB8zsCzPbbWZfxDkuERGRWi+WHrPSjkQgIiIiyabSJGxmZ5RV7u7zqz8cERGR\n5BHLK0q3Rk03APoCi4CBcYlIREQkScRyO/qi6Hkz6wA8HLeIREREkkQsDbNK2wx0q+5AREREkk0s\nz4R/T9BLFgRJuzdBz1kiIiLyDcTyTDgvaroI+Ju7vx2neERERJJGLEl4KlDo7sUAZpZqZo3cfV98\nQxMREandYuoxC2gYNd8QeC0+4YiIiCSPWJJwA3ffE5kJpxvFLyQREZHkEEsS3mtmJ0VmzKwPsD9+\nIYmIiCSHWJ4J3wD83cz+DRjQGhgS16hERESSQCyddSw0sywgMyxa5e4H4xuWiIhI7Vfp7Wgz+xlw\njLvnu3s+0NjMro1/aCIiIrVbLM+Er3b3zyMz7r4TuDqWys3sPDNbZWZrzez2ctb5gZkVmNkKM3su\ntrBFRERqvlieCaeambm7Q/CeMFCvso3C9R4FziHo6nKhmU1z94KodboCdwCnuvtOMzv+cA5CRESk\nJorlSngWMNnMzjazs4G/hWWV6Qusdff17v4lMAm4pNQ6VwOPhlfXuPsnsYcuIiJSs8VyJfw/wGjg\np+H8bOCJGLZrB3wUNb8Z+HapdTIAzOxtIBW4y91jSfAiIiI1Xiyto0uAx8JPZCjDm4EHq2n/XYEB\nQHtgvpn1iH4GHe5zNMF/BOjYsWM17FZERCTxYhrK0Mxamtm1ZvYmMBdoFcNmW4AOUfPtw7Jom4Fp\n7n7Q3f8FrCZIyodw94nunuvuuS1btowlZBERkaNeuUnYzNLMbISZvQK8D3QGOrl7Z3e/JYa6FwJd\nzayTmdUDLgemlVrnRYKrYMysBcHt6fVVPwwREZGap6Lb0Z8QJN9fAG+5u5vZoFgrdvciMxsDvELw\nvPdJd19hZncDee4+LVx2rpkVAMXAre7+2eEejIiISE1i4ZtHX19gdgPB1esxBC2iJwOz3f1bRy68\nr8vNzfW8vLzKV4xiFqdgvqmyT33CVWdYOvdVU11h6bxXzVEaVrUys0XunpvoOORQ5d6OdveH3f0U\n/vNa0YtAWzP7HzPLOCLRiYiI1GKVNswK3/O9z917ALlAE2Bm3CMTERGp5WJqHR0R9h/9c3fvEq+A\nREREkkWVkrCIiIhUHyVhERGRBFESFhERSZBy3xM2s2XlLQLc3XvGJyQREZHkUFFnHSUEr889B0wH\n9h+RiERERJJERe8J9waGAo0JEvGvge7AFnffeGTCExERqb0qfCbs7ivd/ZfufhLB1fCfgRuPSGQi\nIiK1XIVDGZpZO4KuKwcBOwkS8AtHIC4REZFar6KGWfOANGAK8CMgMrBCPTNr5u47jkB8IiIitVZF\nV8InEDTM+gkwOqrcwvKEDuQgIiJS05WbhN09vbxl4W1qERER+QYOt7OOd6o1ChERkSR0uEn4aB2t\nVEREpMY43CScDGNgi4iIxFVFraN/T9nJ1oDj4haRiIhIkqiodXTeYS4TERGRGFTUOvqZIxmIiIhI\nstFQhiIiIgmiJCwiIpIglSZhM2t+JAIRERFJNrFcCb9rZn83swvMTO8Hi4iIVJNYknAGMBG4Elhj\nZveZWUZ8wxIREan9Kk3CHpjt7kOBq4ERwPtmNs/M+sU9QhERkVqqwvGE4atnwlcQXAl/DFwHTAN6\nA38HOsUzQBERkdqq0iRMMFjDX4DvufvmqPI8M3ssPmGJiIjUfrE8E/6Fu98TnYDN7PsA7j42bpGJ\niIjUcrEk4dvLKLujugMRERFJNhUN4HA+cAHQzszGRy1qAhTFOzAREZHarqJnwv8mGKjhYmBRVPlu\n4MZ4BiUiIpIMKhrAYSmw1MyedXdd+YqIiFSzim5HT3H3HwBLzOxr4wq7e8+4RiYiIlLLVXQ7+r/D\nfy88EoGIiIgkm3JbR7v71nDyWnffGP0Brj0y4YmIiNResbyidE4ZZedXdyAiIiLJptwkbGY/NbPl\nQKaZLYv6/AtYFkvlZnaema0ys7VmVtb7xpH1LjMzN7Pcqh+CiIhIzVTRM+HngJeB33Bohx273X1H\nZRWbWSrwKMGV9GZgoZlNc/eCUuulETx/fq+KsYuIiNRoFT0T3uXuG8LRkzYDBwEHGptZxxjq7gus\ndff17v4lMAm4pIz17gHGAoVVjl5ERKQGq/SZsJmNIRg9aTbwz/AzI4a62wEfRc1vDsui6z4J6ODu\n/6wkhtFmlmdmedu3b49h1yIiIke/WEZRugHIdPfPqnPHZpYCjANGVrauu08EJgLk5uZ+7Z1lERGR\nmiiW1tEfAbsOo+4tQIeo+fZhWUQakAPMNbMNwCnANDXOEhGRZBHLlfB6gkT5T+BApNDdx1Wy3UKg\nq5l1Iki+lwM/jNp+F9AiMm9mc4Fb3D0v5uhFRERqsFiS8KbwUy/8xMTdi8Lnya8AqcCT7r7CzO4G\n8tx92uEELCIiUluYe9UfsZpZnUQN6pCbm+t5eVW7WDaLUzDf1FH6dLs6w9K5r5rqCkvnvWqO0rCq\nlZktcnc97jvKVNRZx1tR038ptfj9uEUkIiKSJCpqmHVM1HROqWVH6/+zRUREaoyKkrCXM13WvIiI\niFRRRQ2zjjOzQQSJ+jgzuzQsN+DYuEcmIiJSy1WUhOcBF0dNXxS1bH7cIhIREUkS5SZhd//RkQxE\nREQk2cTSY5aIiIjEgZKwiIhIgigJi4iIJEi5z4SjWkOXyd2fr/5wREREkkdFraMjraGPB/oDb4Tz\nZwELACVhERGRb6DS1tFm9iqQ7e5bw/k2wNNHJDoREZFaLJZnwh0iCTj0MdAxTvGIiIgkjViGMnzd\nzF4B/hbODwFei19IIiIiyaHSJOzuY8LuK88Iiya6+wvxDUtERKT2i+VKGGAxsNvdXzOzRmaW5u67\n4xmYiIhIbVfpM2EzuxqYCjweFrUDXoxnUCIiIskgloZZPwNOBb4AcPc1BK8tiYiIyDcQSxI+4O5f\nRmbMrA4aT1hEROQbiyUJzzOz/wUamtk5wN+B6fENS0REpPaLJQnfDmwHlgM/AWa6+8/jGpWIiEgS\niKV19Inu/gTwRKTAzC509xnxC0tERKT2i+VK+Akzy4nMmNlQ4P/FLyQREZHkEMuV8GBgqpn9EDgd\nGA6cG9eoREREkkAsPWatN7PLCd4N3gSc6+774x6ZiIhILVfReMLLOfRVpGZAKvCemeHuPeMdnIiI\nSG1W0ZXwhUcsChERkSRUbsMsd9/o7hsJEvW2cLoTcAmw6wjFJyIiUmvF0jr6H0CxmXUBJgIdgOfi\nGpWIiEgSiCUJl7h7EXAp8Ht3vxVoE9+wREREar9YkvDB8N3g4UCkg4668QtJREQkOcSShH8E9AN+\n7e7/MrNOwF/iG5aIiEjtF8t7wgXA9VHz/wLGxjMoERGRZFBpEjazrsBvgGygQaTc3b8Vx7hERERq\nvVhuRz8F/AEoAs4C/gz8NZ5BiYiIJINYknBDd38dsPDd4buA78Y3LBERkdovliR8wMxSgDVmNsbM\nBgGNY6nczM4zs1VmttbMbi9j+U1mVmBmy8zsdTM7oYrxi4iI1FixJOH/BhoRNM7qA1wJjKhsIzNL\nBR4Fzid4njzUzLJLrbYEyA37oZ4KPBB76CIiIjVbLK2jF4aTewheV4pVX2Ctu68HMLNJBF1eFkTV\nPSdq/XeBK6pQv4iISI1W0ShK0yra0N0vrqTudsBHUfObgW9XsP4o4OVK6hQREak1KroS7keQRP8G\nvAdYvIIwsyuAXODMcpaPBkYDdOzYMV5hiIiIHFEVPRNuDfwvkAP8DjgH+NTd57n7vBjq3kIw2ENE\n+7DsEGb2HeDnwMXufqCsitx9orvnuntuy5YtY9i1iIjI0a+ioQyL3X2Wu48ATgHWAnPNbEyMdS8E\nuppZJzOrB1wOHHKL28xOBB4nSMCfHNYRiIiI1FAVNswys/oE7wQPBdKB8cALsVTs7kVhwn4FSAWe\ndPcVZnY3kOfu04AHCV53+rs2OgsvAAANDUlEQVSZAWyK4VmziIhIrVBRw6w/E9yKngn8yt3zq1q5\nu88Mt48uuzNq+jtVrVNERKS2qOhK+ApgL8F7wteHV6oQNNByd28S59hERERqtXKTsLvH0pGHiIiI\nHCYlWhERkQRREhYREUkQJWEREZEEURIWERFJECVhERGRBFESFhERSRAlYRERkQRREhYREUkQJWER\nEZEEURIWERFJECVhERGRBFESFhERSRAlYRERkQRREhYREUkQJWEREZEEURIWERFJECVhERGRBFES\nFhERSRAlYRERkQRREhYREUkQJWEREZEEURIWERFJECVhERGRBFESFhERSRAlYRERkQRREhYREUkQ\nJWEREZEEURIWERFJECVhERGRBFESFhERSRAlYRERkQRREhYREUkQJWEREZEEURIWERFJECVhERGR\nBIlrEjaz88xslZmtNbPby1he38wmh8vfM7P0eMYjIiJyNIlbEjazVOBR4HwgGxhqZtmlVhsF7HT3\nLsBvgbHxikdERORoE88r4b7AWndf7+5fApOAS0qtcwnwTDg9FTjbzCyOMYmIiBw14pmE2wEfRc1v\nDsvKXMfdi4BdQPM4xiQiInLUqJPoAGJhZqOB0eHsHjNblch4qo3RAvg00WGUlhS3InTuE0PnPZEy\nEx2AfF08k/AWoEPUfPuwrKx1NptZHeBY4LPSFbn7RGBinOJMGDPLc/fcRMeRjHTuE0PnPXHMLC/R\nMcjXxfN29EKgq5l1MrN6wOXAtFLrTANGhNODgTfc3eMYk4iIyFEjblfC7l5kZmOAV4BU4El3X2Fm\ndwN57j4N+BPwFzNbC+wgSNQiIiJJIa7PhN19JjCzVNmdUdOFwPfjGcNRrtbdYq9BdO4TQ+c9cXTu\nj0Kmu78iIiKJoW4rRUREEkRJOEZm5mb2UNT8LWZ2Vxz287+l5hdU9z5qsur8HszsODO79jC33WBm\nLQ5n25rIzIrN7AMzyzezv5tZo8Oo44+RXvP0OxcJKAnH7gBw6RH4w3vIHyd37x/n/dU01fk9HAeU\nmYTDV+bkP/a7e293zwG+BK6pagXufpW7F4Sz+p2LoCRcFUUEDRtuLL3AzFqa2T/MbGH4OTWqfLaZ\nrQivAjZGkoeZvWhmi8Jlo8Oy+4GG4RXHs2HZnvDfSWb23ah9Pm1mg80s1cweDPe7zMx+EvczkViH\n8z3cZWa3RK2XHw4Wcj/QOTzfD5rZADN708ymAQXhul/7noQ3gS4AZnZTeD7zzeyGsOwYM/unmS0N\ny4eE5XPNLFe/c5Eo7q5PDB9gD9AE2EDQqcgtwF3hsueA08LpjsCH4fQjwB3h9HmAAy3C+Wbhvw2B\nfKB5ZD+l9xv+Owh4JpyuR9DdZ0OCnsR+EZbXB/KATok+X0fZ93AXcEtUHflAevjJjyofAOyNPn8V\nfE8bIt9lMnyifod1gJeAnwJ9gOXAMUBjYAVwInAZ8ETUtseG/84FcqPrK6N+/c71SaqPbrlVgbt/\nYWZ/Bq4H9kct+g6QHTX2RBMzawycRvBHBXefZWY7o7a53swGhdMdgK6U0VtYlJeB35lZfYKEPt/d\n95vZuUBPMxscrndsWNe/Dvc4j3aH8T1UxfvuHn3uqvo91VYNzeyDcPpNgnf8fwq84O57AczseeB0\nYBbwkJmNBWa4+5tV2I9+55JUlISr7mFgMfBUVFkKcIoH7z1/xcoZEMrMBhAkjH7uvs/M5gINKtqp\nuxeG6/0XMIRgVCoIur29zt1fqeqB1HBV+R6KOPTRS0Xnem/UdgOo4vdUi+13997RBeX9vt19tZmd\nBFwA3Gtmr7v73bHsRL9zSTZ6JlxF7r4DmEIwFnLEq8B1kRkzi/yxehv4QVh2LtA0LD+WYBzlfWaW\nBZwSVddBM6tbzu4nAz/iP1cbEPRI9tPINmaWYWbHHObh1RhV/B42ACeFZScBncLy3UBaBbup6HuS\n4Ir4e2bWKPzNDQLeNLO2wD53/yvwIOG5L0W/cxGUhA/XQ0B069zrgdywwUgB/2k5+ivgXDPLJ+gZ\nbBvBH/5ZQB0z+5CgcdC7UXVNBJZFGqyU8ipwJvCaB2M0A/yRoBHR4nA/j5M8dzhi/R7+ATQzsxXA\nGGA1gLt/BrwdNh56sIz6K/qekp67LwaeBt4H3gP+6O5LgB7A++Ht618C95axuX7nIqjHrLgKn2sV\ne9CPdj/gD6Vv6YmISPLS/yTjqyMwxcxSCN6tvDrB8YiIyFFEV8IiIiIJomfCIiIiCaIkLCIikiBK\nwiIiIgmiJJxkzOznYT/Iy8K+e799mPX0NrMLouYvNrPbqy/SMvc5wMzK7ejfzM43szwzKzCzJRaO\ntlS67+hqiGNB1PSD4fl80MyuMbPhh1HfIaM5mVlbM5taDXG+EH7Ha81sVzj9QUXn8DD3U8fMPq/C\n+vdG+pmOR/0iNYlaRyeR8DWpC4GT3P2ABYNJ1DvM6noDucBMAHefBkyrlkDLN4Cg7+ivDXtnZjkE\nfXV/191XmlkqQX/D1c4PHfFnNEH/0sXfoMrIaE4Twvr/DQyucIsYuPsg+Krnr1vc/cKy1jOzOu5e\n9E33JyJVpyvh5NIG+NTdDwC4+6fhH3zMrI+ZzbNgxKBXzKxNWD7XzMaa2ftmttrMTjezesDdwJDw\nymqImY00s0fCbZ42sz+Y2btmtj68gn3SzD40s6cjwZjZuWb2jpkttmCM2sZh+QYz+1VYvtzMsiwY\n9ega4MZwn6eXOrbbgF+7+8rw2Ird/Q+lT4CZXW3BSDxLLRhxqVFY/v2w046lZjY/LOseHvcH4Z2D\nrmF5ZMSfaQQDFywKz8FXV9xm1sXMXgvrW2xmnc2ssZm9HnVcl4RhlR7NKT3skAIza2BmT4XrLzGz\ns8LykWb2vJnNMrM1ZvZAVX4IZrbZzO43syXAIDN7y8IexsystZmtDafrmNm48DwsM7OrqrCPS8zs\nvTDuV83s+KjFJ4a/jzVm9uOobW6P2tedVTkmkRop0SNI6HPkPgQJ4wOCHqMmAGeG5XUJri5bhvND\ngCfD6bnAQ+H0BQS9GAGMBB6JqvureYJelCYR9Pd7CfAFQS9KKcAigqvoFsB84Jhwm/8B7gynNxD0\nEwzBFeIfw+m7iBoNqdSxLQZ6lbPsq+0IR0EKp++N2s9yoF04fVz47++BYeF0PaBhOL0nqo495ezn\nPWBQON0AaERw56lJWNYCWBueo3QOHc3pq3ng5qjvIgvYFNY3ElhP0LVmA2Aj0KGc4x9AMJBCdNlm\n4Kao+beA3uF0a2Bt1Pm/PZyuDywBOpaqqw7weRn7bcp/XoO8Bhgbdd4Xh3EfH8bSiuD3NSE8JykE\nPZb1L69+ffSpDR/djk4i7r7HzPoQ9Ml7FjDZgue4eUAOMNuCTvlTga1Rmz4f/ruIIEHEYrq7u5kt\nBz529+UAFnQdmQ60B7IJuo2EIMm9U84+L439KCuVY2b3EtwCbkzQJzEE/Xw/bWZTovb9DvBzM2sP\nPO/ua2LZgZmlEST0FyAYlCAsrwvcZ2ZnACVAO4LkU5HTCP4zgAe32TcCGeGy1919V1h3AXACwdB/\nsZocwzrnAt3M7PJwPjJ60aYYto10VtOaIIGvjlr2YnheCsM7DycTDJZxPkGih+D7ySDoFlOkVlIS\nTjIePLucC8wNE+QIgkS3wt37lbPZgfDfYmL/zUS2KYmajszXCeua7e5Dq2mfKwjGt11ayXpPA99z\n96VmNpLgKhF3v8aCRmrfJbi93MfdnzOz98KymWb2E3d/I4ZYyjMMaAn0cfeDZraBbzYqU/R5rcp3\nE7E3ajp6pKnomAy41t1fr3p4PArc5+4zzew7QHTDvdK9BHm4r3vd/U/RC8xMf6ek1tIz4SRiZpmR\n55qh3gS3MVcBLS1ouIWZ1TWz7pVUV9kIRJV5FzjVzLqE+zzGzDIq2aaifT4I/G+kDjNLMbNrylgv\nDdgaXpUOixSaWWd3f8/d7wS2Ax3M7FvAencfTzCQfc9YDszddwObzex7Yd31w2fPxwKfhAn4LIIr\n18qO681InOGxdST4vqrbBoL/xMChjcJeAa6NJMLwN9QwxjqPBbZYcKtjRKll3wvPS0uCOzN54b5G\nWTg6kpm1t6DxoEitpSScXBoDz1jwCs8ygtvBd3kwUs1gYKyZLSV4blzZayxzgOywMdGQqgbi7tsJ\nnmv+LYzlHYJnnhWZTtCI6GsNs9x9GXBDWN+HQD7wrTLq+H8Ez2vfBlZGlT8YNn7KJ3g+vpRgGMp8\nC0YDygH+XIVDvBK4Pjy2BQTPWZ8lGOVpOTA8sn+veDSnCUBKuM1kYKSHDeuq2YPAf5vZYv4z5CYE\noxWtAT4Iz80fKPuKu0nY2CvyuZ7gGfkLwELg41Lr5wPzCM7NL939Y3efCUwF3g2PdwrBb1ak1lLf\n0SIiIgmiK2EREZEEURIWERFJECVhERGRBFESFhERSRAlYRERkQRREhYREUkQJWEREZEEURIWERFJ\nkP8Pxr9valW1OCUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HKwE4hrs6LJ",
        "colab_type": "code",
        "outputId": "ab9533a0-3ead-4b0f-b43b-b1b30025f141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cc_neu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8133068696860807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il1l5oLaB7dD",
        "colab_type": "code",
        "outputId": "0f18bb91-7961-4641-a3e0-ca973a175157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "float(masked_lm_acc/len(correctly_classified))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.878968253968254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngem-phNpjuD",
        "colab_type": "text"
      },
      "source": [
        "#End of Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPHXLYLhih5Z",
        "colab_type": "code",
        "outputId": "cd735ef6-cb0c-43b7-f821-5cc71c444302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "random_test_features = run_classifier.convert_examples_to_features(test_InputExamples_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "fixed_test_features = run_classifier.convert_examples_to_features(test_InputExamples_fixed_doc, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "ind = 1\n",
        "for ckpt in range(92,691,46):\n",
        "  ind += 1\n",
        "  print('----------------------starting epoch %d ----------------------'%ind)\n",
        "  print('evaluating trainset: ')\n",
        "  predictions = model_predict(estimator_from_tfhub,train_InputExamples_doc,train_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(train_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('evaluating devset: ')\n",
        "  \n",
        "  predictions = model_predict(estimator_from_tfhub,eval_examples,eval_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  print('evaluating random testset: ')\n",
        "  \n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc,random_test_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "    \n",
        "  print('evaluating fixed testset: ')\n",
        "\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc,fixed_test_features,checkpoint_path=OUTPUT_DIR+'/model.ckpt-%d'%ckpt)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------starting epoch 2 ----------------------\n",
            "evaluating trainset: \n",
            "[[  0   0 145]\n",
            " [  0   0 443]\n",
            " [  0   0 913]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       145\n",
            "     Neutral       0.00      0.00      0.00       443\n",
            "    Positive       0.61      1.00      0.76       913\n",
            "\n",
            "   micro avg       0.61      0.61      0.61      1501\n",
            "   macro avg       0.20      0.33      0.25      1501\n",
            "weighted avg       0.37      0.61      0.46      1501\n",
            "\n",
            "evaluating devset: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  0   0  23]\n",
            " [  0   0  97]\n",
            " [  0   0 164]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.00      0.00      0.00        97\n",
            "    Positive       0.58      1.00      0.73       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.19      0.33      0.24       284\n",
            "weighted avg       0.33      0.58      0.42       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  0   0  28]\n",
            " [  0   0  87]\n",
            " [  0   0 170]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        28\n",
            "     Neutral       0.00      0.00      0.00        87\n",
            "    Positive       0.60      1.00      0.75       170\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       285\n",
            "   macro avg       0.20      0.33      0.25       285\n",
            "weighted avg       0.36      0.60      0.45       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  0   0  50]\n",
            " [  0   0 116]\n",
            " [  0   0 173]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        50\n",
            "     Neutral       0.00      0.00      0.00       116\n",
            "    Positive       0.51      1.00      0.68       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.17      0.33      0.23       339\n",
            "weighted avg       0.26      0.51      0.34       339\n",
            "\n",
            "----------------------starting epoch 3 ----------------------\n",
            "evaluating trainset: \n",
            "[[ 80  57   8]\n",
            " [ 37 268 138]\n",
            " [ 10 102 801]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.63      0.55      0.59       145\n",
            "     Neutral       0.63      0.60      0.62       443\n",
            "    Positive       0.85      0.88      0.86       913\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      1501\n",
            "   macro avg       0.70      0.68      0.69      1501\n",
            "weighted avg       0.76      0.77      0.76      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  7  12   4]\n",
            " [  4  37  56]\n",
            " [  3  35 126]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.30      0.38        23\n",
            "     Neutral       0.44      0.38      0.41        97\n",
            "    Positive       0.68      0.77      0.72       164\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       284\n",
            "   macro avg       0.54      0.48      0.50       284\n",
            "weighted avg       0.58      0.60      0.59       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  6  13   9]\n",
            " [  4  41  42]\n",
            " [  3  39 128]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.46      0.21      0.29        28\n",
            "     Neutral       0.44      0.47      0.46        87\n",
            "    Positive       0.72      0.75      0.73       170\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       285\n",
            "   macro avg       0.54      0.48      0.49       285\n",
            "weighted avg       0.61      0.61      0.61       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  9  25  16]\n",
            " [  8  53  55]\n",
            " [ 10  58 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.18      0.23        50\n",
            "     Neutral       0.39      0.46      0.42       116\n",
            "    Positive       0.60      0.61      0.60       173\n",
            "\n",
            "   micro avg       0.49      0.49      0.49       339\n",
            "   macro avg       0.44      0.41      0.42       339\n",
            "weighted avg       0.49      0.49      0.49       339\n",
            "\n",
            "----------------------starting epoch 4 ----------------------\n",
            "evaluating trainset: \n",
            "[[127  14   4]\n",
            " [ 70 327  46]\n",
            " [  8 112 793]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.88      0.73       145\n",
            "     Neutral       0.72      0.74      0.73       443\n",
            "    Positive       0.94      0.87      0.90       913\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      1501\n",
            "   macro avg       0.76      0.83      0.79      1501\n",
            "weighted avg       0.85      0.83      0.83      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 9 12  2]\n",
            " [ 6 53 38]\n",
            " [ 4 65 95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.47      0.39      0.43        23\n",
            "     Neutral       0.41      0.55      0.47        97\n",
            "    Positive       0.70      0.58      0.64       164\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       284\n",
            "   macro avg       0.53      0.51      0.51       284\n",
            "weighted avg       0.58      0.55      0.56       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 9 13  6]\n",
            " [12 45 30]\n",
            " [ 9 62 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.32      0.31        28\n",
            "     Neutral       0.38      0.52      0.43        87\n",
            "    Positive       0.73      0.58      0.65       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.47      0.47      0.46       285\n",
            "weighted avg       0.58      0.54      0.55       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[16 25  9]\n",
            " [10 67 39]\n",
            " [13 75 85]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.41      0.32      0.36        50\n",
            "     Neutral       0.40      0.58      0.47       116\n",
            "    Positive       0.64      0.49      0.56       173\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       339\n",
            "   macro avg       0.48      0.46      0.46       339\n",
            "weighted avg       0.52      0.50      0.50       339\n",
            "\n",
            "----------------------starting epoch 5 ----------------------\n",
            "evaluating trainset: \n",
            "[[135   7   3]\n",
            " [ 44 350  49]\n",
            " [  7  23 883]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.73      0.93      0.82       145\n",
            "     Neutral       0.92      0.79      0.85       443\n",
            "    Positive       0.94      0.97      0.96       913\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      1501\n",
            "   macro avg       0.86      0.90      0.87      1501\n",
            "weighted avg       0.92      0.91      0.91      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  2  14   7]\n",
            " [  1  37  59]\n",
            " [  3  41 120]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.40      0.38      0.39        97\n",
            "    Positive       0.65      0.73      0.69       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.46      0.40      0.41       284\n",
            "weighted avg       0.54      0.56      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  5  10  13]\n",
            " [ 10  28  49]\n",
            " [  5  38 127]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.18      0.21        28\n",
            "     Neutral       0.37      0.32      0.34        87\n",
            "    Positive       0.67      0.75      0.71       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.43      0.42      0.42       285\n",
            "weighted avg       0.54      0.56      0.55       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[ 10  21  19]\n",
            " [  5  45  66]\n",
            " [  7  45 121]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.45      0.20      0.28        50\n",
            "     Neutral       0.41      0.39      0.40       116\n",
            "    Positive       0.59      0.70      0.64       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.48      0.43      0.44       339\n",
            "weighted avg       0.51      0.52      0.50       339\n",
            "\n",
            "----------------------starting epoch 6 ----------------------\n",
            "evaluating trainset: \n",
            "[[135   7   3]\n",
            " [  8 425  10]\n",
            " [  6  33 874]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.91      0.93      0.92       145\n",
            "     Neutral       0.91      0.96      0.94       443\n",
            "    Positive       0.99      0.96      0.97       913\n",
            "\n",
            "   micro avg       0.96      0.96      0.96      1501\n",
            "   macro avg       0.94      0.95      0.94      1501\n",
            "weighted avg       0.96      0.96      0.96      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 2 18  3]\n",
            " [ 2 57 38]\n",
            " [ 2 72 90]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.39      0.59      0.47        97\n",
            "    Positive       0.69      0.55      0.61       164\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       284\n",
            "   macro avg       0.47      0.41      0.41       284\n",
            "weighted avg       0.56      0.52      0.52       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 2 19  7]\n",
            " [ 4 51 32]\n",
            " [ 1 78 91]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.07      0.11        28\n",
            "     Neutral       0.34      0.59      0.43        87\n",
            "    Positive       0.70      0.54      0.61       170\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       285\n",
            "   macro avg       0.44      0.40      0.38       285\n",
            "weighted avg       0.55      0.51      0.51       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[ 6 32 12]\n",
            " [ 0 74 42]\n",
            " [ 4 78 91]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.12      0.20        50\n",
            "     Neutral       0.40      0.64      0.49       116\n",
            "    Positive       0.63      0.53      0.57       173\n",
            "\n",
            "   micro avg       0.50      0.50      0.50       339\n",
            "   macro avg       0.54      0.43      0.42       339\n",
            "weighted avg       0.55      0.50      0.49       339\n",
            "\n",
            "----------------------starting epoch 7 ----------------------\n",
            "evaluating trainset: \n",
            "[[139   3   3]\n",
            " [  4 432   7]\n",
            " [  6   8 899]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.93      0.96      0.95       145\n",
            "     Neutral       0.98      0.98      0.98       443\n",
            "    Positive       0.99      0.98      0.99       913\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      1501\n",
            "   macro avg       0.97      0.97      0.97      1501\n",
            "weighted avg       0.98      0.98      0.98      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  2  16   5]\n",
            " [  2  48  47]\n",
            " [  2  54 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.09      0.14        23\n",
            "     Neutral       0.41      0.49      0.45        97\n",
            "    Positive       0.68      0.66      0.67       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.47      0.41      0.42       284\n",
            "weighted avg       0.56      0.56      0.55       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  1  20   7]\n",
            " [  6  43  38]\n",
            " [  1  60 109]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.12      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.71      0.64      0.67       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.39      0.39      0.38       285\n",
            "weighted avg       0.54      0.54      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  4  29  17]\n",
            " [  1  59  56]\n",
            " [  2  54 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.57      0.08      0.14        50\n",
            "     Neutral       0.42      0.51      0.46       116\n",
            "    Positive       0.62      0.68      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.53      0.42      0.41       339\n",
            "weighted avg       0.54      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 8 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  3 436   4]\n",
            " [  4   8 901]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.95      0.97      0.96       145\n",
            "     Neutral       0.98      0.98      0.98       443\n",
            "    Positive       0.99      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      1501\n",
            "   macro avg       0.97      0.98      0.98      1501\n",
            "weighted avg       0.98      0.98      0.98      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  1  18   4]\n",
            " [  2  48  47]\n",
            " [  2  59 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.38      0.49      0.43        97\n",
            "    Positive       0.67      0.63      0.65       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.42      0.39      0.38       284\n",
            "weighted avg       0.53      0.54      0.53       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  3  19   6]\n",
            " [  6  43  38]\n",
            " [  1  65 104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.11      0.16        28\n",
            "     Neutral       0.34      0.49      0.40        87\n",
            "    Positive       0.70      0.61      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.45      0.40      0.40       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  29  16]\n",
            " [  1  64  51]\n",
            " [  2  58 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.10      0.17        50\n",
            "     Neutral       0.42      0.55      0.48       116\n",
            "    Positive       0.63      0.65      0.64       173\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       339\n",
            "   macro avg       0.56      0.43      0.43       339\n",
            "weighted avg       0.56      0.54      0.52       339\n",
            "\n",
            "----------------------starting epoch 9 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  3 438   2]\n",
            " [  3   6 904]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.96      0.97      0.96       145\n",
            "     Neutral       0.98      0.99      0.99       443\n",
            "    Positive       0.99      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.98      0.98      0.98      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 4 16  3]\n",
            " [ 2 51 44]\n",
            " [ 3 62 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.44      0.17      0.25        23\n",
            "     Neutral       0.40      0.53      0.45        97\n",
            "    Positive       0.68      0.60      0.64       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.51      0.43      0.45       284\n",
            "weighted avg       0.56      0.54      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  6  47  34]\n",
            " [  1  69 100]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.22      0.07      0.11        28\n",
            "     Neutral       0.34      0.54      0.42        87\n",
            "    Positive       0.72      0.59      0.65       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.43      0.40      0.39       285\n",
            "weighted avg       0.56      0.52      0.52       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  27  18]\n",
            " [  1  63  52]\n",
            " [  2  60 111]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.10      0.17        50\n",
            "     Neutral       0.42      0.54      0.47       116\n",
            "    Positive       0.61      0.64      0.63       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.55      0.43      0.42       339\n",
            "weighted avg       0.55      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 10 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  1 441   1]\n",
            " [  3   3 907]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.97      0.97      0.97       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.98      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  4  15   4]\n",
            " [  2  43  52]\n",
            " [  2  55 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.17      0.26        23\n",
            "     Neutral       0.38      0.44      0.41        97\n",
            "    Positive       0.66      0.65      0.65       164\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       284\n",
            "   macro avg       0.51      0.42      0.44       284\n",
            "weighted avg       0.55      0.54      0.54       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  19   7]\n",
            " [  7  44  36]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.35      0.51      0.41        87\n",
            "    Positive       0.71      0.62      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  5  25  20]\n",
            " [  0  57  59]\n",
            " [  2  52 119]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.71      0.10      0.18        50\n",
            "     Neutral       0.43      0.49      0.46       116\n",
            "    Positive       0.60      0.69      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.58      0.43      0.42       339\n",
            "weighted avg       0.56      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 11 ----------------------\n",
            "evaluating trainset: \n",
            "[[140   2   3]\n",
            " [  0 442   1]\n",
            " [  1   4 908]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 1 47 49]\n",
            " [ 1 65 98]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.08        23\n",
            "     Neutral       0.36      0.48      0.41        97\n",
            "    Positive       0.65      0.60      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.45      0.38      0.37       284\n",
            "weighted avg       0.53      0.51      0.51       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  1  22   5]\n",
            " [  3  50  34]\n",
            " [  1  67 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.06        28\n",
            "     Neutral       0.36      0.57      0.44        87\n",
            "    Positive       0.72      0.60      0.66       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.43      0.40      0.39       285\n",
            "weighted avg       0.56      0.54      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  2  30  18]\n",
            " [  0  63  53]\n",
            " [  1  58 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.04      0.08        50\n",
            "     Neutral       0.42      0.54      0.47       116\n",
            "    Positive       0.62      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.57      0.41      0.39       339\n",
            "weighted avg       0.56      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 12 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  1 441   1]\n",
            " [  4   2 907]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.97      0.97      0.97       145\n",
            "     Neutral       0.99      1.00      0.99       443\n",
            "    Positive       1.00      0.99      0.99       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.98      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 4 16  3]\n",
            " [ 3 50 44]\n",
            " [ 4 66 94]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.36      0.17      0.24        23\n",
            "     Neutral       0.38      0.52      0.44        97\n",
            "    Positive       0.67      0.57      0.62       164\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       284\n",
            "   macro avg       0.47      0.42      0.43       284\n",
            "weighted avg       0.54      0.52      0.52       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[ 4 19  5]\n",
            " [ 8 46 33]\n",
            " [ 1 70 99]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.31      0.14      0.20        28\n",
            "     Neutral       0.34      0.53      0.41        87\n",
            "    Positive       0.72      0.58      0.64       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.46      0.42      0.42       285\n",
            "weighted avg       0.57      0.52      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  6  27  17]\n",
            " [  1  66  49]\n",
            " [  3  62 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.12      0.20        50\n",
            "     Neutral       0.43      0.57      0.49       116\n",
            "    Positive       0.62      0.62      0.62       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.55      0.44      0.44       339\n",
            "weighted avg       0.55      0.53      0.51       339\n",
            "\n",
            "----------------------starting epoch 13 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  3   1 909]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.98      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[  1  18   4]\n",
            " [  1  43  53]\n",
            " [  2  60 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.07        23\n",
            "     Neutral       0.36      0.44      0.39        97\n",
            "    Positive       0.64      0.62      0.63       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.42      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.51       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  20   6]\n",
            " [  5  45  37]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.07      0.11        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.71      0.62      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.44      0.40      0.40       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  28  19]\n",
            " [  0  58  58]\n",
            " [  1  55 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.06      0.11        50\n",
            "     Neutral       0.41      0.50      0.45       116\n",
            "    Positive       0.60      0.68      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.59      0.41      0.40       339\n",
            "weighted avg       0.56      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 14 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  2   1 910]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 2 47 48]\n",
            " [ 2 66 96]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.36      0.48      0.41        97\n",
            "    Positive       0.65      0.59      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.40      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.50       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  7  45  35]\n",
            " [  1  67 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.34      0.52      0.41        87\n",
            "    Positive       0.72      0.60      0.65       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.52      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  30  17]\n",
            " [  0  61  55]\n",
            " [  2  56 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.06      0.11        50\n",
            "     Neutral       0.41      0.53      0.46       116\n",
            "    Positive       0.61      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.54      0.42      0.40       339\n",
            "weighted avg       0.54      0.53      0.50       339\n",
            "\n",
            "----------------------starting epoch 15 ----------------------\n",
            "evaluating trainset: \n",
            "[[141   1   3]\n",
            " [  0 442   1]\n",
            " [  2   1 910]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.99      0.97      0.98       145\n",
            "     Neutral       1.00      1.00      1.00       443\n",
            "    Positive       1.00      1.00      1.00       913\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1501\n",
            "   macro avg       0.99      0.99      0.99      1501\n",
            "weighted avg       0.99      0.99      0.99      1501\n",
            "\n",
            "evaluating devset: \n",
            "[[ 1 19  3]\n",
            " [ 2 46 49]\n",
            " [ 2 65 97]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.07        23\n",
            "     Neutral       0.35      0.47      0.41        97\n",
            "    Positive       0.65      0.59      0.62       164\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       284\n",
            "   macro avg       0.40      0.37      0.37       284\n",
            "weighted avg       0.51      0.51      0.50       284\n",
            "\n",
            "evaluating random testset: \n",
            "[[  2  21   5]\n",
            " [  7  45  35]\n",
            " [  1  66 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.07      0.11        28\n",
            "     Neutral       0.34      0.52      0.41        87\n",
            "    Positive       0.72      0.61      0.66       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.55      0.53      0.53       285\n",
            "\n",
            "evaluating fixed testset: \n",
            "[[  3  29  18]\n",
            " [  0  61  55]\n",
            " [  2  56 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.06      0.11        50\n",
            "     Neutral       0.42      0.53      0.47       116\n",
            "    Positive       0.61      0.66      0.64       173\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       339\n",
            "   macro avg       0.54      0.42      0.40       339\n",
            "weighted avg       0.54      0.53      0.50       339\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu5R3dU7VsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par)\n",
        "# import numpy as np\n",
        "# from sklearn import metrics\n",
        "# labels = [\"Negative\",\"Neutral\", \"Positive\"]\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_fixed_par['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "# predictions = model_predict(estimator_from_tfhub,test_InputExamples_par)\n",
        "# labels_val = []\n",
        "# for item in predictions:\n",
        "#   labels_val.append(labels[np.argmax(item[1])])\n",
        "# true_label = list(test_par['sentiment'])\n",
        "# print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "# print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqh11HPVAsvx",
        "colab_type": "code",
        "outputId": "9a40b4f2-c66c-4a6b-cd89-e7f156a2a794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "\n",
        "\n",
        "print('-------- Document level Dev Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Fixed Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Random Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Document level Dev Result---------\n",
            "[[  2  11  10]\n",
            " [  0  47  50]\n",
            " [  2  46 116]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.09      0.15        23\n",
            "     Neutral       0.45      0.48      0.47        97\n",
            "    Positive       0.66      0.71      0.68       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.54      0.43      0.43       284\n",
            "weighted avg       0.58      0.58      0.57       284\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  2  27  21]\n",
            " [  4  49  63]\n",
            " [  2  50 121]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.07        50\n",
            "     Neutral       0.39      0.42      0.40       116\n",
            "    Positive       0.59      0.70      0.64       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.41      0.39      0.37       339\n",
            "weighted avg       0.47      0.51      0.48       339\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  11  16]\n",
            " [  2  42  43]\n",
            " [  2  51 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.04      0.06        28\n",
            "     Neutral       0.40      0.48      0.44        87\n",
            "    Positive       0.66      0.69      0.68       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.42      0.40      0.39       285\n",
            "weighted avg       0.54      0.56      0.54       285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdebsYZ08XI",
        "colab_type": "code",
        "outputId": "6e23d840-1db4-4146-a52c-9690ac4bc483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "print('-------- Document level Dev Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(dev_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Fixed Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_fixed_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "print('-------- Document level Random Test Result---------')\n",
        "predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "labels_val = []\n",
        "for item in predictions:\n",
        "  labels_val.append(labels[np.argmax(item[1])])\n",
        "true_label = list(test_doc['sentiment'])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Document level Dev Result---------\n",
            "[[  2  15   6]\n",
            " [  0  48  49]\n",
            " [  2  52 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.09      0.15        23\n",
            "     Neutral       0.42      0.49      0.45        97\n",
            "    Positive       0.67      0.67      0.67       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.53      0.42      0.42       284\n",
            "weighted avg       0.57      0.56      0.55       284\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  3  28  19]\n",
            " [  5  53  58]\n",
            " [  2  60 111]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.06      0.10        50\n",
            "     Neutral       0.38      0.46      0.41       116\n",
            "    Positive       0.59      0.64      0.61       173\n",
            "\n",
            "   micro avg       0.49      0.49      0.49       339\n",
            "   macro avg       0.42      0.39      0.38       339\n",
            "weighted avg       0.47      0.49      0.47       339\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  2  16  10]\n",
            " [  2  45  40]\n",
            " [  2  60 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.07      0.12        28\n",
            "     Neutral       0.37      0.52      0.43        87\n",
            "    Positive       0.68      0.64      0.66       170\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       285\n",
            "   macro avg       0.46      0.41      0.40       285\n",
            "weighted avg       0.55      0.54      0.54       285\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tq0IPSncltM",
        "colab_type": "code",
        "outputId": "e4ce8ce3-e6b4-4541-a45b-c6f28aaf9520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12189
        }
      },
      "source": [
        "mds = []\n",
        "evs = []\n",
        "pds = []\n",
        "tf.logging.set_verbosity(tf.logging.FATAL) \n",
        "for i in range(1,15):\n",
        "  print('----------------------- Starting Epoch %d-----------------------'%i)\n",
        "\n",
        "  NUM_TRAIN_EPOCHS = i\n",
        "  \n",
        "  num_train_steps = int(len(train_InputExamples_all) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "#   model_fn = model_fn_builder(\n",
        "#   num_labels=len(label_list),\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps,\n",
        "#   use_tpu=True,\n",
        "#   bert_hub_module_handle=BERT_MODEL_HUB)\n",
        "\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "    num_labels=len(label_list),\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True,\n",
        "    bert_hub_module_handle=BERT_MODEL_HUB\n",
        "  )\n",
        "\n",
        "  estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=model_fn,\n",
        "    config=get_run_config(OUTPUT_DIR),\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE,\n",
        "  )\n",
        "\n",
        "\n",
        "  \n",
        "  md = model_train(estimator_from_tfhub)\n",
        "  \n",
        "  \n",
        "  print('-------- Document level Dev Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,dev_InputExamples_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('-------- Paragraph level Dev Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,dev_InputExamples_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(dev_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "  print('-------- Document level Fixed Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n",
        "  print('-------- Paragraph level Fixed Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_fixed_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_fixed_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "\n",
        "  print('-------- Document level Random Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_doc)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_doc['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  print('-------- Paragraph level Random Test Result---------')\n",
        "  predictions = model_predict(estimator_from_tfhub,test_InputExamples_par)\n",
        "  labels_val = []\n",
        "  for item in predictions:\n",
        "    labels_val.append(labels[np.argmax(item[1])])\n",
        "  true_label = list(test_par['sentiment'])\n",
        "  print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "  print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------- Starting Epoch 1-----------------------\n",
            "***** Started training at 2019-05-09 12:07:13.408352 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:11:33.467409 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  17   6]\n",
            " [  1  37  59]\n",
            " [  2  36 126]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.38      0.40        97\n",
            "    Positive       0.66      0.77      0.71       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.36      0.38      0.37       284\n",
            "weighted avg       0.52      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  92  53]\n",
            " [  1 317 437]\n",
            " [  4 244 732]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.17      0.01      0.01       146\n",
            "     Neutral       0.49      0.42      0.45       755\n",
            "    Positive       0.60      0.75      0.66       980\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1881\n",
            "   macro avg       0.42      0.39      0.38      1881\n",
            "weighted avg       0.52      0.56      0.53      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  24  25]\n",
            " [  2  47  67]\n",
            " [  0  49 124]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.02      0.04        50\n",
            "     Neutral       0.39      0.41      0.40       116\n",
            "    Positive       0.57      0.72      0.64       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.43      0.38      0.36       339\n",
            "weighted avg       0.48      0.51      0.47       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 109 128]\n",
            " [  0 259 468]\n",
            " [  0 279 567]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.00      0.01       238\n",
            "     Neutral       0.40      0.36      0.38       727\n",
            "    Positive       0.49      0.67      0.56       846\n",
            "\n",
            "   micro avg       0.46      0.46      0.46      1811\n",
            "   macro avg       0.63      0.34      0.32      1811\n",
            "weighted avg       0.52      0.46      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  0  14  14]\n",
            " [  2  33  52]\n",
            " [  0  42 128]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        28\n",
            "     Neutral       0.37      0.38      0.38        87\n",
            "    Positive       0.66      0.75      0.70       170\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       285\n",
            "   macro avg       0.34      0.38      0.36       285\n",
            "weighted avg       0.51      0.56      0.53       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  1 103  63]\n",
            " [  1 300 388]\n",
            " [  0 250 727]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.01      0.01       167\n",
            "     Neutral       0.46      0.44      0.45       689\n",
            "    Positive       0.62      0.74      0.67       977\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1833\n",
            "   macro avg       0.53      0.40      0.38      1833\n",
            "weighted avg       0.55      0.56      0.53      1833\n",
            "\n",
            "----------------------- Starting Epoch 2-----------------------\n",
            "***** Started training at 2019-05-09 12:16:39.097780 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:20:50.255185 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  48  49]\n",
            " [  3  49 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.49      0.45        97\n",
            "    Positive       0.68      0.68      0.68       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.36      0.39      0.38       284\n",
            "weighted avg       0.53      0.56      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  2  96  48]\n",
            " [  2 406 347]\n",
            " [  3 362 615]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.01      0.03       146\n",
            "     Neutral       0.47      0.54      0.50       755\n",
            "    Positive       0.61      0.63      0.62       980\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1881\n",
            "   macro avg       0.45      0.39      0.38      1881\n",
            "weighted avg       0.53      0.54      0.53      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  33  16]\n",
            " [  1  64  51]\n",
            " [  0  66 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.55      0.46       116\n",
            "    Positive       0.61      0.62      0.62       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 137 100]\n",
            " [  3 345 379]\n",
            " [  6 376 464]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.10      0.00      0.01       238\n",
            "     Neutral       0.40      0.47      0.44       727\n",
            "    Positive       0.49      0.55      0.52       846\n",
            "\n",
            "   micro avg       0.45      0.45      0.45      1811\n",
            "   macro avg       0.33      0.34      0.32      1811\n",
            "weighted avg       0.40      0.45      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  17  10]\n",
            " [  0  46  41]\n",
            " [  1  59 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.04      0.07        28\n",
            "     Neutral       0.38      0.53      0.44        87\n",
            "    Positive       0.68      0.65      0.66       170\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       285\n",
            "   macro avg       0.52      0.40      0.39       285\n",
            "weighted avg       0.57      0.55      0.54       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  2 106  59]\n",
            " [  5 387 297]\n",
            " [  1 345 631]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.02       167\n",
            "     Neutral       0.46      0.56      0.51       689\n",
            "    Positive       0.64      0.65      0.64       977\n",
            "\n",
            "   micro avg       0.56      0.56      0.56      1833\n",
            "   macro avg       0.45      0.41      0.39      1833\n",
            "weighted avg       0.54      0.56      0.54      1833\n",
            "\n",
            "----------------------- Starting Epoch 3-----------------------\n",
            "***** Started training at 2019-05-09 12:25:50.742597 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:30:03.691256 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  51  46]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.43      0.53      0.47        97\n",
            "    Positive       0.69      0.69      0.69       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.38      0.40      0.39       284\n",
            "weighted avg       0.55      0.58      0.56       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  99  46]\n",
            " [  1 411 343]\n",
            " [  3 387 590]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.01      0.01       146\n",
            "     Neutral       0.46      0.54      0.50       755\n",
            "    Positive       0.60      0.60      0.60       980\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1881\n",
            "   macro avg       0.42      0.38      0.37      1881\n",
            "weighted avg       0.51      0.53      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  35  14]\n",
            " [  2  59  55]\n",
            " [  0  57 116]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.02      0.04        50\n",
            "     Neutral       0.39      0.51      0.44       116\n",
            "    Positive       0.63      0.67      0.65       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.45      0.40      0.38       339\n",
            "weighted avg       0.50      0.52      0.49       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  1 135 102]\n",
            " [  2 345 380]\n",
            " [  5 391 450]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.12      0.00      0.01       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.33      0.34      0.32      1811\n",
            "weighted avg       0.40      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  17  10]\n",
            " [  2  43  42]\n",
            " [  0  62 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.68      0.64      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.45      0.39      0.38       285\n",
            "weighted avg       0.54      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3 102  62]\n",
            " [  4 393 292]\n",
            " [  3 358 616]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.30      0.02      0.03       167\n",
            "     Neutral       0.46      0.57      0.51       689\n",
            "    Positive       0.64      0.63      0.63       977\n",
            "\n",
            "   micro avg       0.55      0.55      0.55      1833\n",
            "   macro avg       0.47      0.41      0.39      1833\n",
            "weighted avg       0.54      0.55      0.53      1833\n",
            "\n",
            "----------------------- Starting Epoch 4-----------------------\n",
            "***** Started training at 2019-05-09 12:35:13.854729 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:39:34.033193 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  47  50]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.41      0.48      0.45        97\n",
            "    Positive       0.68      0.69      0.68       164\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       284\n",
            "   macro avg       0.36      0.39      0.38       284\n",
            "weighted avg       0.53      0.56      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1 100  45]\n",
            " [  2 412 341]\n",
            " [  1 408 571]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.01       146\n",
            "     Neutral       0.45      0.55      0.49       755\n",
            "    Positive       0.60      0.58      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.36      1881\n",
            "weighted avg       0.51      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  32  17]\n",
            " [  1  60  55]\n",
            " [  0  61 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.52      0.45       116\n",
            "    Positive       0.61      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.39      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  0 130 108]\n",
            " [  2 341 384]\n",
            " [  5 392 449]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.50       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.29      0.33      0.31      1811\n",
            "weighted avg       0.38      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  0  45  42]\n",
            " [  0  65 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.04      0.07        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.67      0.62      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.67      0.39      0.38       285\n",
            "weighted avg       0.61      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3  94  70]\n",
            " [  5 384 300]\n",
            " [  3 376 598]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.27      0.02      0.03       167\n",
            "     Neutral       0.45      0.56      0.50       689\n",
            "    Positive       0.62      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1833\n",
            "   macro avg       0.45      0.40      0.38      1833\n",
            "weighted avg       0.52      0.54      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 5-----------------------\n",
            "***** Started training at 2019-05-09 12:44:37.466301 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:49:11.558773 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  48  49]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.42      0.49      0.45        97\n",
            "    Positive       0.68      0.69      0.68       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.37      0.39      0.38       284\n",
            "weighted avg       0.54      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  1  99  46]\n",
            " [  4 407 344]\n",
            " [  2 399 579]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.14      0.01      0.01       146\n",
            "     Neutral       0.45      0.54      0.49       755\n",
            "    Positive       0.60      0.59      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.40      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  33  16]\n",
            " [  1  59  56]\n",
            " [  0  60 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.39      0.51      0.44       116\n",
            "    Positive       0.61      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.39      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  2 127 109]\n",
            " [  3 344 380]\n",
            " [  6 383 457]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.18      0.01      0.02       238\n",
            "     Neutral       0.40      0.47      0.44       727\n",
            "    Positive       0.48      0.54      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.36      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.42      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  1  43  43]\n",
            " [  1  64 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.34      0.49      0.41        87\n",
            "    Positive       0.67      0.62      0.64       170\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       285\n",
            "   macro avg       0.45      0.38      0.37       285\n",
            "weighted avg       0.54      0.52      0.51       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  3  91  73]\n",
            " [  8 378 303]\n",
            " [  4 374 599]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.20      0.02      0.03       167\n",
            "     Neutral       0.45      0.55      0.49       689\n",
            "    Positive       0.61      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.42      0.39      0.38      1833\n",
            "weighted avg       0.51      0.53      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 6-----------------------\n",
            "***** Started training at 2019-05-09 12:54:14.134432 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 12:58:35.277134 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  48 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.42      0.51      0.46        97\n",
            "    Positive       0.68      0.69      0.69       164\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       284\n",
            "   macro avg       0.37      0.40      0.38       284\n",
            "weighted avg       0.54      0.57      0.55       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  2  93  51]\n",
            " [  5 389 361]\n",
            " [  4 391 585]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.18      0.01      0.03       146\n",
            "     Neutral       0.45      0.52      0.48       755\n",
            "    Positive       0.59      0.60      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.40      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.50      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  30  19]\n",
            " [  1  60  55]\n",
            " [  0  60 113]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.52      0.45       116\n",
            "    Positive       0.60      0.65      0.63       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 119 116]\n",
            " [  4 328 395]\n",
            " [  6 377 463]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.23      0.01      0.02       238\n",
            "     Neutral       0.40      0.45      0.42       727\n",
            "    Positive       0.48      0.55      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.37      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  0  45  42]\n",
            " [  1  65 104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.04      0.07        28\n",
            "     Neutral       0.35      0.52      0.42        87\n",
            "    Positive       0.67      0.61      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.51      0.39      0.38       285\n",
            "weighted avg       0.56      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  4  89  74]\n",
            " [  8 369 312]\n",
            " [  4 361 612]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.02      0.04       167\n",
            "     Neutral       0.45      0.54      0.49       689\n",
            "    Positive       0.61      0.63      0.62       977\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      1833\n",
            "   macro avg       0.44      0.40      0.38      1833\n",
            "weighted avg       0.52      0.54      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 7-----------------------\n",
            "***** Started training at 2019-05-09 13:03:44.372830 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 13:08:37.924851 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  46 115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.43      0.51      0.46        97\n",
            "    Positive       0.69      0.70      0.69       164\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       284\n",
            "   macro avg       0.37      0.40      0.39       284\n",
            "weighted avg       0.54      0.58      0.56       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  5  91  50]\n",
            " [  9 389 357]\n",
            " [  6 385 589]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.03      0.06       146\n",
            "     Neutral       0.45      0.52      0.48       755\n",
            "    Positive       0.59      0.60      0.60       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.38      1881\n",
            "weighted avg       0.51      0.52      0.51      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  30  19]\n",
            " [  1  60  55]\n",
            " [  0  59 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.52      0.45       116\n",
            "    Positive       0.61      0.66      0.63       173\n",
            "\n",
            "   micro avg       0.52      0.52      0.52       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.52      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 124 111]\n",
            " [  5 337 385]\n",
            " [ 11 375 460]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.16      0.01      0.02       238\n",
            "     Neutral       0.40      0.46      0.43       727\n",
            "    Positive       0.48      0.54      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.35      0.34      0.32      1811\n",
            "weighted avg       0.41      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  18   9]\n",
            " [  2  43  42]\n",
            " [  1  62 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.06        28\n",
            "     Neutral       0.35      0.49      0.41        87\n",
            "    Positive       0.68      0.63      0.65       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.43      0.39      0.37       285\n",
            "weighted avg       0.54      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  7  85  75]\n",
            " [ 12 363 314]\n",
            " [  5 365 607]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.29      0.04      0.07       167\n",
            "     Neutral       0.45      0.53      0.48       689\n",
            "    Positive       0.61      0.62      0.62       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.45      0.40      0.39      1833\n",
            "weighted avg       0.52      0.53      0.52      1833\n",
            "\n",
            "----------------------- Starting Epoch 8-----------------------\n",
            "***** Started training at 2019-05-09 13:13:48.450428 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-05-09 13:20:16.284210 *****\n",
            "-------- Document level Dev Result---------\n",
            "[[  0  19   4]\n",
            " [  0  49  48]\n",
            " [  3  53 108]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00        23\n",
            "     Neutral       0.40      0.51      0.45        97\n",
            "    Positive       0.68      0.66      0.67       164\n",
            "\n",
            "   micro avg       0.55      0.55      0.55       284\n",
            "   macro avg       0.36      0.39      0.37       284\n",
            "weighted avg       0.53      0.55      0.54       284\n",
            "\n",
            "-------- Paragraph level Dev Result---------\n",
            "[[  4  92  50]\n",
            " [  6 393 356]\n",
            " [  5 400 575]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.27      0.03      0.05       146\n",
            "     Neutral       0.44      0.52      0.48       755\n",
            "    Positive       0.59      0.59      0.59       980\n",
            "\n",
            "   micro avg       0.52      0.52      0.52      1881\n",
            "   macro avg       0.43      0.38      0.37      1881\n",
            "weighted avg       0.50      0.52      0.50      1881\n",
            "\n",
            "-------- Document level Fixed Test Result---------\n",
            "[[  1  31  18]\n",
            " [  1  63  52]\n",
            " [  0  63 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      0.02      0.04        50\n",
            "     Neutral       0.40      0.54      0.46       116\n",
            "    Positive       0.61      0.64      0.62       173\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       339\n",
            "   macro avg       0.50      0.40      0.37       339\n",
            "weighted avg       0.52      0.51      0.48       339\n",
            "\n",
            "-------- Paragraph level Fixed Test Result---------\n",
            "[[  3 126 109]\n",
            " [  5 343 379]\n",
            " [  4 390 452]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.01      0.02       238\n",
            "     Neutral       0.40      0.47      0.43       727\n",
            "    Positive       0.48      0.53      0.51       846\n",
            "\n",
            "   micro avg       0.44      0.44      0.44      1811\n",
            "   macro avg       0.38      0.34      0.32      1811\n",
            "weighted avg       0.42      0.44      0.41      1811\n",
            "\n",
            "-------- Document level Random Test Result---------\n",
            "[[  1  19   8]\n",
            " [  1  46  40]\n",
            " [  1  66 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.33      0.04      0.06        28\n",
            "     Neutral       0.35      0.53      0.42        87\n",
            "    Positive       0.68      0.61      0.64       170\n",
            "\n",
            "   micro avg       0.53      0.53      0.53       285\n",
            "   macro avg       0.46      0.39      0.38       285\n",
            "weighted avg       0.55      0.53      0.52       285\n",
            "\n",
            "-------- Paragraph level Random Test Result---------\n",
            "[[  6  89  72]\n",
            " [ 12 366 311]\n",
            " [  6 375 596]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.25      0.04      0.06       167\n",
            "     Neutral       0.44      0.53      0.48       689\n",
            "    Positive       0.61      0.61      0.61       977\n",
            "\n",
            "   micro avg       0.53      0.53      0.53      1833\n",
            "   macro avg       0.43      0.39      0.38      1833\n",
            "weighted avg       0.51      0.53      0.51      1833\n",
            "\n",
            "----------------------- Starting Epoch 9-----------------------\n",
            "***** Started training at 2019-05-09 13:25:28.332869 *****\n",
            "  Num examples = 19178\n",
            "  Batch size = 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBRPuijQenOx",
        "colab_type": "code",
        "outputId": "a9c3ddc7-c7c3-4810-893c-f256de98746c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGQUnOsgfA9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}