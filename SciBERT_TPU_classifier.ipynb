{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciBERT_TPU_classifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/SciBERT_TPU_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_",
        "colab_type": "text"
      },
      "source": [
        "# Fine Tuning SciBERT on BioMed dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPgpRl5g2e2",
        "colab_type": "text"
      },
      "source": [
        "In this experiment, we will be pre-training a state-of-the-art Natural Language Understanding model [BERT](https://arxiv.org/abs/1810.04805.) on arbitrary text data using Google Cloud infrastructure.\n",
        "\n",
        "This guide covers all stages of the procedure, including:\n",
        "\n",
        "1. Setting up the training environment\n",
        "2. Downloading raw text data\n",
        "3. Preprocessing text data\n",
        "4. Learning a new vocabulary\n",
        "5. Creating sharded pre-training data\n",
        "6. Setting up GCS storage for data and model\n",
        "7. Training the model on a cloud TPU\n",
        "\n",
        "For persistent storage of training data and model, you will require a Google Cloud Storage bucket. \n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) to create a GCP account and GCS bucket. New Google Cloud users have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. \n",
        "\n",
        "Steps 1-5 of this tutorial can be run without a GCS bucket for demonstration purposes. In that case, however, you will not be able to train the model.\n",
        "\n",
        "**Note** \n",
        "The only parameter you *really have to set* is BUCKET_NAME in steps 5 and 6. Everything else has default values which should work for most use-cases.\n",
        "\n",
        "**Note** \n",
        "Pre-training a BERT-Base model on a TPUv2 will take about 54 hours. Google Colab is not designed for executing such long-running jobs and will interrupt the training process every 8 hours or so. For uninterrupted training, consider using a preemptible TPUv2 instance. \n",
        "\n",
        "That said, at the time of writing (09.05.2019), with a Colab TPU, pre-training a BERT model from scratch can be achieved at a negligible cost of storing the said model and data in GCS  (~1 USD).\n",
        "\n",
        "Now, let's get to business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODimOhBR05yR",
        "colab_type": "text"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) [2019] [Antyukhov Denis Olegovich]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjad5jsr9YaM",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: setting up training environment\n",
        "First and foremost, we get the packages required to train the model. \n",
        "The Jupyter environment allows executing bash commands directly from the notebook by using an exclamation mark ‘!’. I will be exploiting this approach to make use of several other bash commands throughout the experiment.\n",
        "\n",
        "Now, let’s import the packages and authorize ourselves in Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab_type": "code",
        "outputId": "12a6354d-73e0-459b-ce5c-81b1c5e987ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "# !git clone https://github.com/google-research/bert\n",
        "!git clone https://github.com/allenai/scibert.git\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "!pip install bert_tensorflow\n",
        "\n",
        "from bert import modeling, optimization, tokenization, run_classifier\n",
        "# from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Cloning into 'scibert'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 1553 (delta 31), reused 27 (delta 14), pack-reused 1503\u001b[K\n",
            "Receiving objects: 100% (1553/1553), 53.08 MiB | 21.10 MiB/s, done.\n",
            "Resolving deltas: 100% (779/779), done.\n",
            "Collecting bert_tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert_tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 23:10:11,557 :  Using TPU runtime\n",
            "2020-01-22 23:10:11,567 :  TPU address is grpc://10.106.209.170:8470\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVXMoC-aMy1",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: getting the data\n",
        "\n",
        "We begin with obtaining a corpus of raw text data. For this experiment, we will be using the [OpenSubtitles](http://www.opensubtitles.org/) dataset, which is available for 65 languages [here](http://opus.nlpl.eu/OpenSubtitles-v2016.php). \n",
        "\n",
        "Unlike more common text datasets (like Wikipedia) it does not require any complex pre-processing. It also comes pre-formatted with one sentence per line.\n",
        "\n",
        "Feel free to use the dataset for your language instead by changing the language code (en) below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotFkkshbdvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_pref = 'gs://ie_scibert/bert_model_classifier/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5TPsPOppn0",
        "colab_type": "text"
      },
      "source": [
        "For demonstration purposes, we will only use a small fraction of the whole corpus for this experiment. \n",
        "\n",
        "When training the real model, make sure to uncheck the DEMO_MODE checkbox to use a 100x larger dataset.\n",
        "\n",
        "Rest assured, 100M lines are perfectly sufficient to train a reasonably good BERT-base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDR5Z1MDgB1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEMO_MODE = True #@param {type:\"boolean\"}\n",
        "\n",
        "# if DEMO_MODE:\n",
        "#   CORPUS_SIZE = 1000000\n",
        "# else:\n",
        "#   CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
        "  \n",
        "# !(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
        "# !mv subdataset.txt dataset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRQd4-v0nQqH",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: preprocessing text\n",
        "\n",
        "The raw text data we have downloaded contains punсtuation, uppercase letters and non-UTF symbols which we will remove before proceeding. During inference, we will apply the same normalization procedure to new data.\n",
        "\n",
        "If your use-case requires different preprocessing (e.g. if uppercase letters or punctuation are expected during inference), feel free to modify the function below to accomodate for your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCi2oSdInRkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "\n",
        "# def normalize_text(text):\n",
        "# #   # lowercase text\n",
        "#    text = str(text).lower()\n",
        "# #   # remove non-UTF\n",
        "#    text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "# #   # remove punktuation symbols\n",
        "#    text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "#    return text\n",
        "\n",
        "# def count_lines(filename):\n",
        "#    count = 0\n",
        "#    with open(filename) as fi:\n",
        "#      for line in fi:\n",
        "#        count += 1\n",
        "#    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtwtDnesaQQ",
        "colab_type": "text"
      },
      "source": [
        "Check how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gngtEZWqVhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize_text('Thanks to the advance, they have succeeded in getting over their adversaries.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-Kvnx6sUFS",
        "colab_type": "text"
      },
      "source": [
        "Apply normalization to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjxQe5awo1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  data_pref = 'gs://ie_scibert/bert_model_classifier/'\n",
        "#  RAW_DATA_FPATH = \"train.csv\" #@param {type: \"string\"}\n",
        "#  PRC_DATA_FPATH = \"proc_train.csv\" #@param {type: \"string\"}\n",
        "\n",
        "# # # apply normalization to the dataset\n",
        "# # # this will take a minute or two\n",
        "# import tensorflow as tf\n",
        " \n",
        "#  fi = tf.gfile.GFile(data_pref +str(RAW_DATA_FPATH),mode='r')\n",
        "#  lines =  fi.readlines()\n",
        "#  total_lines = len(lines)\n",
        "#  bar = Progbar(total_lines)\n",
        "#  # #  with open(fraw,encoding = 'utf-8') as ff:\n",
        "#  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "#      for l in lines:\n",
        "#        fo.write(normalize_text(l)+\"\\n\")\n",
        "#        bar.add(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3A64RZjwo9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f = open(PRC_DATA_FPATH,encoding=\"utf-8\")\n",
        "# f.readlines()[1].split('\\t')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVO9EnUwrluQ",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: building the vocabulary\n",
        "\n",
        "For the next step, we will learn a new vocabulary that we will use to represent our dataset. \n",
        "\n",
        "The BERT paper uses a WordPiece tokenizer, which is not available in opensource. Instead, we will be using SentencePiece tokenizer in unigram mode. While it is not directly compatible with BERT, with a small hack we can make it work.\n",
        "\n",
        "SentencePiece requires quite a lot of RAM, so running it on the full dataset in Colab will crash the kernel. To avoid this, we will randomly subsample a fraction of the dataset for building the vocabulary. Another option would be to use a machine with more RAM for this step - that decision is up to you.\n",
        "\n",
        "Also, SentencePiece adds BOS and EOS control symbols to the vocabulary by default. We disable them explicitly by setting their indices to -1.\n",
        "\n",
        "The typical values for VOC_SIZE are somewhere in between 32000 and 128000. We reserve NUM_PLACEHOLDERS tokens in case one wants to update the vocabulary and fine-tune the model after the pre-training phase is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
        "# ## 32000\n",
        "VOC_SIZE = 32000 #@param {type:\"integer\"} \n",
        "# ## 128000\n",
        "SUBSAMPLE_SIZE = 128000 #@param {type:\"integer\"}\n",
        "# ## 256\n",
        "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
        "\n",
        "# SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "#                '--vocab_size={} --input_sentence_size={} '\n",
        "#                '--shuffle_input_sentence=true ' \n",
        "#                '--bos_id=-1 --eos_id=-1').format(\n",
        "#                PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "#                VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "# spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAowCoH2u1iZ",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how we can make SentencePiece tokenizer work for the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLaw7kPW3he",
        "colab_type": "text"
      },
      "source": [
        "Below is a sentence tokenized using the WordPiece vocabulary from a pretrained English [BERT-base](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) model from the official [repo](https://github.com/google-research/bert). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwHAp_Gh5OPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testcase = \"Colorless geothermal substations are generating furiously\"\n",
        "# wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEGAVl_5YRD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")\n",
        "\n",
        "['color',\n",
        " '##less',\n",
        " 'geo',\n",
        " '##thermal',\n",
        " 'sub',\n",
        " '##station',\n",
        " '##s',\n",
        " 'are',\n",
        " 'generating',\n",
        " 'furiously']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNiLdWXTh9cj",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the WordPiece tokenizer prepends the subwords which occur in the middle of words with '##'. The subwords occurring at the beginning of words are unchanged. If the subword occurs both in the beginning and in the middle of words, both versions (with and without '##') are added to the vocabulary.\n",
        "\n",
        "Now let's have a look at the vocabulary that the SentencePiece tokenizer has learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8_ebLxqTnWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlBqiv5UD-j",
        "colab_type": "text"
      },
      "source": [
        "SentencePiece has created two files: tokenizer.model and tokenizer.vocab. Let's have a look at the learned vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJ9QmNMUEQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !head -n 100 tokenizer.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsURk_h5jw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def read_sentencepiece_vocab(filepath):\n",
        "#   voc = []\n",
        "#   with open(filepath, encoding='utf-8') as fi:\n",
        "#     for line in fi:\n",
        "#       voc.append(line.split(\"\\t\")[0])\n",
        "#   # skip the first <unk> token\n",
        "#   voc = voc[1:]\n",
        "#   return voc\n",
        "\n",
        "# snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "# print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "# print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YjqHVRpmlKq",
        "colab_type": "text"
      },
      "source": [
        "As we may observe, SentencePiece does quite the opposite to WordPiece. From the [documentation](https://github.com/google/sentencepiece/blob/master/README.md):\n",
        "\n",
        "\n",
        "SentencePiece first escapes the whitespace with a meta-symbol \"▁\" (U+2581) as follows:\n",
        "\n",
        "`Hello▁World`.\n",
        "\n",
        "Then, this text is segmented into small pieces, for example:\n",
        "\n",
        "`[Hello] [▁Wor] [ld] [.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-28l_0p0PQ",
        "colab_type": "text"
      },
      "source": [
        "Subwords which occur after whitespace (which are also those that most words begin with) are prepended with '▁', while others are unchanged. This excludes subwords which only occur at the beginning of sentences and nowhere else. These cases should be quite rare, however. \n",
        "\n",
        "So, in order to obtain a vocabulary analogous to WordPiece, we need to perform a simple conversion, removing \"▁\" from the tokens that contain it and adding \"##\"  to the ones that don't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJGFjzOMbfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def parse_sentencepiece_token(token):\n",
        "#     if token.startswith(\"▁\"):\n",
        "#         return token[1:]\n",
        "#     else:\n",
        "#         return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dcVgD98S28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9ZR3RN9IMA",
        "colab_type": "text"
      },
      "source": [
        "We also add some special control symbols which are required by the BERT architecture. By convention, we put those at the beginning of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdTlXDPL8cHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "# bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1jvEr9EIdd",
        "colab_type": "text"
      },
      "source": [
        "We also append some placeholder tokens to the vocabulary. Those are useful if one wishes to update the pre-trained model with new, task-specific tokens. \n",
        "\n",
        "In that case, the placeholder tokens are replaced with new real ones, the pre-training data is re-generated, and the model is fine-tuned on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLYSTil4E0Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "# print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKk_7JI-MtW",
        "colab_type": "text"
      },
      "source": [
        "Finally, we write the obtained vocabulary to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1jg0cj9Duf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# with open(VOC_FNAME, \"w\") as fo:\n",
        "#   for token in bert_vocab:\n",
        "#     fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqXZnc3FCuY",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how the new vocabulary works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsSOnEnC-jG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "# # bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauD8ndhEA-z",
        "colab_type": "text"
      },
      "source": [
        "Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFtStCo__QX",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: generating pre-training data\n",
        "\n",
        "With the vocabulary at hand, we are ready to generate pre-training data for the BERT model. Since our dataset might be quite large, we will split it into shards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "colab_type": "code",
        "outputId": "079e9c09-8bc9-4637-c5b4-f943ed8f5781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "split: cannot open './shards/shard_' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FSq3zNFvLs",
        "colab_type": "text"
      },
      "source": [
        "Before we start generating, we need to set some model-specific parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MibOBkFam2",
        "colab_type": "text"
      },
      "source": [
        "Now, for each shard we need to call *create_pretraining_data.py* script. To that end, we will employ the  *xargs* command. \n",
        "\n",
        "Running this might take quite some time depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZjIeVP0T36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XARGS_CMD = (\"ls ./shards/ | \"\n",
        "#              \"xargs -n 1 -P {} -I{} \"\n",
        "#              \"python3 bert/create_pretraining_data.py \"\n",
        "#              \"--input_file=./shards/{} \"\n",
        "#              \"--output_file={}/{}.tfrecord \"\n",
        "#              \"--vocab_file={} \"\n",
        "#              \"--do_lower_case={} \"\n",
        "#              \"--max_predictions_per_seq={} \"\n",
        "#              \"--max_seq_length={} \"\n",
        "#              \"--masked_lm_prob={} \"\n",
        "#              \"--random_seed=34 \"\n",
        "#              \"--dupe_factor=5\")\n",
        "\n",
        "# XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "#                              VOC_FNAME, DO_LOWER_CASE, \n",
        "#                              MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyo9_LcQ0pla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5XzaY8xCdiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "# !$XARGS_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnW_hVxPoP6",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: setting up persistent storage\n",
        "\n",
        "To preserve our hard-earned assets, we will persist them to Google Cloud Storage. Provided that you have created the GCS bucket, this should be simple.\n",
        "\n",
        "We will create two directories in GCS, one for the data and one for the model.\n",
        "In the model directory, we will put the model vocabulary and configuration file.\n",
        "\n",
        "**Configure your BUCKET_NAME variable here before proceeding, otherwise the model and data will not be saved.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtrt68QQIHs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"ie_scibert\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"scibert_model_classifier\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigCcV-hSHVH",
        "colab_type": "text"
      },
      "source": [
        "Below is the sample hyperparameter configuration for BERT-base. Change at your own risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # use this for BERT-base\n",
        "\n",
        "# bert_base_config = {\n",
        "#   \"attention_probs_dropout_prob\": 0.1, \n",
        "#   \"directionality\": \"bidi\", \n",
        "#   \"hidden_act\": \"gelu\", \n",
        "#   \"hidden_dropout_prob\": 0.1, \n",
        "#   \"hidden_size\": 768, \n",
        "#   \"initializer_range\": 0.02, \n",
        "#   \"intermediate_size\": 3072, \n",
        "#   \"max_position_embeddings\": 512, \n",
        "#   \"num_attention_heads\": 12, \n",
        "#   \"num_hidden_layers\": 12, \n",
        "#   \"pooler_fc_size\": 768, \n",
        "#   \"pooler_num_attention_heads\": 12, \n",
        "#   \"pooler_num_fc_layers\": 3, \n",
        "#   \"pooler_size_per_head\": 128, \n",
        "#   \"pooler_type\": \"first_token_transform\", \n",
        "#   \"type_vocab_size\": 2, \n",
        "#   \"vocab_size\": VOC_SIZE\n",
        "# }\n",
        "\n",
        "# with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "#   json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "# with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "#   for token in bert_vocab:\n",
        "#     fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgrEDugRG48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if BUCKET_NAME:\n",
        "#   !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DL6xuCAYPrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: training the model\n",
        "\n",
        "We are almost ready to begin training our model. If you wish  to continue an interrupted training run, you may skip steps 2-6 and proceed from here.\n",
        "\n",
        "**Make sure that you have set the BUCKET_NAME here as well.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "dfeb154b-cbf1-44b6-bb56-cce23aed3b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "BUCKET_NAME = \"ie_scibert\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"sciBERT_Model/scibert_scivocab_uncased\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 200000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "print(BERT_GCS_DIR)\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "# INIT_CHECKPOINT = tf.saved_model.load_v2(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))\n",
        "log.info(\"Using {} vocab file\".format(VOCAB_FILE))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://ie_scibert/sciBERT_Model/scibert_scivocab_uncased\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 23:10:43,564 :  From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "2020-01-22 23:10:43,853 :  Using checkpoint: gs://ie_scibert/sciBERT_Model/scibert_scivocab_uncased/model.ckpt-140000\n",
            "2020-01-22 23:10:43,854 :  Using 2 data shards\n",
            "2020-01-22 23:10:43,856 :  Using gs://ie_scibert/sciBERT_Model/scibert_scivocab_uncased/vocab.txt vocab file\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwF-WqcZHUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTFjITdd53F",
        "colab_type": "text"
      },
      "source": [
        "Prepare the training run configuration, build the estimator and input function, power up the bass cannon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn = model_fn_builder(\n",
        "#       bert_config=bert_config,\n",
        "#       init_checkpoint=INIT_CHECKPOINT,\n",
        "#       learning_rate=LEARNING_RATE,\n",
        "#       num_train_steps=TRAIN_STEPS,\n",
        "#       num_warmup_steps=10,\n",
        "#       use_tpu=USE_TPU,\n",
        "#       use_one_hot_embeddings=True)\n",
        "\n",
        "# tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "# run_config = tf.contrib.tpu.RunConfig(\n",
        "#     cluster=tpu_cluster_resolver,\n",
        "#     model_dir=BERT_GCS_DIR,\n",
        "#     save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "#     tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "#         iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "#         num_shards=NUM_TPU_CORES,\n",
        "#         per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#     use_tpu=USE_TPU,\n",
        "#     model_fn=model_fn,\n",
        "#     config=run_config,\n",
        "#     train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#     eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "# train_input_fn = input_fn_builder(\n",
        "#         input_files=input_files,\n",
        "#         max_seq_length=MAX_SEQ_LENGTH,\n",
        "#         max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "#         is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB",
        "colab_type": "text"
      },
      "source": [
        "Fire!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OeXod-fHMT",
        "colab_type": "text"
      },
      "source": [
        "Training the model with the default parameters for 1 million steps will take ~53 hours. \n",
        "\n",
        "In case the kernel is restarted, you may always continue training from the latest checkpoint. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZIAAATfzdF",
        "colab_type": "text"
      },
      "source": [
        "This concludes the guide to pre-training BERT from scratch on a cloud TPU. However, the really fun stuff is still  to come, so stay tuned.\n",
        "\n",
        "Keep learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BrQ2dOkWo4p",
        "colab_type": "text"
      },
      "source": [
        "# Run classifier for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7_2pKWzfiZ",
        "colab_type": "code",
        "outputId": "65770557-685c-42cb-c19e-37a1f7a2cf77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "f = tf.gfile.GFile(data_pref + str('train.csv'))\n",
        "all_train = pd.read_csv(f, encoding='latin-1')\n",
        "all_train = all_train.dropna()\n",
        "train = all_train[500000:1000000]\n",
        "\n",
        "dev = pd.read_csv(tf.gfile.GFile(data_pref + str('dev.csv')), encoding='latin-1')\n",
        "dev = dev.dropna()\n",
        "\n",
        "\n",
        "test = pd.read_csv(tf.gfile.GFile(data_pref + str('test.csv')), encoding='latin-1')\n",
        "test = test.dropna()\n",
        "\n",
        "print('---------- dataset statistics: ---------')\n",
        "print('training set length : ')\n",
        "print(len(all_train))\n",
        "\n",
        "print('dev set length : ')\n",
        "print(len(dev))\n",
        "\n",
        "print('test set length : ')\n",
        "print(len(test))\n",
        "\n",
        "print('number of unique element types ( controllee types) in train set : ')\n",
        "print(len(set(all_train['element_type'])))\n",
        "\n",
        "print('number of unique reg types ( controller types) in train set : ')\n",
        "print(len(set(all_train['reg_type'])))\n",
        "\n",
        "print('number of unique element types ( controllee types) in dev set : ')\n",
        "print(len(set(dev['element_type'])))\n",
        "\n",
        "print('number of unique reg types ( controller types) in dev set : ')\n",
        "print(len(set(dev['reg_type'])))\n",
        "\n",
        "print('number of unique element types ( controllee types) in test set : ')\n",
        "print(len(set(test['element_type'])))\n",
        "\n",
        "print('number of unique reg types ( controller types) in test set : ')\n",
        "print(len(set(test['reg_type'])))\n",
        "\n",
        "\n",
        "print('number of unique element types ( controller types) for the whole dataset : ')\n",
        "a = list(all_train['reg_type']) + list(dev['reg_type']) + list(test['reg_type'])\n",
        "print(len(a))\n",
        "\n",
        "print('number of unique element types ( controllee types) for the whole dataset : ')\n",
        "a = list(all_train['element_type']) + list(dev['element_type']) + list(test['element_type'])\n",
        "print(len(a))\n",
        "\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- dataset statistics: ---------\n",
            "training set length : \n",
            "3129209\n",
            "dev set length : \n",
            "347689\n",
            "test set length : \n",
            "386320\n",
            "number of unique element types ( controllee types) in train set : \n",
            "96\n",
            "number of unique element types ( controller types) in train set : \n",
            "328\n",
            "number of unique element types ( controllee types) in dev set : \n",
            "52\n",
            "number of unique element types ( controller types) in dev set : \n",
            "151\n",
            "number of unique element types ( controllee types) in test set : \n",
            "56\n",
            "number of unique element types ( controller types) in test set : \n",
            "151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWf8XXpZ5Tep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = [0,1]\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'reg_label'\n",
        "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whHU9DMhE9ZY",
        "colab_type": "code",
        "outputId": "6a9d0b80-dfb2-46b7-e632-3f6e8ef28b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pickle\n",
        "from bert import tokenization \n",
        "DO_LOWER_CASE = True\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
        "\n",
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "# f = tf.gfile.GFile(data_pref + str('train_features.out'), mode='rb')#file_io.FileIO(data_pref + str('train_features.out'),mode='r')\n",
        "# train_features = pickle.load(f,encoding='utf-8')\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 23:11:33,242 :  From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "2020-01-22 23:11:33,243 :  Writing example 0 of 499662\n",
            "2020-01-22 23:11:33,245 :  *** Example ***\n",
            "2020-01-22 23:11:33,246 :  guid: None\n",
            "2020-01-22 23:11:33,246 :  tokens: [CLS] free fab / ( fab ) 2 likely originate from the fragmentation of whole abs driven by the catalytic activity of gr ##p ##94 itself [ xr ##ef _ bib ##r , xr ##ef _ bib ##r ] that is similar to that displayed by serine proteases on the igg molecule [ xr ##ef _ bib ##r , xr ##ef _ bib ##r ] . [SEP]\n",
            "2020-01-22 23:11:33,247 :  input_ids: 102 2159 4987 1352 145 4987 546 170 1987 18503 263 111 11961 131 2868 1430 6920 214 111 8741 1071 131 544 30121 3609 3987 260 17106 5730 4627 13652 30114 422 17106 5730 4627 13652 30114 1901 198 165 868 147 198 5983 214 13966 19031 191 111 6902 5745 260 17106 5730 4627 13652 30114 422 17106 5730 4627 13652 30114 1901 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,248 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,248 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,249 :  label: 1 (id = 1)\n",
            "2020-01-22 23:11:33,251 :  *** Example ***\n",
            "2020-01-22 23:11:33,251 :  guid: None\n",
            "2020-01-22 23:11:33,252 :  tokens: [CLS] the hfd induced rise in tg levels is accompanied by increased signalling through the target of rapamycin ( tor ) pathway , decreased expression of adipose triglyceride lipase ( atg ##l , encoded by the bm ##m gene in flies ) and a corresponding increase in fatty acid synthase ( fas ) expression . [SEP]\n",
            "2020-01-22 23:11:33,253 :  input_ids: 102 111 25273 2651 5292 121 7720 1049 165 8861 214 1175 10636 833 111 1549 131 20881 145 3928 546 3430 422 2664 940 131 12342 20841 21652 145 17547 30115 422 9195 214 111 3295 30119 983 121 15940 546 137 106 1737 1242 121 6462 1397 12242 145 9237 546 940 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,253 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,254 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,254 :  label: 0 (id = 0)\n",
            "2020-01-22 23:11:33,256 :  *** Example ***\n",
            "2020-01-22 23:11:33,256 :  guid: None\n",
            "2020-01-22 23:11:33,257 :  tokens: [CLS] secondary , or post - traumatic , injury is caused by a complex set of cellular and biochemical cascades that worse ##n the primary injury , accounting for the greatest number of tbi deaths occurring in hospitals [ xr ##ef _ bib ##r ] . [SEP]\n",
            "2020-01-22 23:11:33,258 :  input_ids: 102 3287 422 234 1422 579 14220 422 3771 165 3111 214 106 1127 610 131 3431 137 7235 23959 198 8946 30111 111 1916 3771 422 9314 168 111 9145 649 131 18412 9267 6734 121 7775 260 17106 5730 4627 13652 30114 1901 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,258 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,259 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,260 :  label: 1 (id = 1)\n",
            "2020-01-22 23:11:33,261 :  *** Example ***\n",
            "2020-01-22 23:11:33,262 :  guid: None\n",
            "2020-01-22 23:11:33,263 :  tokens: [CLS] xr ##ef _ bib ##r we previously reported that mil ##nac ##ipr ##an , another snr ##i , increased the plasma levels of 3 - meth ##oxy - 4 - hydroxy ##phenyl ##gly ##col ( mh ##pg ) ( a major metabolite of nor ##adrenal ##ine ) in patients with mdd ; this increase was related to the mil ##nac ##ipr ##an associated clinical improvement in patients with mdd . [SEP]\n",
            "2020-01-22 23:11:33,263 :  input_ids: 102 17106 5730 4627 13652 30114 185 2049 1214 198 3466 20830 23562 133 422 1783 8535 30109 422 1175 111 2780 1049 131 239 579 513 6312 579 286 579 15513 12351 6601 3439 145 10483 12466 546 145 106 1626 13681 131 2247 23915 276 546 121 568 190 25274 1814 238 1242 241 1482 147 111 3466 20830 23562 133 1111 1329 3523 121 568 190 25274 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,264 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,265 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,265 :  label: 1 (id = 1)\n",
            "2020-01-22 23:11:33,266 :  *** Example ***\n",
            "2020-01-22 23:11:33,267 :  guid: None\n",
            "2020-01-22 23:11:33,268 :  tokens: [CLS] the authors noted that the poor prognosis of pda ##c could be linked to psychological distress , promoted by beta - adren ##ergic signaling and that gaba inhibits these responses in in - vitro and in - vivo mouse models of pda ##c . [SEP]\n",
            "2020-01-22 23:11:33,268 :  input_ids: 102 111 1991 3742 198 111 3228 8419 131 22291 30116 968 195 5094 147 6175 10581 422 13390 214 6130 579 26330 5777 3354 137 198 12310 9233 407 2528 121 121 579 3335 137 121 579 3570 3475 1262 131 22291 30116 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,269 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,269 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-22 23:11:33,270 :  label: 0 (id = 0)\n",
            "2020-01-22 23:11:40,363 :  Writing example 10000 of 499662\n",
            "2020-01-22 23:11:47,322 :  Writing example 20000 of 499662\n",
            "2020-01-22 23:11:54,635 :  Writing example 30000 of 499662\n",
            "2020-01-22 23:12:01,760 :  Writing example 40000 of 499662\n",
            "2020-01-22 23:12:08,750 :  Writing example 50000 of 499662\n",
            "2020-01-22 23:12:15,853 :  Writing example 60000 of 499662\n",
            "2020-01-22 23:12:23,069 :  Writing example 70000 of 499662\n",
            "2020-01-22 23:12:30,498 :  Writing example 80000 of 499662\n",
            "2020-01-22 23:12:37,731 :  Writing example 90000 of 499662\n",
            "2020-01-22 23:12:44,713 :  Writing example 100000 of 499662\n",
            "2020-01-22 23:12:51,756 :  Writing example 110000 of 499662\n",
            "2020-01-22 23:12:58,746 :  Writing example 120000 of 499662\n",
            "2020-01-22 23:13:05,731 :  Writing example 130000 of 499662\n",
            "2020-01-22 23:13:12,719 :  Writing example 140000 of 499662\n",
            "2020-01-22 23:13:20,592 :  Writing example 150000 of 499662\n",
            "2020-01-22 23:13:27,588 :  Writing example 160000 of 499662\n",
            "2020-01-22 23:13:34,689 :  Writing example 170000 of 499662\n",
            "2020-01-22 23:13:41,816 :  Writing example 180000 of 499662\n",
            "2020-01-22 23:13:48,797 :  Writing example 190000 of 499662\n",
            "2020-01-22 23:13:55,905 :  Writing example 200000 of 499662\n",
            "2020-01-22 23:14:02,901 :  Writing example 210000 of 499662\n",
            "2020-01-22 23:14:10,824 :  Writing example 220000 of 499662\n",
            "2020-01-22 23:14:18,036 :  Writing example 230000 of 499662\n",
            "2020-01-22 23:14:25,099 :  Writing example 240000 of 499662\n",
            "2020-01-22 23:14:32,115 :  Writing example 250000 of 499662\n",
            "2020-01-22 23:14:39,350 :  Writing example 260000 of 499662\n",
            "2020-01-22 23:14:46,285 :  Writing example 270000 of 499662\n",
            "2020-01-22 23:14:53,397 :  Writing example 280000 of 499662\n",
            "2020-01-22 23:15:00,573 :  Writing example 290000 of 499662\n",
            "2020-01-22 23:15:07,519 :  Writing example 300000 of 499662\n",
            "2020-01-22 23:15:14,494 :  Writing example 310000 of 499662\n",
            "2020-01-22 23:15:23,077 :  Writing example 320000 of 499662\n",
            "2020-01-22 23:15:29,970 :  Writing example 330000 of 499662\n",
            "2020-01-22 23:15:37,139 :  Writing example 340000 of 499662\n",
            "2020-01-22 23:15:44,080 :  Writing example 350000 of 499662\n",
            "2020-01-22 23:15:51,258 :  Writing example 360000 of 499662\n",
            "2020-01-22 23:15:58,363 :  Writing example 370000 of 499662\n",
            "2020-01-22 23:16:05,351 :  Writing example 380000 of 499662\n",
            "2020-01-22 23:16:12,323 :  Writing example 390000 of 499662\n",
            "2020-01-22 23:16:19,502 :  Writing example 400000 of 499662\n",
            "2020-01-22 23:16:26,606 :  Writing example 410000 of 499662\n",
            "2020-01-22 23:16:33,752 :  Writing example 420000 of 499662\n",
            "2020-01-22 23:16:40,946 :  Writing example 430000 of 499662\n",
            "2020-01-22 23:16:49,857 :  Writing example 440000 of 499662\n",
            "2020-01-22 23:16:56,937 :  Writing example 450000 of 499662\n",
            "2020-01-22 23:17:03,901 :  Writing example 460000 of 499662\n",
            "2020-01-22 23:17:10,789 :  Writing example 470000 of 499662\n",
            "2020-01-22 23:17:18,035 :  Writing example 480000 of 499662\n",
            "2020-01-22 23:17:25,066 :  Writing example 490000 of 499662\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmBAZP0hSQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### save features on file\n",
        "# len(train_features)\n",
        "# import pickle\n",
        "# pickle.dump(train_features,open('train_features.out','wb') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCnci2jcW-nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "def model_train(estimator):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  \n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_InputExamples)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", TRAIN_STEPS)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMEqK5OHXOU1",
        "colab_type": "code",
        "outputId": "2eb9c3ff-fabd-4ac6-8160-cd6775234590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True,\n",
        "      num_labels = 2)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)\n",
        "  \n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features = train_features,\n",
        "    seq_length = MAX_SEQ_LENGTH,\n",
        "    drop_remainder = True,\n",
        "    is_training=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 23:17:40,796 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fee9a319d08>) includes params argument, but params are not passed to Estimator.\n",
            "2020-01-22 23:17:40,797 :  Using config: {'_model_dir': 'gs://ie_scibert/sciBERT_Model/scibert_scivocab_uncased', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.106.209.170:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee92adf0b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.106.209.170:8470', '_evaluation_master': 'grpc://10.106.209.170:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fee92ae6080>}\n",
            "2020-01-22 23:17:40,798 :  _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLpwM5LZXaTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_train(estimator=estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6bRgIqlkVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_predict(estimator,prediction_examples,input_features,checkpoint_path=None):\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afUhhyjT9UvE",
        "colab_type": "code",
        "outputId": "3c1f2dfc-6fb2-4a1b-bff9-f99cbfe2edac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "label_list = [0,1]\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'reg_label'\n",
        "dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "347943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juODKK1BMFch",
        "colab_type": "code",
        "outputId": "5295aa11-a1f0-463e-da8a-231374c93435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "predictions = model_predict(estimator=estimator, prediction_examples=dev_InputExamples, input_features=dev_features)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 23:27:27,983 :  Error recorded from infeed: Step was cancelled by an explicit call to `Session::Close()`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_vXCtJN9EX",
        "colab_type": "code",
        "outputId": "62dafef6-5aea-40eb-d7bd-8608894c0182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "labels_val = []\n",
        "true_label = list(dev['reg_label'])\n",
        "for item in predictions:\n",
        "    labels_val.append(label_list[np.argmax(item[1])])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 96106  14675]\n",
            " [ 15021 221887]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87    110781\n",
            "           1       0.94      0.94      0.94    236908\n",
            "\n",
            "    accuracy                           0.91    347689\n",
            "   macro avg       0.90      0.90      0.90    347689\n",
            "weighted avg       0.91      0.91      0.91    347689\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trJsGe2fV_zV",
        "colab_type": "code",
        "outputId": "ec67e953-78d3-46e7-dfdc-64b8c9e884bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "predictions = model_predict(estimator=estimator, prediction_examples=train_InputExamples, input_features=train_features)\n",
        "labels_val = []\n",
        "true_label = list(train['reg_label'])\n",
        "for item in predictions:\n",
        "    labels_val.append(label_list[np.argmax(item[1])])\n",
        "print(metrics.confusion_matrix(y_pred=labels_val,y_true=true_label))\n",
        "print(metrics.classification_report(y_pred=labels_val,y_true = true_label))\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 20:47:57,304 :  Closing session due to error Step was cancelled by an explicit call to `Session::Close()`.\n",
            "2020-01-22 20:55:11,726 :  Error recorded from infeed: Step was cancelled by an explicit call to `Session::Close()`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[156688   1919]\n",
            " [  3272 337778]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98    158607\n",
            "           1       0.99      0.99      0.99    341050\n",
            "\n",
            "    accuracy                           0.99    499657\n",
            "   macro avg       0.99      0.99      0.99    499657\n",
            "weighted avg       0.99      0.99      0.99    499657\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUqmnu65tdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e59c39fb-e62e-4a4f-d48a-dc8c16e2fe6f"
      },
      "source": [
        "list(train['sentence'][:5])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"In contrast gabapentin could n't prevent the increase of duration time as it showed a rapid increase from 4.52 s to 8.09 s from 2 week to 4 week .\",\n",
              " 'Adenosine , on the other hand , acts on adenosine receptors , of which A 1 and A 3 adenosine receptors inhibit , while A 2A and A 2B adenosine receptors stimulate adenylyl cyclase .',\n",
              " 'XREF_BIBR Ketoconazole is a potent cytochrome P450 enzyme inhibitor , and has interactions with the following drugs : benzodiazepines aside from lorazepam ; a variety of calcium channel blockers such as nifedipine and verapamil ; 3-hydroxy-3-methyl-glutaryl- CoA reductase inhibitors , aside from pravastatin and fluvastatin ; phenytoin ; warfarin ; and theophylline .',\n",
              " 'H3K9me1 and H3K9me2 are catalyzed by the histone methyltransferase G9a , whereas H3K9me3 is established by the histone methyltransferase Suv39h1 , which is often expressed in senescence and postmitotic cells .',\n",
              " 'In animal models , native GLP-1 stimulates beta-cell proliferation and inhibits apoptosis , possibly increasing beta-cell mass and function .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cfjRAc6CfNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c7401502-7e8b-4f11-9512-85a332bef71f"
      },
      "source": [
        "train[:5]"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence</th>\n",
              "      <th>element_name</th>\n",
              "      <th>element_type</th>\n",
              "      <th>reg_name</th>\n",
              "      <th>reg_type</th>\n",
              "      <th>reg_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>680292</td>\n",
              "      <td>In contrast gabapentin could n't prevent the i...</td>\n",
              "      <td>duration</td>\n",
              "      <td>Chemical</td>\n",
              "      <td>gabapentin</td>\n",
              "      <td>Chemical</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3233655</td>\n",
              "      <td>Adenosine , on the other hand , acts on adenos...</td>\n",
              "      <td>adenylyl cyclase</td>\n",
              "      <td>Protein Family|Protein Complex</td>\n",
              "      <td>adenosine receptors</td>\n",
              "      <td>Other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>866649</td>\n",
              "      <td>XREF_BIBR Ketoconazole is a potent cytochrome ...</td>\n",
              "      <td>calcium</td>\n",
              "      <td>Chemical</td>\n",
              "      <td>verapamil</td>\n",
              "      <td>Chemical</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1252943</td>\n",
              "      <td>H3K9me1 and H3K9me2 are catalyzed by the histo...</td>\n",
              "      <td>H3K9me1</td>\n",
              "      <td>Other</td>\n",
              "      <td>histone</td>\n",
              "      <td>Protein Family|Protein Complex</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>700504</td>\n",
              "      <td>In animal models , native GLP-1 stimulates bet...</td>\n",
              "      <td>proliferation</td>\n",
              "      <td>Biological Process</td>\n",
              "      <td>GLP-1</td>\n",
              "      <td>Protein</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... reg_label\n",
              "0      680292  ...         0\n",
              "1     3233655  ...         1\n",
              "2      866649  ...         0\n",
              "3     1252943  ...         1\n",
              "4      700504  ...         1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHDdWbEKCsKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0834bcec-a3e4-48e3-c2c5-b08d146715b4"
      },
      "source": [
        "print(len(all_train))\n",
        "print(len(dev))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3129209\n",
            "347689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH_04JGMu_vy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe2a2bdf-72b1-4c05-80f3-8958343e2c40"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(tf.gfile.GFile(data_pref + str('test.csv')), encoding='latin-1')\n",
        "test = test.dropna()\n",
        "print(len(test))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "386320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7oUZRRMzpXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = list(all_train['sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr0K7gom3l5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6efb9bcb-be3b-4f77-a512-f09f899f8211"
      },
      "source": [
        "len(max(a , key=len))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28393"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KKahFzh3oer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "568f0f0d-929d-4056-b90a-264230f5a6e0"
      },
      "source": [
        "len(all_train) + len(dev)+ len(test)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3863218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxKtAC01D6YB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdXMdMnZEMKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "349e0726-f973-440a-d237-f07130aaeb69"
      },
      "source": [
        "len(set(a))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2vig3tpEMu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}