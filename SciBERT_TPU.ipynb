{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciBERT_TPU",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHDBST/BERT_examples/blob/master/SciBERT_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_",
        "colab_type": "text"
      },
      "source": [
        "# Fine Tuning SciBERT on BioMed dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPgpRl5g2e2",
        "colab_type": "text"
      },
      "source": [
        "In this experiment, we will be pre-training a state-of-the-art Natural Language Understanding model [BERT](https://arxiv.org/abs/1810.04805.) on arbitrary text data using Google Cloud infrastructure.\n",
        "\n",
        "This guide covers all stages of the procedure, including:\n",
        "\n",
        "1. Setting up the training environment\n",
        "2. Downloading raw text data\n",
        "3. Preprocessing text data\n",
        "4. Learning a new vocabulary\n",
        "5. Creating sharded pre-training data\n",
        "6. Setting up GCS storage for data and model\n",
        "7. Training the model on a cloud TPU\n",
        "\n",
        "For persistent storage of training data and model, you will require a Google Cloud Storage bucket. \n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) to create a GCP account and GCS bucket. New Google Cloud users have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. \n",
        "\n",
        "Steps 1-5 of this tutorial can be run without a GCS bucket for demonstration purposes. In that case, however, you will not be able to train the model.\n",
        "\n",
        "**Note** \n",
        "The only parameter you *really have to set* is BUCKET_NAME in steps 5 and 6. Everything else has default values which should work for most use-cases.\n",
        "\n",
        "**Note** \n",
        "Pre-training a BERT-Base model on a TPUv2 will take about 54 hours. Google Colab is not designed for executing such long-running jobs and will interrupt the training process every 8 hours or so. For uninterrupted training, consider using a preemptible TPUv2 instance. \n",
        "\n",
        "That said, at the time of writing (09.05.2019), with a Colab TPU, pre-training a BERT model from scratch can be achieved at a negligible cost of storing the said model and data in GCS  (~1 USD).\n",
        "\n",
        "Now, let's get to business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODimOhBR05yR",
        "colab_type": "text"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) [2019] [Antyukhov Denis Olegovich]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjad5jsr9YaM",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: setting up training environment\n",
        "First and foremost, we get the packages required to train the model. \n",
        "The Jupyter environment allows executing bash commands directly from the notebook by using an exclamation mark ‘!’. I will be exploiting this approach to make use of several other bash commands throughout the experiment.\n",
        "\n",
        "Now, let’s import the packages and authorize ourselves in Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab_type": "code",
        "outputId": "d71507a8-53cb-4321-9fc0-5f31f98af452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "# !git clone https://github.com/google-research/bert\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "!pip install bert_tensorflow\n",
        "\n",
        "from bert import modeling, optimization, tokenization, run_classifier\n",
        "# from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "Requirement already satisfied: bert_tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert_tensorflow) (1.12.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 02:15:44,058 :  Using TPU runtime\n",
            "2020-01-17 02:15:44,065 :  TPU address is grpc://10.57.116.162:8470\n",
            "2020-01-17 02:15:44,066 :  \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVXMoC-aMy1",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: getting the data\n",
        "\n",
        "We begin with obtaining a corpus of raw text data. For this experiment, we will be using the [OpenSubtitles](http://www.opensubtitles.org/) dataset, which is available for 65 languages [here](http://opus.nlpl.eu/OpenSubtitles-v2016.php). \n",
        "\n",
        "Unlike more common text datasets (like Wikipedia) it does not require any complex pre-processing. It also comes pre-formatted with one sentence per line.\n",
        "\n",
        "Feel free to use the dataset for your language instead by changing the language code (en) below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotFkkshbdvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_pref = 'gs://ie_scibert/bert_model_classifier/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5TPsPOppn0",
        "colab_type": "text"
      },
      "source": [
        "For demonstration purposes, we will only use a small fraction of the whole corpus for this experiment. \n",
        "\n",
        "When training the real model, make sure to uncheck the DEMO_MODE checkbox to use a 100x larger dataset.\n",
        "\n",
        "Rest assured, 100M lines are perfectly sufficient to train a reasonably good BERT-base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDR5Z1MDgB1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEMO_MODE = True #@param {type:\"boolean\"}\n",
        "\n",
        "# if DEMO_MODE:\n",
        "#   CORPUS_SIZE = 1000000\n",
        "# else:\n",
        "#   CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
        "  \n",
        "# !(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
        "# !mv subdataset.txt dataset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRQd4-v0nQqH",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: preprocessing text\n",
        "\n",
        "The raw text data we have downloaded contains punсtuation, uppercase letters and non-UTF symbols which we will remove before proceeding. During inference, we will apply the same normalization procedure to new data.\n",
        "\n",
        "If your use-case requires different preprocessing (e.g. if uppercase letters or punctuation are expected during inference), feel free to modify the function below to accomodate for your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCi2oSdInRkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "\n",
        "def normalize_text(text):\n",
        "#   # lowercase text\n",
        "   text = str(text).lower()\n",
        "#   # remove non-UTF\n",
        "   text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "#   # remove punktuation symbols\n",
        "   text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "   return text\n",
        "\n",
        "def count_lines(filename):\n",
        "   count = 0\n",
        "   with open(filename) as fi:\n",
        "     for line in fi:\n",
        "       count += 1\n",
        "   return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtwtDnesaQQ",
        "colab_type": "text"
      },
      "source": [
        "Check how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gngtEZWqVhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize_text('Thanks to the advance, they have succeeded in getting over their adversaries.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-Kvnx6sUFS",
        "colab_type": "text"
      },
      "source": [
        "Apply normalization to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjxQe5awo1v",
        "colab_type": "code",
        "outputId": "053f0095-a3bc-4764-d44b-1923276ff8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " data_pref = 'gs://ie_scibert/bert_model_classifier/'\n",
        " RAW_DATA_FPATH = \"train.csv\" #@param {type: \"string\"}\n",
        " PRC_DATA_FPATH = \"proc_train.csv\" #@param {type: \"string\"}\n",
        "\n",
        "# # apply normalization to the dataset\n",
        "# # this will take a minute or two\n",
        "import tensorflow as tf\n",
        " \n",
        " fi = tf.gfile.GFile(data_pref +str(RAW_DATA_FPATH),mode='r')\n",
        " lines =  fi.readlines()\n",
        " total_lines = len(lines)\n",
        " bar = Progbar(total_lines)\n",
        " # #  with open(fraw,encoding = 'utf-8') as ff:\n",
        " with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "     for l in lines:\n",
        "       fo.write(normalize_text(l)+\"\\n\")\n",
        "       bar.add(1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3131488/3131488 [==============================] - 44s 14us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3A64RZjwo9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f = open(PRC_DATA_FPATH,encoding=\"utf-8\")\n",
        "# f.readlines()[1].split('\\t')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVO9EnUwrluQ",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: building the vocabulary\n",
        "\n",
        "For the next step, we will learn a new vocabulary that we will use to represent our dataset. \n",
        "\n",
        "The BERT paper uses a WordPiece tokenizer, which is not available in opensource. Instead, we will be using SentencePiece tokenizer in unigram mode. While it is not directly compatible with BERT, with a small hack we can make it work.\n",
        "\n",
        "SentencePiece requires quite a lot of RAM, so running it on the full dataset in Colab will crash the kernel. To avoid this, we will randomly subsample a fraction of the dataset for building the vocabulary. Another option would be to use a machine with more RAM for this step - that decision is up to you.\n",
        "\n",
        "Also, SentencePiece adds BOS and EOS control symbols to the vocabulary by default. We disable them explicitly by setting their indices to -1.\n",
        "\n",
        "The typical values for VOC_SIZE are somewhere in between 32000 and 128000. We reserve NUM_PLACEHOLDERS tokens in case one wants to update the vocabulary and fine-tune the model after the pre-training phase is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV",
        "colab_type": "code",
        "outputId": "eb306cbb-ac87-46ed-e45b-2625c881c1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
        "# ## 32000\n",
        "VOC_SIZE = 32000 #@param {type:\"integer\"} \n",
        "# ## 128000\n",
        "SUBSAMPLE_SIZE = 128000 #@param {type:\"integer\"}\n",
        "# ## 256\n",
        "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--bos_id=-1 --eos_id=-1').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAowCoH2u1iZ",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how we can make SentencePiece tokenizer work for the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLaw7kPW3he",
        "colab_type": "text"
      },
      "source": [
        "Below is a sentence tokenized using the WordPiece vocabulary from a pretrained English [BERT-base](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) model from the official [repo](https://github.com/google-research/bert). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwHAp_Gh5OPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testcase = \"Colorless geothermal substations are generating furiously\"\n",
        "# wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEGAVl_5YRD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")\n",
        "\n",
        "['color',\n",
        " '##less',\n",
        " 'geo',\n",
        " '##thermal',\n",
        " 'sub',\n",
        " '##station',\n",
        " '##s',\n",
        " 'are',\n",
        " 'generating',\n",
        " 'furiously']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNiLdWXTh9cj",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the WordPiece tokenizer prepends the subwords which occur in the middle of words with '##'. The subwords occurring at the beginning of words are unchanged. If the subword occurs both in the beginning and in the middle of words, both versions (with and without '##') are added to the vocabulary.\n",
        "\n",
        "Now let's have a look at the vocabulary that the SentencePiece tokenizer has learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8_ebLxqTnWu",
        "colab_type": "code",
        "outputId": "3983e270-84b8-41e3-f81a-6236d3441023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\t       proc_train.csv  shards\t\ttokenizer.vocab\n",
            "bert_model_classifier  sample_data     tokenizer.model\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlBqiv5UD-j",
        "colab_type": "text"
      },
      "source": [
        "SentencePiece has created two files: tokenizer.model and tokenizer.vocab. Let's have a look at the learned vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJ9QmNMUEQf",
        "colab_type": "code",
        "outputId": "9760b51c-a8a0-4fc2-b6b5-08258422706c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!head -n 100 tokenizer.vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "s\t-3.34309\n",
            "▁the\t-3.61561\n",
            "▁and\t-3.6847\n",
            "▁of\t-3.7019\n",
            "▁protein\t-3.71013\n",
            "▁1\t-3.90194\n",
            "▁in\t-3.96808\n",
            "d\t-4.09448\n",
            "1\t-4.41868\n",
            "f\t-4.52776\n",
            "▁to\t-4.55149\n",
            "ed\t-4.65651\n",
            "ing\t-4.66733\n",
            "_\t-4.67664\n",
            "▁xre\t-4.68623\n",
            "▁a\t-4.76593\n",
            "▁that\t-4.82168\n",
            "▁by\t-4.84483\n",
            "▁0\t-4.90863\n",
            "2\t-4.90881\n",
            "▁chemical\t-4.93598\n",
            "▁other\t-4.98606\n",
            "bibr\t-5.01241\n",
            "ly\t-5.07178\n",
            "y\t-5.20206\n",
            "▁\t-5.20729\n",
            "▁with\t-5.23067\n",
            "3\t-5.27961\n",
            "▁2\t-5.29763\n",
            "▁cells\t-5.355\n",
            "▁family\t-5.37114\n",
            "▁cell\t-5.43653\n",
            "▁is\t-5.43692\n",
            "▁p\t-5.44109\n",
            "e\t-5.48877\n",
            "▁process\t-5.53333\n",
            "▁biological\t-5.5991\n",
            "▁expression\t-5.61498\n",
            "▁increase\t-5.68976\n",
            "▁il\t-5.71863\n",
            "▁induce\t-5.76153\n",
            "▁as\t-5.76381\n",
            "▁complex\t-5.77693\n",
            "a\t-5.78655\n",
            "4\t-5.78843\n",
            "▁be\t-5.79557\n",
            "▁was\t-5.8471\n",
            "▁3\t-5.91566\n",
            "▁for\t-5.96643\n",
            "▁inhibit\t-5.96884\n",
            "▁or\t-5.97982\n",
            "▁which\t-5.99791\n",
            "ion\t-6.0563\n",
            "fig\t-6.09327\n",
            "▁not\t-6.11934\n",
            "▁we\t-6.14038\n",
            "▁activation\t-6.15365\n",
            "▁level\t-6.19325\n",
            "▁c\t-6.20907\n",
            "▁it\t-6.23385\n",
            "▁this\t-6.24482\n",
            "▁apoptosis\t-6.25483\n",
            "▁on\t-6.26116\n",
            "▁also\t-6.27227\n",
            "n\t-6.27434\n",
            "▁cd\t-6.30242\n",
            "▁activate\t-6.31407\n",
            "▁receptor\t-6.33635\n",
            "▁an\t-6.3393\n",
            "c\t-6.35721\n",
            "▁at\t-6.36519\n",
            "▁beta\t-6.36883\n",
            "▁mediated\t-6.41973\n",
            "▁t\t-6.4235\n",
            "▁m\t-6.42858\n",
            "▁activity\t-6.43539\n",
            "▁proliferation\t-6.44888\n",
            "▁5\t-6.47594\n",
            "▁alpha\t-6.4843\n",
            "▁from\t-6.49328\n",
            "▁4\t-6.49367\n",
            "▁pathway\t-6.50198\n",
            "▁were\t-6.51079\n",
            "5\t-6.52423\n",
            "en\t-6.53543\n",
            "▁decrease\t-6.54624\n",
            "▁can\t-6.55118\n",
            "▁but\t-6.56171\n",
            "▁are\t-6.56632\n",
            "▁these\t-6.56632\n",
            "▁6\t-6.56993\n",
            "▁effect\t-6.57554\n",
            "b\t-6.59835\n",
            "▁inhibitor\t-6.60052\n",
            "▁signaling\t-6.64549\n",
            "al\t-6.65422\n",
            "6\t-6.67729\n",
            "▁kinase\t-6.69189\n",
            "▁e\t-6.69601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsURk_h5jw4",
        "colab_type": "code",
        "outputId": "13a5c639-0e4d-479f-8c91-cc10741f3289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 31743\n",
            "Sample tokens: ['▁revert', 'yama', '7326', '▁angiogene', '▁uzu', '▁glioblastoma', '▁3638', '62242', '▁protru', '▁nek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YjqHVRpmlKq",
        "colab_type": "text"
      },
      "source": [
        "As we may observe, SentencePiece does quite the opposite to WordPiece. From the [documentation](https://github.com/google/sentencepiece/blob/master/README.md):\n",
        "\n",
        "\n",
        "SentencePiece first escapes the whitespace with a meta-symbol \"▁\" (U+2581) as follows:\n",
        "\n",
        "`Hello▁World`.\n",
        "\n",
        "Then, this text is segmented into small pieces, for example:\n",
        "\n",
        "`[Hello] [▁Wor] [ld] [.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-28l_0p0PQ",
        "colab_type": "text"
      },
      "source": [
        "Subwords which occur after whitespace (which are also those that most words begin with) are prepended with '▁', while others are unchanged. This excludes subwords which only occur at the beginning of sentences and nowhere else. These cases should be quite rare, however. \n",
        "\n",
        "So, in order to obtain a vocabulary analogous to WordPiece, we need to perform a simple conversion, removing \"▁\" from the tokens that contain it and adding \"##\"  to the ones that don't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJGFjzOMbfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dcVgD98S28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9ZR3RN9IMA",
        "colab_type": "text"
      },
      "source": [
        "We also add some special control symbols which are required by the BERT architecture. By convention, we put those at the beginning of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdTlXDPL8cHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1jvEr9EIdd",
        "colab_type": "text"
      },
      "source": [
        "We also append some placeholder tokens to the vocabulary. Those are useful if one wishes to update the pre-trained model with new, task-specific tokens. \n",
        "\n",
        "In that case, the placeholder tokens are replaced with new real ones, the pre-training data is re-generated, and the model is fine-tuned on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLYSTil4E0Dm",
        "colab_type": "code",
        "outputId": "7aea8f8d-bfab-414b-df60-df04c617c102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKk_7JI-MtW",
        "colab_type": "text"
      },
      "source": [
        "Finally, we write the obtained vocabulary to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1jg0cj9Duf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqXZnc3FCuY",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how the new vocabulary works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsSOnEnC-jG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "# bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauD8ndhEA-z",
        "colab_type": "text"
      },
      "source": [
        "Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFtStCo__QX",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: generating pre-training data\n",
        "\n",
        "With the vocabulary at hand, we are ready to generate pre-training data for the BERT model. Since our dataset might be quite large, we will split it into shards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "colab_type": "code",
        "outputId": "7718caae-5cdd-4b5f-f501-88f79a32184f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./shards’: File exists\n",
            "shard_0000  shard_0003\tshard_0006  shard_0009\tshard_0012\n",
            "shard_0001  shard_0004\tshard_0007  shard_0010\n",
            "shard_0002  shard_0005\tshard_0008  shard_0011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FSq3zNFvLs",
        "colab_type": "text"
      },
      "source": [
        "Before we start generating, we need to set some model-specific parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MibOBkFam2",
        "colab_type": "text"
      },
      "source": [
        "Now, for each shard we need to call *create_pretraining_data.py* script. To that end, we will employ the  *xargs* command. \n",
        "\n",
        "Running this might take quite some time depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZjIeVP0T36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XARGS_CMD = (\"ls ./shards/ | \"\n",
        "#              \"xargs -n 1 -P {} -I{} \"\n",
        "#              \"python3 bert/create_pretraining_data.py \"\n",
        "#              \"--input_file=./shards/{} \"\n",
        "#              \"--output_file={}/{}.tfrecord \"\n",
        "#              \"--vocab_file={} \"\n",
        "#              \"--do_lower_case={} \"\n",
        "#              \"--max_predictions_per_seq={} \"\n",
        "#              \"--max_seq_length={} \"\n",
        "#              \"--masked_lm_prob={} \"\n",
        "#              \"--random_seed=34 \"\n",
        "#              \"--dupe_factor=5\")\n",
        "\n",
        "# XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "#                              VOC_FNAME, DO_LOWER_CASE, \n",
        "#                              MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyo9_LcQ0pla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5XzaY8xCdiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "# !$XARGS_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnW_hVxPoP6",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: setting up persistent storage\n",
        "\n",
        "To preserve our hard-earned assets, we will persist them to Google Cloud Storage. Provided that you have created the GCS bucket, this should be simple.\n",
        "\n",
        "We will create two directories in GCS, one for the data and one for the model.\n",
        "In the model directory, we will put the model vocabulary and configuration file.\n",
        "\n",
        "**Configure your BUCKET_NAME variable here before proceeding, otherwise the model and data will not be saved.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtrt68QQIHs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"ie_scibert\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model_classifier\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigCcV-hSHVH",
        "colab_type": "text"
      },
      "source": [
        "Below is the sample hyperparameter configuration for BERT-base. Change at your own risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this for BERT-base\n",
        "\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgrEDugRG48",
        "colab_type": "code",
        "outputId": "5882f887-7cbb-4423-c44b-5e0ae1d7fba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: No URLs matched: pretraining_data\n",
            "Copying file://bert_model_classifier/bert_config.json [Content-Type=application/json]...\n",
            "Copying file://bert_model_classifier/vocab.txt [Content-Type=text/plain]...\n",
            "/ [2/2 files][219.3 KiB/219.3 KiB] 100% Done                                    \n",
            "Operation completed over 2 objects/219.3 KiB.                                    \n",
            "CommandException: 1 file/object could not be transferred.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DL6xuCAYPrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: training the model\n",
        "\n",
        "We are almost ready to begin training our model. If you wish  to continue an interrupted training run, you may skip steps 2-6 and proceed from here.\n",
        "\n",
        "**Make sure that you have set the BUCKET_NAME here as well.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "6c77f16e-bd1c-4a9b-dd74-b4789a26caaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BUCKET_NAME = \"ie_scibert\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model_classifier\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 02:18:02,435 :  Using checkpoint: gs://ie_scibert/bert_model_classifier/model.ckpt-267500\n",
            "2020-01-17 02:18:02,436 :  Using 2 data shards\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwF-WqcZHUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTFjITdd53F",
        "colab_type": "text"
      },
      "source": [
        "Prepare the training run configuration, build the estimator and input function, power up the bass cannon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn = model_fn_builder(\n",
        "#       bert_config=bert_config,\n",
        "#       init_checkpoint=INIT_CHECKPOINT,\n",
        "#       learning_rate=LEARNING_RATE,\n",
        "#       num_train_steps=TRAIN_STEPS,\n",
        "#       num_warmup_steps=10,\n",
        "#       use_tpu=USE_TPU,\n",
        "#       use_one_hot_embeddings=True)\n",
        "\n",
        "# tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "# run_config = tf.contrib.tpu.RunConfig(\n",
        "#     cluster=tpu_cluster_resolver,\n",
        "#     model_dir=BERT_GCS_DIR,\n",
        "#     save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "#     tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "#         iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "#         num_shards=NUM_TPU_CORES,\n",
        "#         per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#     use_tpu=USE_TPU,\n",
        "#     model_fn=model_fn,\n",
        "#     config=run_config,\n",
        "#     train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#     eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "# train_input_fn = input_fn_builder(\n",
        "#         input_files=input_files,\n",
        "#         max_seq_length=MAX_SEQ_LENGTH,\n",
        "#         max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "#         is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB",
        "colab_type": "text"
      },
      "source": [
        "Fire!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OeXod-fHMT",
        "colab_type": "text"
      },
      "source": [
        "Training the model with the default parameters for 1 million steps will take ~53 hours. \n",
        "\n",
        "In case the kernel is restarted, you may always continue training from the latest checkpoint. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZIAAATfzdF",
        "colab_type": "text"
      },
      "source": [
        "This concludes the guide to pre-training BERT from scratch on a cloud TPU. However, the really fun stuff is still  to come, so stay tuned.\n",
        "\n",
        "Keep learning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiFH_9Lbze5f",
        "colab_type": "code",
        "outputId": "313f5c81-2435-4800-9219-ad593f7b6c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!gsutil cp gs://hermes_assets/russian_uncased_L-12_H-768_A-12.zip gs://bert_resourses/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AccessDeniedException: 403 mohaddeseh.bastan@stonybrook.edu does not have storage.objects.list access to hermes_assets.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BrQ2dOkWo4p",
        "colab_type": "text"
      },
      "source": [
        "# Run classifier for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7_2pKWzfiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# import pandas as pd\n",
        "# train = pd.read_csv(tf.gfile.GFile(data_pref + str('train.csv')), encoding='latin-1')[:200000]\n",
        "# print(len(train))\n",
        "# #pd.read_csv(open('train.csv'),error_bad_lines=False)\n",
        "# train = train.dropna()\n",
        "# label_list = [0,1]\n",
        "# DATA_COLUMN = 'sentence'\n",
        "# LABEL_COLUMN = 'reg_label'\n",
        "# train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whHU9DMhE9ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, bert_tokenizer)\n",
        "\n",
        "# f = tf.gfile.GFile(data_pref + str('train_features.out'), mode='rb')#file_io.FileIO(data_pref + str('train_features.out'),mode='r')\n",
        "# train_features = pickle.load(f,encoding='utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmBAZP0hSQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# len(train_features)\n",
        "# import pickle\n",
        "# pickle.dump(train_features,open('train_features.out','wb') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCnci2jcW-nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "def model_train(estimator):\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  \n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_InputExamples)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", TRAIN_STEPS)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMEqK5OHXOU1",
        "colab_type": "code",
        "outputId": "965c2add-489a-411b-b9c8-099523a4e2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True,\n",
        "      num_labels = 2)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)\n",
        "  \n",
        "# train_input_fn = run_classifier.input_fn_builder(\n",
        "#     features = train_features,\n",
        "#     seq_length = MAX_SEQ_LENGTH,\n",
        "#     drop_remainder = True,\n",
        "#     is_training=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 02:18:05,580 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f6d00482620>) includes params argument, but params are not passed to Estimator.\n",
            "2020-01-17 02:18:05,582 :  Using config: {'_model_dir': 'gs://ie_scibert/bert_model_classifier', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.57.116.162:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6d4103f7b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.57.116.162:8470', '_evaluation_master': 'grpc://10.57.116.162:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f6d4103f400>}\n",
            "2020-01-17 02:18:05,583 :  _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLpwM5LZXaTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_train(estimator=estimator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6bRgIqlkVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_predict(estimator,prediction_examples,input_features,checkpoint_path=None):\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  if checkpoint_path: \n",
        "    predictions = estimator.predict(predict_input_fn,checkpoint_path=checkpoint_path)\n",
        "  else:\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities']) for sentence, prediction in zip(prediction_examples, predictions)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afUhhyjT9UvE",
        "colab_type": "code",
        "outputId": "e8868c4d-570a-4726-fb82-9bbd86207bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "dev = pd.read_csv(tf.gfile.GFile(data_pref + str('dev.csv')), encoding='latin-1')\n",
        "print(len(dev))\n",
        "dev = dev.dropna()\n",
        "label_list = [0,1]\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'reg_label'\n",
        "dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, bert_tokenizer)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "347943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 02:18:17,775 :  From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "2020-01-17 02:18:17,776 :  Writing example 0 of 347689\n",
            "2020-01-17 02:18:17,778 :  *** Example ***\n",
            "2020-01-17 02:18:17,779 :  guid: None\n",
            "2020-01-17 02:18:17,779 :  tokens: [CLS] one possibl ##e explanation for these results is that the know ##n amide [UNK] bond [UNK] cleavage activity of ra ##vz targets not only lc ##3 [UNK] pe but also ubiquitin conjugate ##d molecule ##s on scv ##s [UNK] [UNK] [UNK] [UNK] [UNK] in this context [UNK] ubiquitin or lc ##3 locate ##d on not only bacterial vacuole ##s but also cytosol ##ic bacteria mig ##ht be target ##ed by ra ##vz activity [UNK] [SEP]\n",
            "2020-01-17 02:18:17,780 :  input_ids: 2 410 538 39 3026 53 94 146 37 21 6 335 69 5963 1 2467 1 801 80 8 399 22851 640 59 249 708 32 1 2311 92 68 675 3479 12 391 5 67 20574 5 1 1 1 1 1 11 65 1581 1 675 55 708 32 2110 12 67 59 249 1240 6191 5 92 68 1068 109 1586 4089 5274 50 166 16 22 399 22851 80 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,781 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,782 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,783 :  label: 1 (id = 1)\n",
            "2020-01-17 02:18:17,785 :  *** Example ***\n",
            "2020-01-17 02:18:17,786 :  guid: None\n",
            "2020-01-17 02:18:17,786 :  tokens: [CLS] identifi ##cation of p ##130 [UNK] ca ##s [UNK] as a mediator of focal adhesion kinase promote ##d cell migration [UNK] xre ##f [UNK] bi ##br [UNK] fit ##s well to its interaction with pr [UNK] 39 [UNK] give ##n the finding that syndecan [UNK] 4 modulate ##s focal adhesion kinase phosphorylation [UNK] xre ##f [UNK] bi ##br [UNK] and may be activate ##d by pr [UNK] 39 as suggest ##ed by our observe ##d inhibition of pr [UNK] 39 [UNK] induce ##d chemotaxi ##s of neutrophil ##s with antibodies to syndecan [UNK] 4 [UNK] [SEP]\n",
            "2020-01-17 02:18:17,787 :  input_ids: 2 1234 2114 8 38 1671 1 145 5 1 46 20 879 8 2264 298 102 110 12 36 286 1 19 14 1 1233 5534 1 6062 5 227 15 179 412 31 1194 1 1920 1 1167 69 6 345 21 5832 1 85 337 5 2264 298 102 115 1 19 14 1 1233 5534 1 7 128 50 71 12 22 1194 1 1920 46 108 16 22 163 263 12 135 8 1194 1 1920 1 45 12 4026 5 8 945 5 31 948 15 5832 1 85 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,788 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,789 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,790 :  label: 1 (id = 1)\n",
            "2020-01-17 02:18:17,792 :  *** Example ***\n",
            "2020-01-17 02:18:17,793 :  guid: None\n",
            "2020-01-17 02:18:17,794 :  tokens: [CLS] cyclo [UNK] his pro [UNK] [UNK] via nrf ##2 activation [UNK] increase ##d ho [UNK] 1 expression [UNK] elevated ho activity and protect ##ed pc ##12 cells from ros toxicit ##y [UNK] xre ##f [UNK] bi ##br [UNK] [UNK] thus suggest ##ing that ho activity result ##ing in increase ##d production of antioxidant end [UNK] product ##s suppresses ros mediated nf [UNK] kappab activation [UNK] [SEP]\n",
            "2020-01-17 02:18:17,795 :  input_ids: 2 3072 1 4820 326 1 1 224 700 24 61 1 43 12 756 1 10 42 1 671 756 80 7 1148 16 655 424 34 84 198 1649 29 1 19 14 1 1233 5534 1 1 231 108 17 21 756 80 242 17 11 43 12 106 8 1065 601 1 721 5 591 198 77 167 1 176 61 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,796 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,797 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,798 :  label: 1 (id = 1)\n",
            "2020-01-17 02:18:17,799 :  *** Example ***\n",
            "2020-01-17 02:18:17,800 :  guid: None\n",
            "2020-01-17 02:18:17,801 :  tokens: [CLS] as shown in xre ##f [UNK] fig [UNK] both fusion protein mg ##7 [UNK] scfv and seb and seb could dramatical ##ly induce the production ##s of cytokine il [UNK] 2 and ifn [UNK] gamma as compared with ns control group [UNK] p [UNK] [UNK] 0 ##1 [UNK] [UNK] [SEP]\n",
            "2020-01-17 02:18:17,802 :  input_ids: 2 46 126 11 19 14 1 807 1 105 1297 9 318 157 1 7817 7 6958 7 6958 168 1629 28 45 6 106 5 8 415 44 1 33 7 203 1 202 46 213 31 1506 215 246 1 38 1 1 23 13 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,802 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,803 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,804 :  label: 1 (id = 1)\n",
            "2020-01-17 02:18:17,805 :  *** Example ***\n",
            "2020-01-17 02:18:17,806 :  guid: None\n",
            "2020-01-17 02:18:17,807 :  tokens: [CLS] similar ##ly [UNK] when cytokines ##is was prevent ##ed by inhibition of plk ##1 [UNK] polo like kinase 1 [UNK] activity during metaphase [UNK] disassembl ##y of mis ##12 was only sub ##tl ##y delay ##ed [UNK] xre ##f [UNK] fig [UNK] [UNK] [SEP]\n",
            "2020-01-17 02:18:17,807 :  input_ids: 2 270 28 1 251 419 380 51 208 16 22 135 8 1511 13 1 7371 292 102 10 1 80 260 8507 1 4212 29 8 4267 424 51 249 1081 11231 29 1384 16 1 19 14 1 807 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,808 :  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,809 :  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2020-01-17 02:18:17,809 :  label: 1 (id = 1)\n",
            "2020-01-17 02:18:24,986 :  Writing example 10000 of 347689\n",
            "2020-01-17 02:18:32,230 :  Writing example 20000 of 347689\n",
            "2020-01-17 02:18:39,868 :  Writing example 30000 of 347689\n",
            "2020-01-17 02:18:47,070 :  Writing example 40000 of 347689\n",
            "2020-01-17 02:18:54,278 :  Writing example 50000 of 347689\n",
            "2020-01-17 02:19:01,612 :  Writing example 60000 of 347689\n",
            "2020-01-17 02:19:09,362 :  Writing example 70000 of 347689\n",
            "2020-01-17 02:19:16,713 :  Writing example 80000 of 347689\n",
            "2020-01-17 02:19:24,096 :  Writing example 90000 of 347689\n",
            "2020-01-17 02:19:31,397 :  Writing example 100000 of 347689\n",
            "2020-01-17 02:19:38,786 :  Writing example 110000 of 347689\n",
            "2020-01-17 02:19:46,883 :  Writing example 120000 of 347689\n",
            "2020-01-17 02:19:54,225 :  Writing example 130000 of 347689\n",
            "2020-01-17 02:20:01,440 :  Writing example 140000 of 347689\n",
            "2020-01-17 02:20:08,850 :  Writing example 150000 of 347689\n",
            "2020-01-17 02:20:16,167 :  Writing example 160000 of 347689\n",
            "2020-01-17 02:20:23,355 :  Writing example 170000 of 347689\n",
            "2020-01-17 02:20:31,550 :  Writing example 180000 of 347689\n",
            "2020-01-17 02:20:38,872 :  Writing example 190000 of 347689\n",
            "2020-01-17 02:20:46,037 :  Writing example 200000 of 347689\n",
            "2020-01-17 02:20:53,261 :  Writing example 210000 of 347689\n",
            "2020-01-17 02:21:00,428 :  Writing example 220000 of 347689\n",
            "2020-01-17 02:21:07,797 :  Writing example 230000 of 347689\n",
            "2020-01-17 02:21:14,976 :  Writing example 240000 of 347689\n",
            "2020-01-17 02:21:22,174 :  Writing example 250000 of 347689\n",
            "2020-01-17 02:21:30,828 :  Writing example 260000 of 347689\n",
            "2020-01-17 02:21:38,081 :  Writing example 270000 of 347689\n",
            "2020-01-17 02:21:45,459 :  Writing example 280000 of 347689\n",
            "2020-01-17 02:21:52,776 :  Writing example 290000 of 347689\n",
            "2020-01-17 02:22:00,159 :  Writing example 300000 of 347689\n",
            "2020-01-17 02:22:07,927 :  Writing example 310000 of 347689\n",
            "2020-01-17 02:22:15,548 :  Writing example 320000 of 347689\n",
            "2020-01-17 02:22:23,235 :  Writing example 330000 of 347689\n",
            "2020-01-17 02:22:30,944 :  Writing example 340000 of 347689\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juODKK1BMFch",
        "colab_type": "code",
        "outputId": "30c65223-84e9-48d3-b1a4-4b874fcba80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "predictions = model_predict(estimator=estimator, prediction_examples=dev_InputExamples, input_features=dev_features)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 02:28:13,378 :  Error recorded from infeed: Step was cancelled by an explicit call to `Session::Close()`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_vXCtJN9EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}